{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":61542,"databundleVersionId":6888007,"sourceType":"competition"},{"sourceId":7352392,"sourceType":"datasetVersion","datasetId":4261771},{"sourceId":7358502,"sourceType":"datasetVersion","datasetId":4274044}],"dockerImageVersionId":30626,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# LLM - Detect AI Generated Text\n## Import","metadata":{}},{"cell_type":"code","source":"import math\nimport random\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nimport nltk\nnltk.download('punkt')\nnltk.download('stopwords')\nfrom nltk.tokenize import word_tokenize,sent_tokenize\nfrom nltk.corpus import stopwords\n\nfrom gensim.corpora import Dictionary\nfrom gensim.models import word2vec\nfrom gensim.models import doc2vec\n\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.utils import resample\nfrom sklearn.metrics import roc_curve ,precision_recall_curve,auc,confusion_matrix,ConfusionMatrixDisplay\n\nfrom tokenizers import (\n    decoders,\n    models,\n    normalizers,\n    pre_tokenizers,\n    processors,\n    trainers,\n    Tokenizer,\n)\nfrom transformers import PreTrainedTokenizerFast\n\nimport keras","metadata":{"execution":{"iopub.status.busy":"2024-01-07T22:58:53.870719Z","iopub.execute_input":"2024-01-07T22:58:53.871498Z","iopub.status.idle":"2024-01-07T22:59:33.953714Z","shell.execute_reply.started":"2024-01-07T22:58:53.871459Z","shell.execute_reply":"2024-01-07T22:59:33.952643Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Inferred Features","metadata":{}},{"cell_type":"code","source":"def features(dataset):\n    token_count=dataset[\"text\"].apply(lambda x: len(x))\n    sentence_count = []\n    punctuation_count = []\n    apostrofees_count = []\n    unk_count = []\n    for doc in dataset[\"text\"]:\n        unk = 0\n        dot = 0\n        punctuation = 0\n        apostrofees = 0\n        for token in doc:\n            if(token.endswith(\".\")):\n                dot+=1\n                punctuation+=1\n            elif(token.endswith(\",\") or token.endswith(\"?\") or token.endswith(\"!\")):\n                punctuation+=1\n            elif(token.count(\"'\")>0):\n                    apostrofees+=token.count(\"'\")\n            elif(token==\"[UNK]\"):\n                unk+=1\n        sentence_count.append(dot)\n        punctuation_count.append(punctuation)\n        apostrofees_count.append(apostrofees)\n        unk_count.append(unk)\n    df = pd.DataFrame(\n        columns=[\"token_num\",\"sent_num\",\"punct_sym\",\"apostrof_sym\",\"unk_num\"]\n    )\n    df[\"token_num\"]=token_count\n    df[\"sent_num\"]=sentence_count\n    df[\"punct_sym\"]=punctuation_count\n    df[\"apostrof_sym\"]=apostrofees_count\n    df[\"unk_num\"]=unk_count\n    return df","metadata":{"execution":{"iopub.status.busy":"2024-01-07T22:59:33.956080Z","iopub.execute_input":"2024-01-07T22:59:33.956818Z","iopub.status.idle":"2024-01-07T22:59:33.968585Z","shell.execute_reply.started":"2024-01-07T22:59:33.956774Z","shell.execute_reply":"2024-01-07T22:59:33.967510Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_dataset = pd.read_csv(\"/kaggle/input/llm-detect-ai-generated-text/test_essays.csv\")\ntokenizer = PreTrainedTokenizerFast.from_pretrained(\"/kaggle/input/models-wights/weight/preTrainedTokenizer/\")\ndoc_model = doc2vec.Doc2Vec.load(\"/kaggle/input/models-wights/weight/EmbeddingModel.bin\")\nmodel = keras.models.load_model(\"/kaggle/input/models-wights/DenseNetwork.h5\")","metadata":{"execution":{"iopub.status.busy":"2024-01-07T22:59:33.969817Z","iopub.execute_input":"2024-01-07T22:59:33.970139Z","iopub.status.idle":"2024-01-07T22:59:34.577253Z","shell.execute_reply.started":"2024-01-07T22:59:33.970111Z","shell.execute_reply":"2024-01-07T22:59:34.576228Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Tokenizing","metadata":{}},{"cell_type":"code","source":"#load tokenizer\ntokenized_test = test_dataset;\ntokenized_test[\"text\"] = test_dataset[\"text\"].apply(lambda x : tokenizer.tokenize(text=x))","metadata":{"execution":{"iopub.status.busy":"2024-01-07T22:59:34.578622Z","iopub.execute_input":"2024-01-07T22:59:34.578985Z","iopub.status.idle":"2024-01-07T22:59:34.596744Z","shell.execute_reply.started":"2024-01-07T22:59:34.578954Z","shell.execute_reply":"2024-01-07T22:59:34.595866Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Embedding","metadata":{}},{"cell_type":"code","source":"#load embeddings model\nembeddings_matrix = [doc_model.infer_vector(tokenList,epochs=10) for tokenList in tokenized_test[\"text\"]]\nembeddings_test_dataset = pd.DataFrame(embeddings_matrix)\nembeddings_test_dataset","metadata":{"execution":{"iopub.status.busy":"2024-01-07T22:59:34.600277Z","iopub.execute_input":"2024-01-07T22:59:34.600674Z","iopub.status.idle":"2024-01-07T22:59:34.646012Z","shell.execute_reply.started":"2024-01-07T22:59:34.600641Z","shell.execute_reply":"2024-01-07T22:59:34.644882Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Normalizado de embedding","metadata":{}},{"cell_type":"code","source":"embeddings_test_dataset[\"norm\"]=np.linalg.norm(embeddings_test_dataset, axis=1)\nnorm_embeddings_test_dataset = pd.DataFrame(np.apply_along_axis(lambda x: x / np.linalg.norm(x), axis=1, arr=embeddings_test_dataset))\nnorm_embeddings_test_dataset[\"normalized_norm\"] = (embeddings_test_dataset['norm'] - embeddings_test_dataset['norm'].min()) / (embeddings_test_dataset['norm'].max() - embeddings_test_dataset['norm'].min())\nnorm_embeddings_test_dataset = norm_embeddings_test_dataset.drop([100],axis=1)\npre_processed_test_data = pd.concat([tokenized_test,norm_embeddings_test_dataset],axis=1)\npre_processed_test_data","metadata":{"execution":{"iopub.status.busy":"2024-01-07T22:59:34.647606Z","iopub.execute_input":"2024-01-07T22:59:34.648050Z","iopub.status.idle":"2024-01-07T22:59:34.691787Z","shell.execute_reply.started":"2024-01-07T22:59:34.648007Z","shell.execute_reply":"2024-01-07T22:59:34.690958Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Feature inferring","metadata":{}},{"cell_type":"code","source":"test_features = features(pre_processed_test_data)\nfor col in test_features.columns:\n    if(test_features[col].max()>0):\n        test_features[col] = test_features[col] / np.linalg.norm(test_features[col])\npre_processed_test_data=pd.concat([pre_processed_test_data,test_features],axis=1).drop(\"text\",axis=1)","metadata":{"execution":{"iopub.status.busy":"2024-01-07T22:59:34.693016Z","iopub.execute_input":"2024-01-07T22:59:34.693359Z","iopub.status.idle":"2024-01-07T22:59:34.712601Z","shell.execute_reply.started":"2024-01-07T22:59:34.693330Z","shell.execute_reply":"2024-01-07T22:59:34.711395Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pre_processed_test_data","metadata":{"execution":{"iopub.status.busy":"2024-01-07T22:59:34.714593Z","iopub.execute_input":"2024-01-07T22:59:34.714968Z","iopub.status.idle":"2024-01-07T22:59:34.742870Z","shell.execute_reply.started":"2024-01-07T22:59:34.714935Z","shell.execute_reply":"2024-01-07T22:59:34.741694Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model prediction","metadata":{}},{"cell_type":"markdown","source":"### Dense Neural Network","metadata":{}},{"cell_type":"code","source":"#load model\nX_test = pre_processed_test_data.drop([\"id\"],axis=1)\npred = model.predict(x=X_test)","metadata":{"execution":{"iopub.status.busy":"2024-01-07T22:59:34.744567Z","iopub.execute_input":"2024-01-07T22:59:34.745572Z","iopub.status.idle":"2024-01-07T22:59:35.063641Z","shell.execute_reply.started":"2024-01-07T22:59:34.745536Z","shell.execute_reply":"2024-01-07T22:59:35.062675Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submition = pd.DataFrame()\nsubmition[\"id\"]=pre_processed_test_data[\"id\"].to_numpy()\nsubmition[\"generated\"] = pred.round(4)\nsubmition","metadata":{"execution":{"iopub.status.busy":"2024-01-07T22:59:35.064914Z","iopub.execute_input":"2024-01-07T22:59:35.065276Z","iopub.status.idle":"2024-01-07T22:59:35.080678Z","shell.execute_reply.started":"2024-01-07T22:59:35.065247Z","shell.execute_reply":"2024-01-07T22:59:35.079279Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submition.to_csv(\"/kaggle/working/submission.csv\",index=False)","metadata":{"execution":{"iopub.status.busy":"2024-01-07T23:00:38.718699Z","iopub.execute_input":"2024-01-07T23:00:38.719146Z","iopub.status.idle":"2024-01-07T23:00:38.727577Z","shell.execute_reply.started":"2024-01-07T23:00:38.719111Z","shell.execute_reply":"2024-01-07T23:00:38.726168Z"},"trusted":true},"execution_count":null,"outputs":[]}]}