{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM - Detect AI Generated Text \n",
    "# DATA PRE-PROCESS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/mrtc101/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/mrtc101/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "/home/mrtc101/Desktop/ciencias de la computacion/Cursado/4.2Inteligencia Artificial 2/Final/venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-01-10 16:59:04.737490: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-01-10 16:59:04.737526: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-01-10 16:59:04.739454: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-01-10 16:59:04.748619: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-01-10 16:59:05.929568: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import random\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "from nltk.tokenize import word_tokenize,sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from gensim.models import fasttext\n",
    "from gensim.models import doc2vec\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.utils import resample\n",
    "from sklearn.metrics import roc_curve ,precision_recall_curve,auc,confusion_matrix,ConfusionMatrixDisplay\n",
    "\n",
    "from tokenizers import (\n",
    "    decoders,\n",
    "    models,\n",
    "    normalizers,\n",
    "    pre_tokenizers,\n",
    "    processors,\n",
    "    trainers,\n",
    "    Tokenizer,\n",
    ")\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "import keras\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_dataset = pd.read_csv(\"../data/train_essays.csv\")\n",
    "prompts_dataset = pd.read_csv(\"../data/train_prompts.csv\")\n",
    "custom_data = pd.read_csv(\"../data/custom_essays.csv\")\n",
    "downloaded_data_1 = pd.read_csv(\"../data/train_v4_drcat_01.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding new Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Describing the imbalance of this dataset in terms of ration is 1:500. The dataset presents sever imbalance. Previous aproches using only 20 new LLM generated examples manually and random Downsampling technic, didn't reach a higher score than 0.56. \n",
    "\n",
    "Concluding that more new data is needed, i downloaded data shared by competitors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "downloaded_data_1[\"prompt_id\"] = downloaded_data_1[\"prompt_name\"].apply(lambda name : 0 if name == \"Car-free cities\" else 1 if name == \"Does the electoral college work?\" else 21 )\n",
    "downloaded_data_1 = downloaded_data_1[[\"prompt_id\",\"text\",\"label\"]].rename(columns={\"label\":\"generated\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = pd.concat([custom_data,downloaded_data_1],axis=0,ignore_index=True)\n",
    "new_data[\"id\"] = range(0,new_data.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_data = pd.concat([initial_dataset,new_data],ignore_index=True,axis=0)\n",
    "target_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(target_data.shape)\n",
    "target_data.drop_duplicates(subset=[\"text\"],inplace=True,keep=\"first\")\n",
    "target_data = target_data.dropna()\n",
    "target_data.reset_index(inplace=True,drop=True)\n",
    "print(target_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a Tokenizer\n",
    "Reading the competition discussions lead me to this [Notebook](https://www.kaggle.com/code/datafan07/train-your-own-tokenizer), where is suggested to add words with typos into the vocabulary for better performance by training a tokenizer.\n",
    "\n",
    "1. normalization\n",
    "2. pre-tokenization\n",
    "3. model\n",
    "4. post-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">ByteLevel: \n",
    ">\n",
    ">Splits on whitespaces while remapping all the bytes to a set of visible characters. This technique as been introduced by OpenAI with GPT-2 and has some more or less nice properties:\n",
    "> - Since it maps on bytes, a tokenizer using this only requires 256 characters as initial alphabet (the number of values a byte can have), as opposed to the 130,000+ Unicode characters.\n",
    "> - A consequence of the previous point is that it is absolutely unnecessary to have an unknown token using this since we can represent anything with 256 tokens (Youhou!! üéâüéâ)\n",
    "> - For non ascii characters, it gets completely unreadable, but it works nonetheless!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Byte-Pair Encoding tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Byte-Pair Encoding tokenizer\n",
    "raw_tokenizer = Tokenizer(models.BPE(unk_token=\"[UNK]\"))\n",
    "#Cleaning\n",
    "raw_tokenizer.normalizer =  normalizers.Sequence(\n",
    "    [\n",
    "        normalizers.NFC(),\n",
    "        normalizers.Lowercase(),\n",
    "        normalizers.Replace(\"\\n\",\" \"),\n",
    "        normalizers.Replace(\"\\r\",\" \"),\n",
    "        normalizers.Replace(\"\\t\",\" \")\n",
    "    ]    \n",
    "    )\n",
    "#First tokenization\n",
    "raw_tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel()\n",
    "#Training\n",
    "special_tokens = [\"[UNK]\", \"[PAD]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"]\n",
    "trainer = trainers.BpeTrainer(show_progress=True,special_tokens=special_tokens)\n",
    "\n",
    "def data_iter(dataset):\n",
    "    \"\"\"\n",
    "    A generator function for iterating over a dataset in chunks.\n",
    "    \"\"\"    \n",
    "    for i in range(0, len(dataset), 1000):\n",
    "        yield dataset[i : i + 1000][\"text\"]\n",
    "\n",
    "raw_tokenizer.train_from_iterator(data_iter(target_data),trainer)\n",
    "\n",
    "tokenizer = PreTrainedTokenizerFast(\n",
    "    tokenizer_object=raw_tokenizer,\n",
    "    unk_token=\"[UNK]\",\n",
    "    pad_token=\"[PAD]\",\n",
    "    cls_token=\"[CLS]\",\n",
    "    sep_token=\"[SEP]\",\n",
    "    mask_token=\"[MASK]\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.save_pretrained(\"../data/byte_pair_tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_tokenizer.normalizer.normalize_str(\"Mart√≠n's bag\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.tokenize(\"Mart√≠n's bag\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Words Tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset = target_data.copy()\n",
    "tokenized_dataset[\"text\"] = target_data[\"text\"].apply(lambda x : tokenizer.tokenize(x))\n",
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deleting stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_stopwords=[token for word in stopwords.words('english') for token in tokenizer.tokenize(word)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_docs = []\n",
    "for doc in tokenized_dataset[\"text\"]:\n",
    "    tokens = [word for word in doc if word not in tokenized_stopwords]  # Eliminar stopwords\n",
    "    new_docs.append(tokens)\n",
    "tokenized_dataset[\"text\"]=new_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset.to_csv(\"../data/lexp/tokenized_data/tokenized.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        ([ƒ†cars, ., ƒ†cars, ƒ†around, ƒ†since, ƒ†became, ƒ†...\n",
       "1        ([ƒ†transportation, ƒ†large, ƒ†necessity, ƒ†countr...\n",
       "2        ([ƒ†\", america, ƒ†love, ƒ†affair, ƒ†vehicles, ƒ†see...\n",
       "3        ([ƒ†often, ƒ†ride, ƒ†car, ?, ƒ†drive, ƒ†one, ƒ†motor...\n",
       "4        ([ƒ†cars, ƒ†wonderful, ƒ†thing, ., ƒ†perhaps, ƒ†one...\n",
       "                               ...                        \n",
       "65349    ([ƒ†, ƒ†dear, ƒ†senator, ,, ƒ†, ƒ†writing, ƒ†regardi...\n",
       "65350    ([ƒ†, ƒ†remember, ƒ†day, ƒ†distinctively, ., ƒ†sitt...\n",
       "65351    ([ƒ†, ƒ†dear, ƒ†senator, ,, ƒ†ƒ†, ƒ†writing, ƒ†letter...\n",
       "65352    ([ƒ†, ƒ†dear, ƒ†senator, ,, ƒ†, ƒ†writing, ƒ†urge, ƒ†...\n",
       "65353    ([ƒ†, ƒ†typical, ƒ†summer, ƒ†afternoon, ƒ†hometown,...\n",
       "Name: text, Length: 65354, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_dataset = tokenized_dataset.copy()\n",
    "docs_dataset[\"text\"] = [doc2vec.TaggedDocument(row[2],['z'+row[0]]) for row in tokenized_dataset.values]\n",
    "docs_dataset[\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_model = doc2vec.Doc2Vec(documents=docs_dataset[\"text\"],vector_size=100,epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_model.save(\"../data/lexp/embedding_model/docModel.bin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inferred Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def features(dataset):\n",
    "    token_count=dataset[\"text\"].apply(lambda x: len(x))\n",
    "    sentence_count = []\n",
    "    punctuation_count = []\n",
    "    apostrofees_count = []\n",
    "    unk_count = []\n",
    "    for doc in dataset[\"text\"]:\n",
    "        unk = 0\n",
    "        dot = 0\n",
    "        punctuation = 0\n",
    "        apostrofees = 0\n",
    "        for token in doc.words:\n",
    "            if(token.endswith(\".\")):\n",
    "                dot+=1\n",
    "                punctuation+=1\n",
    "            elif(token.endswith(\",\") or token.endswith(\"?\") or token.endswith(\"!\")):\n",
    "                punctuation+=1\n",
    "            elif(token.count(\"'\")>0):\n",
    "                    apostrofees+=token.count(\"'\")\n",
    "            elif(token==\"[UNK]\"):\n",
    "                unk+=1\n",
    "        sentence_count.append(dot)\n",
    "        punctuation_count.append(punctuation)\n",
    "        apostrofees_count.append(apostrofees)\n",
    "        unk_count.append(unk)\n",
    "    df = pd.DataFrame(\n",
    "        columns=[\"token_num\",\"sent_num\",\"punct_sym\",\"apostrof_sym\",\"unk_num\"]\n",
    "    )\n",
    "    df[\"token_num\"]=token_count\n",
    "    df[\"sent_num\"]=sentence_count\n",
    "    df[\"punct_sym\"]=punctuation_count\n",
    "    df[\"apostrof_sym\"]=apostrofees_count\n",
    "    df[\"unk_num\"]=unk_count\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalizar embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        ([ƒ†cars, ., ƒ†cars, ƒ†around, ƒ†since, ƒ†became, ƒ†...\n",
       "1        ([ƒ†transportation, ƒ†large, ƒ†necessity, ƒ†countr...\n",
       "2        ([ƒ†\", america, ƒ†love, ƒ†affair, ƒ†vehicles, ƒ†see...\n",
       "3        ([ƒ†often, ƒ†ride, ƒ†car, ?, ƒ†drive, ƒ†one, ƒ†motor...\n",
       "4        ([ƒ†cars, ƒ†wonderful, ƒ†thing, ., ƒ†perhaps, ƒ†one...\n",
       "                               ...                        \n",
       "65349    ([ƒ†, ƒ†dear, ƒ†senator, ,, ƒ†, ƒ†writing, ƒ†regardi...\n",
       "65350    ([ƒ†, ƒ†remember, ƒ†day, ƒ†distinctively, ., ƒ†sitt...\n",
       "65351    ([ƒ†, ƒ†dear, ƒ†senator, ,, ƒ†ƒ†, ƒ†writing, ƒ†letter...\n",
       "65352    ([ƒ†, ƒ†dear, ƒ†senator, ,, ƒ†, ƒ†writing, ƒ†urge, ƒ†...\n",
       "65353    ([ƒ†, ƒ†typical, ƒ†summer, ƒ†afternoon, ƒ†hometown,...\n",
       "Name: text, Length: 65354, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset = pd.read_csv(\"../data/tokenized_data/tokenized.csv\",)\n",
    "tokenized_dataset[\"text\"] = tokenized_dataset[\"text\"].apply(lambda x : eval(x))\n",
    "docs_dataset = tokenized_dataset.copy()\n",
    "docs_dataset[\"text\"] = [doc2vec.TaggedDocument(row[2],['z'+row[0]]) for row in tokenized_dataset.values]\n",
    "docs_dataset[\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_model = doc2vec.Doc2Vec.load(\"../data/embedding_model/docModel.bin\")\n",
    "arr = [doc_model.dv[doc.tags] for doc in docs_dataset[\"text\"]]\n",
    "embeddings_dataset = pd.DataFrame(np.reshape(arr,(len(arr), 100)))\n",
    "embeddings_dataset[\"norm\"]=np.linalg.norm(embeddings_dataset, axis=1)\n",
    "norm_embeddings_dataset = pd.DataFrame(np.apply_along_axis(lambda x: x / np.linalg.norm(x), axis=1, arr=embeddings_dataset))\n",
    "norm_embeddings_dataset[\"normalized_norm\"] = (embeddings_dataset['norm'] - embeddings_dataset['norm'].min()) / (embeddings_dataset['norm'].max() - embeddings_dataset['norm'].min())\n",
    "# Eliminar la columna '100' del DataFrame\n",
    "norm_embeddings_dataset = norm_embeddings_dataset.drop([100], axis=1)\n",
    "\n",
    "# Concatenar los DataFrames\n",
    "doc2vec_data = pd.concat([docs_dataset, norm_embeddings_dataset], axis=1)\n",
    "\n",
    "# Calcular caracter√≠sticas adicionales con la funci√≥n 'features' (no proporcionada en tu c√≥digo)\n",
    "doc_features = features(doc2vec_data)\n",
    "\n",
    "# Normalizar las caracter√≠sticas adicionales\n",
    "for col in doc_features.columns:\n",
    "    if doc_features[col].max() > 0:\n",
    "        doc_features[col] = doc_features[col] / np.linalg.norm(doc_features[col])\n",
    "\n",
    "# Concatenar las caracter√≠sticas adicionales al DataFrame principal y eliminar la columna 'text'\n",
    "doc2vec_data = pd.concat([doc2vec_data, doc_features], axis=1).drop(\"text\", axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar el DataFrame resultante en un archivo CSV\n",
    "train,test = doc2vec_data.split()\n",
    "train.to_csv(\"../data/lexp/pre_processed/train.csv\", index=False)\n",
    "test.to_csv(\"../data/lexp/pre_processed/test.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
