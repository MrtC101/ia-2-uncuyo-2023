{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_aB82fSvdYKB"
      },
      "source": [
        "# LLM - Detect AI Generated Text\n",
        "## Import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7itjdvI9dYKD",
        "outputId": "af42ede1-292a-43ea-8e85-bdb1a11a3d73"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /home/mrtc101/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     /home/mrtc101/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "/home/mrtc101/Desktop/ciencias de la computacion/Cursado/4.2Inteligencia Artificial 2/Final/venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "2024-01-08 00:22:45.904102: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-01-08 00:22:45.904149: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-01-08 00:22:46.024654: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-01-08 00:22:46.636929: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-01-08 00:22:48.867053: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
          ]
        }
      ],
      "source": [
        "import math\n",
        "import random\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "from nltk.tokenize import word_tokenize,sent_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "from gensim.corpora import Dictionary\n",
        "from gensim.models import word2vec\n",
        "from gensim.models import doc2vec\n",
        "\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.utils import resample\n",
        "from sklearn.metrics import roc_curve ,precision_recall_curve,auc,confusion_matrix,ConfusionMatrixDisplay\n",
        "\n",
        "from tokenizers import (\n",
        "    decoders,\n",
        "    models,\n",
        "    normalizers,\n",
        "    pre_tokenizers,\n",
        "    processors,\n",
        "    trainers,\n",
        "    Tokenizer,\n",
        ")\n",
        "from transformers import PreTrainedTokenizerFast\n",
        "\n",
        "import keras\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "4LBr4vwNdYKE"
      },
      "outputs": [],
      "source": [
        "initial_dataset = pd.read_csv(\"../data/train_essays.csv\")\n",
        "prompts_dataset = pd.read_csv(\"../data/train_prompts.csv\")\n",
        "custom_data = pd.read_csv(\"../data/custom_essays.csv\")\n",
        "downloaded_data_1 = pd.read_csv(\"../data/train_v3_drcat_01.csv\")\n",
        "#download_data_2 = pd.read_csv(\"../data/train_v3_drcat_02.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BZA-apgXdYKF"
      },
      "source": [
        "## Data analisis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "dG45LDWZdYKF",
        "outputId": "79d8fa5c-2d89-4075-ff59-2012af444706"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>prompt_id</th>\n",
              "      <th>text</th>\n",
              "      <th>generated</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0059830c</td>\n",
              "      <td>0</td>\n",
              "      <td>Cars. Cars have been around since they became ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>005db917</td>\n",
              "      <td>0</td>\n",
              "      <td>Transportation is a large necessity in most co...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>008f63e3</td>\n",
              "      <td>0</td>\n",
              "      <td>\"America's love affair with it's vehicles seem...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>00940276</td>\n",
              "      <td>0</td>\n",
              "      <td>How often do you ride in a car? Do you drive a...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>00c39458</td>\n",
              "      <td>0</td>\n",
              "      <td>Cars are a wonderful thing. They are perhaps o...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1373</th>\n",
              "      <td>fe6ff9a5</td>\n",
              "      <td>1</td>\n",
              "      <td>There has been a fuss about the Elector Colleg...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1374</th>\n",
              "      <td>ff669174</td>\n",
              "      <td>0</td>\n",
              "      <td>Limiting car usage has many advantages. Such a...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1375</th>\n",
              "      <td>ffa247e0</td>\n",
              "      <td>0</td>\n",
              "      <td>There's a new trend that has been developing f...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1376</th>\n",
              "      <td>ffc237e9</td>\n",
              "      <td>0</td>\n",
              "      <td>As we all know cars are a big part of our soci...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1377</th>\n",
              "      <td>ffe1ca0d</td>\n",
              "      <td>0</td>\n",
              "      <td>Cars have been around since the 1800's and hav...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1378 rows Ã— 4 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "            id  prompt_id                                               text  \\\n",
              "0     0059830c          0  Cars. Cars have been around since they became ...   \n",
              "1     005db917          0  Transportation is a large necessity in most co...   \n",
              "2     008f63e3          0  \"America's love affair with it's vehicles seem...   \n",
              "3     00940276          0  How often do you ride in a car? Do you drive a...   \n",
              "4     00c39458          0  Cars are a wonderful thing. They are perhaps o...   \n",
              "...        ...        ...                                                ...   \n",
              "1373  fe6ff9a5          1  There has been a fuss about the Elector Colleg...   \n",
              "1374  ff669174          0  Limiting car usage has many advantages. Such a...   \n",
              "1375  ffa247e0          0  There's a new trend that has been developing f...   \n",
              "1376  ffc237e9          0  As we all know cars are a big part of our soci...   \n",
              "1377  ffe1ca0d          0  Cars have been around since the 1800's and hav...   \n",
              "\n",
              "      generated  \n",
              "0             0  \n",
              "1             0  \n",
              "2             0  \n",
              "3             0  \n",
              "4             0  \n",
              "...         ...  \n",
              "1373          0  \n",
              "1374          0  \n",
              "1375          0  \n",
              "1376          0  \n",
              "1377          0  \n",
              "\n",
              "[1378 rows x 4 columns]"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "initial_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A5Ufk5XndYKG",
        "outputId": "3450810c-2adc-4c00-fc1f-c2d1456b9634"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "id           object\n",
              "prompt_id     int64\n",
              "text         object\n",
              "generated     int64\n",
              "dtype: object"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "initial_dataset.dtypes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y5ClvnTtdYKH",
        "outputId": "196e4691-b015-4bb9-af88-4c546b343f5e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(1378, 4)"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "initial_dataset.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "M6w38AusdYKH",
        "outputId": "b1fdc793-13a5-4d99-d115-2f2e700905a3"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>prompt_id</th>\n",
              "      <th>generated</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>1378.000000</td>\n",
              "      <td>1378.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>0.486212</td>\n",
              "      <td>0.002177</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>0.499991</td>\n",
              "      <td>0.046625</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         prompt_id    generated\n",
              "count  1378.000000  1378.000000\n",
              "mean      0.486212     0.002177\n",
              "std       0.499991     0.046625\n",
              "min       0.000000     0.000000\n",
              "25%       0.000000     0.000000\n",
              "50%       0.000000     0.000000\n",
              "75%       1.000000     0.000000\n",
              "max       1.000000     1.000000"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "initial_dataset.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LNIWdXOudYKJ"
      },
      "source": [
        "`generated` - Whether the essay was written by a student (0) or generated by an LLM (1). This field is the target and is not present in test_essays.csv."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "opIz_MYjdYKK",
        "outputId": "a0779745-675b-420c-ab35-9f76ed996bd4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "generated\n",
              "0    1375\n",
              "1       3\n",
              "Name: count, dtype: int64"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "initial_dataset.value_counts(\"generated\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dR2Ut5ModYKK",
        "outputId": "7116634c-ea81-4443-d71c-ec79f3823973"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "generated\n",
              "0    1375\n",
              "1       3\n",
              "Name: count, dtype: int64"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "generated = initial_dataset.value_counts(\"generated\")\n",
        "generated"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F2vi4-PUdYKL",
        "outputId": "8462130b-38f0-4589-853a-ac85d77928d1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "generated\n",
              "0    0.997823\n",
              "1    0.002177\n",
              "Name: count, dtype: float64"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "generated = generated.apply(lambda x : x/initial_dataset.shape[0])\n",
        "generated"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "T-caBmNmdYKL",
        "outputId": "0e2f175e-45cc-45ad-be23-6aa097778e24"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 400x300 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "fig, ax = plt.subplots(figsize=(4,3))\n",
        "fig.suptitle(\"Generated\")\n",
        "ax.bar(\n",
        "    x=[\"By studentes\",\"By LLM\"],\n",
        "    height=generated,\n",
        "    width=0.2,\n",
        "    color=[\"red\",\"green\"],\n",
        "    align=\"center\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "id": "cEvFa1lQdYKL",
        "outputId": "cda624f9-81ba-42a5-eb31-d723b1f7ac01"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>prompt_id</th>\n",
              "      <th>prompt_name</th>\n",
              "      <th>instructions</th>\n",
              "      <th>source_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>Car-free cities</td>\n",
              "      <td>Write an explanatory essay to inform fellow ci...</td>\n",
              "      <td># In German Suburb, Life Goes On Without Cars ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>Does the electoral college work?</td>\n",
              "      <td>Write a letter to your state senator in which ...</td>\n",
              "      <td># What Is the Electoral College? by the Office...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   prompt_id                       prompt_name  \\\n",
              "0          0                   Car-free cities   \n",
              "1          1  Does the electoral college work?   \n",
              "\n",
              "                                        instructions  \\\n",
              "0  Write an explanatory essay to inform fellow ci...   \n",
              "1  Write a letter to your state senator in which ...   \n",
              "\n",
              "                                         source_text  \n",
              "0  # In German Suburb, Life Goes On Without Cars ...  \n",
              "1  # What Is the Electoral College? by the Office...  "
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "prompts_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aJArVOlmdYKL",
        "outputId": "5db7ce67-19ba-45b9-9925-b71fe40f2819"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "prompt_id\n",
              "0    708\n",
              "1    670\n",
              "Name: count, dtype: int64"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "prompts = initial_dataset.value_counts(\"prompt_id\")\n",
        "prompts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "O-fg234YdYKM",
        "outputId": "3a4c4755-4c18-4789-c7ed-b079cfffd362"
      },
      "outputs": [
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAbkAAAEwCAYAAAAq6w84AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8g+/7EAAAACXBIWXMAAA9hAAAPYQGoP6dpAAA1XklEQVR4nO3de1xUZf4H8M9wG4fLDIIyI4lC5gXKS2LhqKUmSopWK7rVskZFubGIeUlbWtd7WZphvtbUbAO3NMtttbK8oqkhqVEmKmKgBooDlsKIxXD7/v7wx9lGQBzE0OPn/Xqd14t5nuec85zDnPPhzHkOoxERARERkQo5NXcHiIiIrheGHBERqRZDjoiIVIshR0REqsWQIyIi1WLIERGRajHkiIhItRhyRESkWgw5IiJSLYYcERGpFkOObhgpKSnQaDTK1KJFC3Tq1Anjxo1DYWFhc3fPYQUFBZg5cyYOHDjQ3F0humW5NHcHiC43e/ZsBAUFoaysDF999RWWLl2KL774AocOHYK7u3tzd++qFRQUYNasWQgMDESPHj2auztEtySGHN1whg4dil69egEAnnnmGfj6+uKNN97AJ598gscff7xW+4sXL8LDw+P37iYR3QT4cSXd8B544AEAwIkTJ/Dkk0/C09MTubm5GDZsGLy8vBAdHQ3gUthNnjwZAQEB0Gq16Ny5M15//XVc/kUbGo0G48aNw9q1axESEgKdTgez2YzMzEwAwPLly3HHHXegRYsWGDBgAE6ePGk3/4ABA3DXXXchIyMDffr0gU6nQ1BQEJYtW6a0+fLLL3HPPfcAAJ566inlI9iUlBQAwA8//ICoqCiYTCa0aNECbdu2xWOPPYaSkpLrsQuJblm8kqMbXm5uLgDA19cXAFBZWYmIiAj069cPr7/+Otzd3SEieOihh7Bjxw7ExsaiR48e2Lx5M6ZMmYLTp08jKSnJbpm7d+/Gp59+ivj4eADAvHnzMHz4cEydOhVvvfUW/vrXv+L8+fOYP38+nn76aWzfvt1u/vPnz2PYsGH44x//iMcffxwfffQR4uLi4ObmhqeffhrBwcGYPXs2pk+fjrFjx+K+++4DAPTp0wfl5eWIiIiAzWZDQkICTCYTTp8+jQ0bNqC4uBgGg+F671KiW4cQ3SCSk5MFgGzbtk3Onj0r+fn5smbNGvH19RWdTienTp2SmJgYASB/+9vf7OZdv369AJC5c+falY8aNUo0Go3k5OQoZQBEq9XKiRMnlLLly5cLADGZTGK1WpXyxMREAWDXtn///gJAFi5cqJTZbDbp0aOH+Pn5SXl5uYiI7N+/XwBIcnKyXZ++++47ASBr165t7K4ioqvEjyvphhMeHo7WrVsjICAAjz32GDw9PbFu3TrcdtttSpu4uDi7eb744gs4Oztj/PjxduWTJ0+GiGDjxo125YMGDUJgYKDyOiwsDAAQFRUFLy+vWuXHjx+3m9/FxQV/+ctflNdubm74y1/+gqKiImRkZFxx+2qu1DZv3oxffvnlim2J6Now5OiGs2TJEmzduhU7duzAkSNHcPz4cURERCj1Li4uaNu2rd08P/74I/z9/e0CCgCCg4OV+t9q166d3eua4AkICKiz/Pz583bl/v7+tQa7dOrUCQBq3cO7XFBQECZNmoR33nkHrVq1QkREBJYsWcL7cUTXAUOObjj33nsvwsPDMWDAAAQHB8PJyf5tqtVqa5U5ytnZ2aFyuWzwyrVauHAhDh48iJdeegm//vorxo8fjzvvvBOnTp1q0vUQ3eoYcqQK7du3R0FBAS5cuGBXfvToUaW+KRUUFODixYt2ZceOHQMA5WNQjUZzxWV07doV06ZNw65du7B7926cPn3aboQmEV07hhypwrBhw1BVVYV//vOfduVJSUnQaDQYOnRok66vsrISy5cvV16Xl5dj+fLlaN26NUJDQwFA+TizuLjYbl6r1YrKykq7sq5du8LJyQk2m61J+0l0q+MjBKQKI0aMwMCBA/H3v/8dJ0+eRPfu3bFlyxZ88sknmDBhAjp06NCk6/P398drr72GkydPolOnTvjwww9x4MABvP3223B1dQUAdOjQAd7e3li2bBm8vLzg4eGBsLAwfP/99xg3bhxGjx6NTp06obKyEu+99x6cnZ0RFRXVpP0kutUx5EgVnJyc8Omnn2L69On48MMPkZycjMDAQCxYsACTJ09u8vW1bNkSK1euREJCAlasWAGj0Yh//vOfePbZZ5U2rq6uWLlyJRITE/Hcc8+hsrISycnJ6N+/PyIiIvDZZ5/h9OnTcHd3R/fu3bFx40b07t27yftKdCvTSFPfUSdSuQEDBuCnn37CoUOHmrsrRNQA3pMjIiLVYsgREZFqMeSIiEi1eE+OiIhUi1dyRESkWgw5IiJSLYYcERGpFkOOiIhUiyFHRESqxZAjIiLVYsgREZFqMeSIiEi1GHJERKRaN+VX7VRXV6OgoABeXl4NfvsyEdHNQERw4cIF+Pv7w8mJ1x9N5aYMuYKCAgQEBDR3N4iImlx+fj7atm3b3N1QjZsy5Ly8vABcejPo9fpm7g0R0bWzWq0ICAhQzm/UNG7KkKv5iFKv1zPkiEhVeAumafGDXyIiUi2GHBERqRZDjoiIVIshR0REqsWQIyIi1WLIERGRajHkiIhItRwKucDAQGg0mlpTfHw8AKCsrAzx8fHw9fWFp6cnoqKiUFhYaLeMvLw8REZGwt3dHX5+fpgyZQoqKyubbouIiIj+n0MPg+/fvx9VVVXK60OHDmHw4MEYPXo0AGDixIn4/PPPsXbtWhgMBowbNw4jR45EWloaAKCqqgqRkZEwmUzYs2cPzpw5gyeeeAKurq545ZVXmnCzruBmftBSpLl7QER0c5Fr8Pzzz0uHDh2kurpaiouLxdXVVdauXavUZ2VlCQBJT08XEZEvvvhCnJycxGKxKG2WLl0qer1ebDbbVa+3pKREAEhJSYnjnb4UFTfnRESqdU3nNapXo+/JlZeX4/3338fTTz8NjUaDjIwMVFRUIDw8XGnTpUsXtGvXDunp6QCA9PR0dO3aFUajUWkTEREBq9WKw4cP17sum80Gq9VqNxERETWk0SG3fv16FBcX48knnwQAWCwWuLm5wdvb266d0WiExWJR2vw24Grqa+rqM2/ePBgMBmXiNxAQEdHVaHTI/etf/8LQoUPh7+/flP2pU2JiIkpKSpQpPz//uq+TiIhufo36FoIff/wR27Ztw3//+1+lzGQyoby8HMXFxXZXc4WFhTCZTEqbffv22S2rZvRlTZu6aLVaaLXaxnSViIhuYY26kktOToafnx8iIyOVstDQULi6uiI1NVUpy87ORl5eHsxmMwDAbDYjMzMTRUVFSputW7dCr9cjJCSksdtARERUJ4ev5Kqrq5GcnIyYmBi4uPxvdoPBgNjYWEyaNAk+Pj7Q6/VISEiA2WxG7969AQBDhgxBSEgIxowZg/nz58NisWDatGmIj4/nlRoRETU5h0Nu27ZtyMvLw9NPP12rLikpCU5OToiKioLNZkNERATeeustpd7Z2RkbNmxAXFwczGYzPDw8EBMTg9mzZ1/bVhAREdVBI3LzPWFstVphMBhQUlLi+DeD82FwIroBXdN5jerF/11JRESqxZAjIiLVatQjBERE15tm1s17a0Fm8NbCjYJXckREpFoMOSIiUi2GHBERqRZDjoiIVIshR0REqsWQIyIi1WLIERGRajHkiIhItRhyRESkWgw5IiJSLYYcERGpFkOOiIhUiyFHRESqxZAjIiLVYsgREZFqMeSIiEi1GHJERKRaDofc6dOn8ec//xm+vr7Q6XTo2rUrvvnmG6VeRDB9+nS0adMGOp0O4eHh+OGHH+yWce7cOURHR0Ov18Pb2xuxsbEoLS299q0hIiL6DYdC7vz58+jbty9cXV2xceNGHDlyBAsXLkTLli2VNvPnz8fixYuxbNky7N27Fx4eHoiIiEBZWZnSJjo6GocPH8bWrVuxYcMG7Nq1C2PHjm26rSIiIgKgERG52sZ/+9vfkJaWht27d9dZLyLw9/fH5MmT8cILLwAASkpKYDQakZKSgsceewxZWVkICQnB/v370atXLwDApk2bMGzYMJw6dQr+/v4N9sNqtcJgMKCkpAR6vf5qu3+JRuNY+xvJ1f+qiG56mlk377EqMxw/Vq/pvEb1cuhK7tNPP0WvXr0wevRo+Pn54e6778aKFSuU+hMnTsBisSA8PFwpMxgMCAsLQ3p6OgAgPT0d3t7eSsABQHh4OJycnLB3794612uz2WC1Wu0mIiKihjgUcsePH8fSpUvRsWNHbN68GXFxcRg/fjxWrlwJALBYLAAAo9FoN5/RaFTqLBYL/Pz87OpdXFzg4+OjtLncvHnzYDAYlCkgIMCRbhMR0S3KoZCrrq5Gz5498corr+Duu+/G2LFj8eyzz2LZsmXXq38AgMTERJSUlChTfn7+dV0fERGpg0Mh16ZNG4SEhNiVBQcHIy8vDwBgMpkAAIWFhXZtCgsLlTqTyYSioiK7+srKSpw7d05pczmtVgu9Xm83ERERNcShkOvbty+ys7Ptyo4dO4b27dsDAIKCgmAymZCamqrUW61W7N27F2azGQBgNptRXFyMjIwMpc327dtRXV2NsLCwRm8IERHR5VwcaTxx4kT06dMHr7zyCv74xz9i3759ePvtt/H2228DADQaDSZMmIC5c+eiY8eOCAoKwj/+8Q/4+/vjkUceAXDpyu/BBx9UPuasqKjAuHHj8Nhjj13VyEoiIqKr5VDI3XPPPVi3bh0SExMxe/ZsBAUFYdGiRYiOjlbaTJ06FRcvXsTYsWNRXFyMfv36YdOmTWjRooXSZtWqVRg3bhwGDRoEJycnREVFYfHixU23VURERHDwObkbBZ+TI1I/PidHTYH/u5KIiFSLIUdERKrFkCMiItViyBERkWox5IiISLUYckREpFoMOSIiUi2GHBERqRZDjoiIVIshR0REqsWQIyIi1WLIERGRajHkiIhItRhyRESkWgw5IiJSLYYcERGpFkOOiIhUiyFHRESqxZAjIiLVYsgREZFqORRyM2fOhEajsZu6dOmi1JeVlSE+Ph6+vr7w9PREVFQUCgsL7ZaRl5eHyMhIuLu7w8/PD1OmTEFlZWXTbA0REdFvuDg6w5133olt27b9bwEu/1vExIkT8fnnn2Pt2rUwGAwYN24cRo4cibS0NABAVVUVIiMjYTKZsGfPHpw5cwZPPPEEXF1d8corrzTB5hAREf2PwyHn4uICk8lUq7ykpAT/+te/sHr1ajzwwAMAgOTkZAQHB+Prr79G7969sWXLFhw5cgTbtm2D0WhEjx49MGfOHLz44ouYOXMm3Nzcrn2LiIiI/p/D9+R++OEH+Pv74/bbb0d0dDTy8vIAABkZGaioqEB4eLjStkuXLmjXrh3S09MBAOnp6ejatSuMRqPSJiIiAlarFYcPH653nTabDVar1W4iIiJqiEMhFxYWhpSUFGzatAlLly7FiRMncN999+HChQuwWCxwc3ODt7e33TxGoxEWiwUAYLFY7AKupr6mrj7z5s2DwWBQpoCAAEe6TUREtyiHPq4cOnSo8nO3bt0QFhaG9u3b46OPPoJOp2vyztVITEzEpEmTlNdWq5VBR0REDbqmRwi8vb3RqVMn5OTkwGQyoby8HMXFxXZtCgsLlXt4JpOp1mjLmtd13eerodVqodfr7SYiIqKGXFPIlZaWIjc3F23atEFoaChcXV2Rmpqq1GdnZyMvLw9msxkAYDabkZmZiaKiIqXN1q1bodfrERISci1dISIiqsWhjytfeOEFjBgxAu3bt0dBQQFmzJgBZ2dnPP744zAYDIiNjcWkSZPg4+MDvV6PhIQEmM1m9O7dGwAwZMgQhISEYMyYMZg/fz4sFgumTZuG+Ph4aLXa67KBRER063Io5E6dOoXHH38cP//8M1q3bo1+/frh66+/RuvWrQEASUlJcHJyQlRUFGw2GyIiIvDWW28p8zs7O2PDhg2Ii4uD2WyGh4cHYmJiMHv27KbdKiIiIgAaEZHm7oSjrFYrDAYDSkpKHL8/p9Fcn079Hm6+XxVRo2lm3bzHqsxw/Fi9pvMa1Yv/u5KIiFSLIUdERKrFkCMiItViyBERkWox5IiISLUYckREpFoMOSIiUi2GHBERqRZDjoiIVIshR0REqsWQIyIi1WLIERGRajHkiIhItRhyRESkWgw5IiJSLYYcERGpFkOOiIhUiyFHRESqxZAjIiLVYsgREZFqXVPIvfrqq9BoNJgwYYJSVlZWhvj4ePj6+sLT0xNRUVEoLCy0my8vLw+RkZFwd3eHn58fpkyZgsrKymvpChERUS2NDrn9+/dj+fLl6Natm135xIkT8dlnn2Ht2rXYuXMnCgoKMHLkSKW+qqoKkZGRKC8vx549e7By5UqkpKRg+vTpjd8KIiKiOjQq5EpLSxEdHY0VK1agZcuWSnlJSQn+9a9/4Y033sADDzyA0NBQJCcnY8+ePfj6668BAFu2bMGRI0fw/vvvo0ePHhg6dCjmzJmDJUuWoLy8vGm2ioiICI0Mufj4eERGRiI8PNyuPCMjAxUVFXblXbp0Qbt27ZCeng4ASE9PR9euXWE0GpU2ERERsFqtOHz4cJ3rs9lssFqtdhMREVFDXBydYc2aNfj222+xf//+WnUWiwVubm7w9va2KzcajbBYLEqb3wZcTX1NXV3mzZuHWbNmOdpVIiK6xTl0JZefn4/nn38eq1atQosWLa5Xn2pJTExESUmJMuXn5/9u6yYiopuXQyGXkZGBoqIi9OzZEy4uLnBxccHOnTuxePFiuLi4wGg0ory8HMXFxXbzFRYWwmQyAQBMJlOt0ZY1r2vaXE6r1UKv19tNREREDXEo5AYNGoTMzEwcOHBAmXr16oXo6GjlZ1dXV6SmpirzZGdnIy8vD2azGQBgNpuRmZmJoqIipc3WrVuh1+sREhLSRJtFRETk4D05Ly8v3HXXXXZlHh4e8PX1VcpjY2MxadIk+Pj4QK/XIyEhAWazGb179wYADBkyBCEhIRgzZgzmz58Pi8WCadOmIT4+Hlqttok2i4iIqBEDTxqSlJQEJycnREVFwWazISIiAm+99ZZS7+zsjA0bNiAuLg5msxkeHh6IiYnB7Nmzm7orRER0i9OIiDR3JxxltVphMBhQUlLi+P05jeb6dOr3cPP9qogaTTPr5j1WZYbjx+o1ndeoXvzflUREpFoMOSIiUi2GHBERqRZDjoiIVIshR0REqsWQIyIi1WLIERGRajHkiIhItRhyRESkWgw5IiJSLYYcERGpFkOOiIhUiyFHRESqxZAjIiLVYsgREZFqMeSIiEi1GHJERKRaDDkiIlIthhwREakWQ46IiFTLoZBbunQpunXrBr1eD71eD7PZjI0bNyr1ZWVliI+Ph6+vLzw9PREVFYXCwkK7ZeTl5SEyMhLu7u7w8/PDlClTUFlZ2TRbQ0RE9BsOhVzbtm3x6quvIiMjA9988w0eeOABPPzwwzh8+DAAYOLEifjss8+wdu1a7Ny5EwUFBRg5cqQyf1VVFSIjI1FeXo49e/Zg5cqVSElJwfTp05t2q4iIiABoRESuZQE+Pj5YsGABRo0ahdatW2P16tUYNWoUAODo0aMIDg5Geno6evfujY0bN2L48OEoKCiA0WgEACxbtgwvvvgizp49Czc3t6tap9VqhcFgQElJCfR6vWMd1mgca38jubZfFdFNRTPr5j1WZYbjx+o1ndeoXo2+J1dVVYU1a9bg4sWLMJvNyMjIQEVFBcLDw5U2Xbp0Qbt27ZCeng4ASE9PR9euXZWAA4CIiAhYrVblarAuNpsNVqvVbiIiImqIwyGXmZkJT09PaLVaPPfcc1i3bh1CQkJgsVjg5uYGb29vu/ZGoxEWiwUAYLFY7AKupr6mrj7z5s2DwWBQpoCAAEe7TUREtyCHQ65z5844cOAA9u7di7i4OMTExODIkSPXo2+KxMRElJSUKFN+fv51XR8REamDi6MzuLm54Y477gAAhIaGYv/+/XjzzTfx6KOPory8HMXFxXZXc4WFhTCZTAAAk8mEffv22S2vZvRlTZu6aLVaaLVaR7tKRES3uGt+Tq66uho2mw2hoaFwdXVFamqqUpednY28vDyYzWYAgNlsRmZmJoqKipQ2W7duhV6vR0hIyLV2hYiIyI5DV3KJiYkYOnQo2rVrhwsXLmD16tX48ssvsXnzZhgMBsTGxmLSpEnw8fGBXq9HQkICzGYzevfuDQAYMmQIQkJCMGbMGMyfPx8WiwXTpk1DfHw8r9SIiKjJORRyRUVFeOKJJ3DmzBkYDAZ069YNmzdvxuDBgwEASUlJcHJyQlRUFGw2GyIiIvDWW28p8zs7O2PDhg2Ii4uD2WyGh4cHYmJiMHv27KbdKiIiIjTBc3LNgc/JEakfn5OjpsD/XUlERKrFkCMiItViyBERkWox5IiISLUYckREpFoMOSIiUi2GHBERqRZDjoiIVIshR0REqsWQIyIi1WLIERGRajHkiIhItRhyRESkWgw5IiJSLYYcERGpFkOOiIhUiyFHRESqxZAjIiLVYsgREZFqMeSIiEi1HAq5efPm4Z577oGXlxf8/PzwyCOPIDs7265NWVkZ4uPj4evrC09PT0RFRaGwsNCuTV5eHiIjI+Hu7g4/Pz9MmTIFlZWV1741REREv+FQyO3cuRPx8fH4+uuvsXXrVlRUVGDIkCG4ePGi0mbixIn47LPPsHbtWuzcuRMFBQUYOXKkUl9VVYXIyEiUl5djz549WLlyJVJSUjB9+vSm2yoiIiIAGhGRxs589uxZ+Pn5YefOnbj//vtRUlKC1q1bY/Xq1Rg1ahQA4OjRowgODkZ6ejp69+6NjRs3Yvjw4SgoKIDRaAQALFu2DC+++CLOnj0LNze3BtdrtVphMBhQUlICvV7vWKc1Goe384bR+F8V0U1HM+vmPVZlhuPH6jWd16he13RPrqSkBADg4+MDAMjIyEBFRQXCw8OVNl26dEG7du2Qnp4OAEhPT0fXrl2VgAOAiIgIWK1WHD58uM712Gw2WK1Wu4mIiKghjQ656upqTJgwAX379sVdd90FALBYLHBzc4O3t7ddW6PRCIvForT5bcDV1NfU1WXevHkwGAzKFBAQ0NhuExHRLaTRIRcfH49Dhw5hzZo1TdmfOiUmJqKkpESZ8vPzr/s6iYjo5ufSmJnGjRuHDRs2YNeuXWjbtq1SbjKZUF5ejuLiYrurucLCQphMJqXNvn377JZXM/qyps3ltFottFptY7pKRES3MIeu5EQE48aNw7p167B9+3YEBQXZ1YeGhsLV1RWpqalKWXZ2NvLy8mA2mwEAZrMZmZmZKCoqUtps3boVer0eISEh17ItREREdhy6kouPj8fq1avxySefwMvLS7mHZjAYoNPpYDAYEBsbi0mTJsHHxwd6vR4JCQkwm83o3bs3AGDIkCEICQnBmDFjMH/+fFgsFkybNg3x8fG8WiMioiblUMgtXboUADBgwAC78uTkZDz55JMAgKSkJDg5OSEqKgo2mw0RERF46623lLbOzs7YsGED4uLiYDab4eHhgZiYGMyePfvatoSIiOgy1/ScXHPhc3JE6sfn5Kgp8H9XEhGRajHkiIhItRhyRESkWgw5IiJSLYYcERGpFkOOiIhUiyFHRESqxZAjIiLVYsgREZFqMeSIiEi1GHJERKRaDDkiIlIthhwREakWQ46IiFSLIUdERKrFkCMiItViyBERkWox5IiISLUYckREpFoMOSIiUi2HQ27Xrl0YMWIE/P39odFosH79ert6EcH06dPRpk0b6HQ6hIeH44cffrBrc+7cOURHR0Ov18Pb2xuxsbEoLS29pg0hIiK6nMMhd/HiRXTv3h1Lliyps37+/PlYvHgxli1bhr1798LDwwMREREoKytT2kRHR+Pw4cPYunUrNmzYgF27dmHs2LGN3woiIqI6aEREGj2zRoN169bhkUceAXDpKs7f3x+TJ0/GCy+8AAAoKSmB0WhESkoKHnvsMWRlZSEkJAT79+9Hr169AACbNm3CsGHDcOrUKfj7+ze4XqvVCoPBgJKSEuj1ekc77Vj7G0njf1VENx3NrJv3WJUZjh+r13Reo3o16T25EydOwGKxIDw8XCkzGAwICwtDeno6ACA9PR3e3t5KwAFAeHg4nJycsHfv3jqXa7PZYLVa7SYiIqKGNGnIWSwWAIDRaLQrNxqNSp3FYoGfn59dvYuLC3x8fJQ2l5s3bx4MBoMyBQQENGW3iYhIpW6K0ZWJiYkoKSlRpvz8/ObuEhER3QSaNORMJhMAoLCw0K68sLBQqTOZTCgqKrKrr6ysxLlz55Q2l9NqtdDr9XYTERFRQ5o05IKCgmAymZCamqqUWa1W7N27F2azGQBgNptRXFyMjIwMpc327dtRXV2NsLCwpuwOERHd4lwcnaG0tBQ5OTnK6xMnTuDAgQPw8fFBu3btMGHCBMydOxcdO3ZEUFAQ/vGPf8Df318ZgRkcHIwHH3wQzz77LJYtW4aKigqMGzcOjz322FWNrCQiIrpaDofcN998g4EDByqvJ02aBACIiYlBSkoKpk6diosXL2Ls2LEoLi5Gv379sGnTJrRo0UKZZ9WqVRg3bhwGDRoEJycnREVFYfHixU2wOURERP9zTc/JNRc+J0ekfnxOjprCTTG6koiIqDEYckREpFoMOSIiUi2GHBERqRZDjoiIVIshR0REqsWQIyIi1WLIERGRajHkiIhItRhyRESkWgw5IiJSLYYcERGpFkOOiIhUiyFHRESqxZAjIiLVYsgREZFqMeSIiEi1GHJERKRaDDkiIlIthhwREalWs4XckiVLEBgYiBYtWiAsLAz79u1rrq4QEZFKNUvIffjhh5g0aRJmzJiBb7/9Ft27d0dERASKioqaoztERKRSzRJyb7zxBp599lk89dRTCAkJwbJly+Du7o533323ObpDREQq5fJ7r7C8vBwZGRlITExUypycnBAeHo709PQ657HZbLDZbMrrkpISAIDVar2+nb3R3GrbS7e2subuQOM15txUM4+INHV3bmm/e8j99NNPqKqqgtFotCs3Go04evRonfPMmzcPs2bNqlUeEBBwXfp4wzIYmrsHRHQVDK82/li9cOECDDzWm8zvHnKNkZiYiEmTJimvq6urce7cOfj6+kKj0TRjz+xZrVYEBAQgPz8fer2+ubtDRPW4EY9VEcGFCxfg7+/f3F1Rld895Fq1agVnZ2cUFhbalRcWFsJkMtU5j1arhVartSvz9va+Xl28Znq9/oY5cIiofjfascoruKb3uw88cXNzQ2hoKFJTU5Wy6upqpKamwmw2/97dISIiFWuWjysnTZqEmJgY9OrVC/feey8WLVqEixcv4qmnnmqO7hARkUo1S8g9+uijOHv2LKZPnw6LxYIePXpg06ZNtQaj3Gy0Wi1mzJhR66NVIrqx8Fi9dWiE41WJiEil+L8riYhItRhyRESkWgw5IiJSLYYcERGpFkMOwMyZM2E0GqHRaLB+/frm7o7i5MmT0Gg0OHDgwBXbDRgwABMmTPhd+kQ3r+Z6fwcGBmLRokW/+3qbysyZM9GjR4/rtvzLj/Mvv/wSGo0GxcXF122dN5LrvX9vyJCzWCxISEjA7bffDq1Wi4CAAIwYMcLuAfKmkpWVhVmzZmH58uU4c+YMhg4d2uTraKyAgACcOXMGd911F4D63/z//e9/MWfOnGbo4a3rySefhEajgUajgaurK4xGIwYPHox3330X1dXVzdq3633SaG43e2iSY3bt2oUhQ4bAx8cHrVq1wjPPPIOysqv/7903XMidPHkSoaGh2L59OxYsWIDMzExs2rQJAwcORHx8fKOXW1VVVefJJzc3FwDw8MMPw2Qy1fncTHl5eaPXey2cnZ1hMpng4nLlxxl9fHzg5eX1O/WKajz44IM4c+YMTp48iY0bN2LgwIF4/vnnMXz4cFRWVjZ396gBzXVc0yUiclXHyfbt2zFq1Cikp6fjo48+wqefforXXnvNoRXdUIYOHSq33XablJaW1qo7f/688vPChQvlrrvuEnd3d2nbtq3ExcXJhQsXlPrk5GQxGAzyySefSHBwsDg7O8uJEyfsljdjxgwBYDeJiMTExMjDDz8sc+fOlTZt2khgYKCIiOTl5cno0aPFYDBIy5Yt5aGHHqq1zBUrVkiXLl1Eq9VK586dZcmSJVfc3qqqKnnttdekQ4cO4ubmJgEBATJ37lwRETlx4oQAkO+++075+bdTTEyMiIj0799fnn/+eWWZZWVlMnnyZPH39xd3d3e59957ZceOHUr9yZMnZfjw4eLt7S3u7u4SEhIin3/++RX7SfZq3iOXS01NFQCyYsUKpezHH3+Uhx56SDw8PMTLy0tGjx4tFovFbr7169fL3XffLVqtVoKCgmTmzJlSUVEhIiLV1dUyY8YMCQgIEDc3N2nTpo0kJCTU2a/k5ORa75Pk5GQREaVfjzzyiOh0Ornjjjvkk08+sZs/MzNTHnzwQfHw8BA/Pz/585//LGfPnr3ivti9e7f069dPWrRoIW3btpWEhAS747d9+/aSlJSkvD5//rzExsZKq1atxMvLSwYOHCgHDhywW+ann34qvXr1Eq1WK76+vvLII4+IyKX3el3HrIjIf/7zHwkJCRE3Nzdp3769vP7663bLbN++vcyePVvGjBkjXl5eyvEzdepU6dixo+h0OgkKCpJp06ZJeXm5Mt+MGTOke/fuV9wHhw4dksjISPHy8hJPT0/p16+f5OTkiMilY3zWrFly2223iZubm3Tv3l02btyozPvb41xEZMeOHQLA7nzX0D4uKCiQYcOGSYsWLSQwMFBWrVrVqP3+W1FRURIfH6+8fv755wWAZGVliYiIzWYTd3d32bp1q4hcOu8kJCRI69atRavVSt++fWXfvn3K/DXb9cUXX0jPnj3F1dVVduzYUWv/5uTkSFBQkMTHx0t1dXWtfo0YMUJiY2Ov8Nuwd0OF3M8//ywajUZeeeWVBtsmJSXJ9u3b5cSJE5KamiqdO3eWuLg4pT45OVlcXV2lT58+kpaWJkePHpWLFy/aLePChQvKSeHMmTNy5swZEbl0AvP09JQxY8bIoUOH5NChQ1JeXi7BwcHy9NNPy8GDB+XIkSPypz/9STp37iw2m01ERN5//31p06aNfPzxx3L8+HH5+OOPxcfHR1JSUurdjqlTp0rLli0lJSVFcnJyZPfu3coJ8rdv/srKSvn4448FgGRnZ8uZM2ekuLhYRGqH3DPPPCN9+vSRXbt2SU5OjixYsEC0Wq0cO3ZMREQiIyNl8ODBcvDgQcnNzZXPPvtMdu7ceRW/IapRX8iJiHTv3l2GDh0qIpdOcD169JB+/frJN998I19//bWEhoZK//79lfa7du0SvV4vKSkpkpubK1u2bJHAwECZOXOmiIisXbtW9Hq9fPHFF/Ljjz/K3r175e23365z3b/88otMnjxZ7rzzTuU9/csvv4jIpZBr27atrF69Wn744QcZP368eHp6ys8//ywil06CrVu3lsTERMnKypJvv/1WBg8eLAMHDqx3P+Tk5IiHh4ckJSXJsWPHJC0tTe6++2558sknlTaXn2zDw8NlxIgRsn//fjl27JhMnjxZfH19lX5s2LBBnJ2dZfr06XLkyBE5cOCAck74+eefpW3btjJ79my7Y/abb74RJycnmT17tmRnZ0tycrLodDol4Gv6odfr5fXXX5ecnBwlhObMmSNpaWly4sQJ+fTTT8VoNMprr72mzNdQyJ06dUp8fHxk5MiRsn//fsnOzpZ3331Xjh49KiIib7zxhuj1evnggw/k6NGjMnXqVHF1dVWOx4ZC7mr2cXh4uPTo0UO+/vprycjIkP79+4tOp3Nov19u8eLFcueddyqve/ToIa1atZKlS5eKiMhXX30lrq6uynl1/Pjx4u/vL1988YUcPnxYYmJipGXLlsrya7arW7dusmXLFsnJyZGff/7Zbv9+//33YjKZ5O9//3udfdq8ebPodDqHzlc3VMjt3btXAMh///tfh+ddu3at+Pr6Kq9rwutKf6mIiKxbt87ur0GRSycwo9GohJeIyHvvvSedO3e2+8vCZrOJTqeTzZs3i4hIhw4dZPXq1XbLmjNnjpjN5jrXbbVaRavV2v3V/1tX8xeeiH3I/fjjj+Ls7CynT5+2azNo0CBJTEwUEZGuXbsqJ1BqnCuF3KOPPirBwcEiIrJlyxZxdnaWvLw8pf7w4cMCQPkrd9CgQbX+sHvvvfekTZs2InLpU4tOnTrZXV1cSX0nZQAybdo05XVpaakAUK4q5syZI0OGDLGbJz8/X/nDqi6xsbEyduxYu7Ldu3eLk5OT/PrrryJiH3K7d+8WvV4vZWVldvN06NBBli9fLiIiZrNZoqOj692+y0NTRORPf/qTDB482K5sypQpEhISYjdfzRXhlSxYsEBCQ0OV1w2FXGJiogQFBdX7+/H395eXX37Zruyee+6Rv/71ryLS8HHe0D7OysoSALJ//36l/ocffhAADu33yx08eFA0Go0UFRXJuXPnxM3NTebMmSOPPvqoiIjMnTtX+vTpIyKX3kuurq6yatUqZf7y8nLx9/eX+fPn223X+vXr7dZTs3/T0tKkZcuWta7Aa2zZskU8PDxkzZo1ddbX54b6Pjlx4D+Mbdu2DfPmzcPRo0dhtVpRWVmJsrIy/PLLL3B3dwdw6RsPunXrBgDIy8tDSEiIMv9LL72El156qd7ld+3aFW5ubsrr77//Hjk5ObXufZWVlSE3NxcXL15Ebm4uYmNj8eyzzyr1lZWV9X59RlZWFmw2GwYNGnTV292QzMxMVFVVoVOnTnblNpsNvr6+AIDx48cjLi4OW7ZsQXh4OKKiopT9RNdORJTvOczKykJAQIDdF/yGhITA29sbWVlZuOeee/D9998jLS0NL7/8stKmqqpKeT+PHj0aixYtwu23344HH3wQw4YNw4gRIxq8V1uX3/6ePTw8oNfrUVRUBODSe3zHjh3w9PSsNV9ubm6t91TNPAcPHsSqVavstr+6uhonTpxAcHBwrfalpaXKe7HGr7/+qtwfP3DggN0xdDWysrLw8MMP25X17dsXixYtQlVVFZydnQEAvXr1qjXvhx9+iMWLFyM3NxelpaWorKx06Ot3Dhw4gPvuuw+urq616qxWKwoKCtC3b99affv++++vavkN7eNjx47BxcUFPXv2VOrvuOMOtGzZ0m4ZDe33y911113w8fHBzp074ebmhrvvvhvDhw/HkiVLAAA7d+7EgAEDAFx6f1RUVNhtp6urK+69915kZWXZLbeu30FeXh4GDx6Ml19+ud6R4hMmTEBCQgIeffTROuvrc0OFXMeOHaHRaOr9hvAaJ0+exPDhwxEXF4eXX34ZPj4++OqrrxAbG4vy8nIl5HQ6nXKy8ff3txuK7+Pjc8V1eHh42L0uLS1FaGio3RutRuvWrVFaWgoAWLFiBcLCwuzqaw6wy+l0uiv2oTFKS0vh7OyMjIyMWuutOXk988wziIiIwOeff44tW7Zg3rx5WLhwIRISEpq8P7eirKwsBAUFXXX70tJSzJo1CyNHjqxV16JFCwQEBCA7Oxvbtm3D1q1b8de//hULFizAzp076zyxXsnl7TUajTIgq7S0FCNGjKjzpn6bNm3q7ftf/vIXjB8/vlZdu3bt6mzfpk0bfPnll7Xqar4j8nocFzUuP67T09MRHR2NWbNmISIiAgaDAWvWrMHChQuvepnXs79Aw/v42LFjV7WMhvb75TQaDe6//358+eWX0Gq1GDBgALp16wabzYZDhw5hz549eOGFFxzdnFq/A+DSOdTf3x8ffPABnn766Tr/yCgoKEDnzp0dXt8NFXI+Pj6IiIjAkiVLMH78+Fo7o7i4GN7e3sjIyEB1dTUWLlwIJ6dLA0Q/+uijKy7bxcUFd9xxR6P71rNnT3z44Yfw8/Or8xdgMBjg7++P48ePIzo6+qqW2bFjR+h0OqSmpuKZZ55psH3NlWVVVVW9be6++25UVVWhqKgI9913X73tAgIC8Nxzz+G5555DYmIiVqxYwZBrAtu3b0dmZiYmTpwIAAgODkZ+fj7y8/OVq7kjR46guLhY+WShZ8+eyM7OvuL7U6fTYcSIERgxYgTi4+PRpUsXZGZm2v31XsPNze2K75H69OzZEx9//DECAwOv+iqxZ8+eOHLkyFUfWz179oTFYoGLiwsCAwPrbNOtWzekpqbW+9VbdW1fcHAw0tLS7MrS0tLQqVOnev/IBIA9e/agffv2+Pvf/66U/fjjj1e1Lb/t78qVK1FRUVHrjwi9Xg9/f3+kpaWhf//+dn279957r2r5De3jzp07o7KyEt999x1CQ0MBADk5OTh//rzdMhra73Xp378/VqxYAa1Wi5dffhlOTk64//77sWDBAthsNuXKrUOHDnBzc0NaWhrat28PAKioqMD+/fuv6hlenU6HDRs2YNiwYYiIiMCWLVtqfWq2Y8cOu09ErppDH27+DnJzc8VkMklISIj85z//kWPHjsmRI0fkzTfflC5duoiIyIEDBwSALFq0SHJzc+Xf//633HbbbXafY9eMrmxIfffkLr/fcvHiRenYsaMMGDBAdu3aJcePH5cdO3ZIQkKC5Ofni8ilkZU6nU7efPNNyc7OloMHD8q7774rCxcurHf9M2fOlJYtW8rKlSslJydH0tPT5Z133hGR2p/Vnzp1SjQajaSkpEhRUZEymvTygSfR0dESGBioDIDZu3evvPLKK7JhwwYRuTRKatOmTXL8+HHJyMiQsLAw+eMf/9jgvqL/iYmJkQcffFDOnDkjp06dkoyMDHn55ZfF09NThg8fLpWVlSJyaWRkjx495L777pOMjAzZu3dvrYEnmzZtEhcXF5k5c6YcOnRIjhw5Ih988IFy8z05OVneeecdyczMlNzcXJk2bZrodDr56aef6uzbqlWrxMPDQ7777js5e/asch8GgKxbt86urcFgUAZnnD59Wlq3bi2jRo2Sffv2SU5OjmzatEmefPJJZXsu9/3334tOp5P4+Hj57rvv5NixY7J+/Xq7UXm/vYdWXV0t/fr1k+7du8vmzZvlxIkTkpaWJi+99JJyT2nHjh3i5OSkDDw5ePCgvPrqq8ryBg8eLA899JCcOnVKGfmZkZFhN/AkJSWlzoEnl9/L++STT8TFxUU++OADycnJkTfffFN8fHzszh0N3ZP76aefxNfXVxl4cuzYMfn3v/+tDDxJSkoSvV4va9askaNHj8qLL77o0MCTq9nH4eHh0rNnT9m7d698++23MnDgQNHpdLJo0aKr3u91OXDggGg0GtFqtcr5JikpSZydnaV37952bZ9//nnx9/eXjRs32g08OXfuXJ3bVdf+vXDhgvTr10/69u1rN1peRKRz586NGq9xw4WcyKXhsPHx8dK+fXtxc3OT2267TR566CG7YfBvvPGGtGnTRnQ6nURERMi///3v6xpyIiJnzpyRJ554Qlq1aiVarVZuv/12efbZZ6WkpERps2rVKunRo4e4ublJy5Yt5f7777/iL6aqqkrmzp0r7du3F1dXV2nXrp0yCOHyN7+IyOzZs8VkMolGo6n3EYLy8nKZPn26BAYGiqurq7Rp00b+8Ic/yMGDB0VEZNy4cdKhQwfRarXSunVrGTNmTL0nTKpbTEyMMoTdxcVFWrduLeHh4fLuu+9KVVWVXdureYRg06ZN0qdPH9HpdKLX6+Xee+9VRlCuW7dOwsLCRK/Xi4eHh/Tu3Vu2bdtWb9/KysokKipKvL29az1CcKWQExE5duyY/OEPfxBvb2/R6XTSpUsXmTBhQp1DuWvs27dPBg8eLJ6enuLh4SHdunWzG2hxebhYrVZJSEgQf39/cXV1lYCAAImOjrYbnPPxxx8rx1GrVq1k5MiRSl16erp069ZNtFptnY8Q1BxHCxYssOtnXSEncmmAiq+vr3h6esqjjz4qSUlJDoWcyKUgGjJkiLi7u4uXl5fcd999kpubKyKXjvGZM2fKbbfdJq6uro16hKChfVxQUCBDhw4VrVYr7du3l9WrV4ufn58sW7bMof1+uaqqKmnZsqWEhYUpZd99950AkL/97W92bX/99VdJSEhQzo/1PUJwpZATuRR0ffr0kfvvv9/uMYnfvpcdwe+TIyJSmVOnTiEgIADbtm1r0oFtNyOGHBHRTW779u0oLS1F165dcebMGUydOhWnT5/GsWPHHB6cpDY31MATIiJyXEVFBV566SUcP34cXl5e6NOnD1atWnXLBxzAKzkiIlKxG+4fNBMRETUVhhwREakWQ46IiFSLIUdERKrFkCMiItViyBERkWox5IiISLUYckREpFr/B4R7hl52qItSAAAAAElFTkSuQmCC",
            "text/plain": [
              "<Figure size 400x300 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "fig, ax = plt.subplots(figsize=(4,3))\n",
        "fig.suptitle(\"Prompts\")\n",
        "ax.bar(\n",
        "    x=[\"Car-free cities\",\"Does the electoral college work?\"],\n",
        "    height=prompts,\n",
        "    width=0.2,\n",
        "    color=[\"red\",\"green\"],\n",
        "    align=\"center\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yHzxLez5dYKM"
      },
      "source": [
        "## Handle imbalanced Data\n",
        "\n",
        "> Because the class distribution is not balanced, most machine learning algorithms will perform\n",
        "poorly and require modification to avoid simply predicting the majority class in all cases.\n",
        "Additionally, metrics like classification accuracy lose their meaning and alternate methods for\n",
        "evaluating predictions on imbalanced examples are required, like ROC area under curve. This is\n",
        "the foundational challenge of imbalanced classification.\n",
        ">\n",
        "> - Jason Brownlee, *Imbalanced Classification with Python* (2020)\n",
        "\n",
        "> If we think about feature space spatially, we might like all examples in one class to be located on one part\n",
        "of the space, and those from the other class to appear in another part of the space. [...]\n",
        ">This is rarely the case, and it is more likely that each class has multiple **concepts** resulting in multiple different groups or clusters of examples in feature space.\n",
        ">\n",
        "> - Jason Brownlee, *Imbalanced Classification with Python* (2020)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-53GminWdYKM"
      },
      "source": [
        "### Metrics for Imbalanced Data\n",
        "\n",
        ">Although widely used, classification accuracy is almost universally inappropriate for imbalanced classification. The reason is, a high accuracy (or low error) is achievable by a no skill model that only predicts the majority class. [...]\n",
        ">\n",
        "> - Jason Brownlee, *Imbalanced Classification with Python* (2020)\n",
        "\n",
        ">There are two groups of metrics that may be useful for imbalanced classification because they focus on one class; they are sensitivity-specificity and precision-recall.\n",
        ">\n",
        "> - Jason Brownlee, *Imbalanced Classification with Python* (2020)\n",
        "\n",
        ">One limitation of these metrics is that they assume that the class distribution observed in the training dataset will match the distribution in the test set and in real data when the model is used to make predictions. [...] Ranking metrics donâ€™t make any assumptions about class distributions.\n",
        ">\n",
        "> - Jason Brownlee, *Imbalanced Classification with Python* (2020)\n",
        "\n",
        ">The most commonly used ranking metric is the ROC Curve or ROC Analysis. ROC is an\n",
        "acronym that means Receiver Operating Characteristic and summarizes a field of study for\n",
        "analyzing binary classifiers based on their ability to discriminate classes.\n",
        ">\n",
        "> - Jason Brownlee, *Imbalanced Classification with Python* (2020)\n",
        "\n",
        ">Although generally effective, the ROC Curve and ROC AUC can be optimistic under a severe class imbalance, especially when the number of examples in the minority class is small.\n",
        "An alternative to the ROC Curve is the precision-recall curve that can be used in a similar way, although focuses on the performance of the classifier on the minority class.\n",
        ">\n",
        "> - Jason Brownlee, *Imbalanced Classification with Python* (2020)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5EjLehn3dYKN"
      },
      "source": [
        "#### ROC AUC curve"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "FvEDVQHVdYKN"
      },
      "outputs": [],
      "source": [
        "def ROCcurves(model,x_val,y_val):\n",
        "    fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(10, 4))\n",
        "    y_pred = model.predict(x_val).ravel()\n",
        "    \n",
        "    fpr, tpr , _ = roc_curve(y_val,  y_pred)\n",
        "    auc_ss = auc(fpr,tpr)\n",
        "    precision, recall, _ = precision_recall_curve(y_val,  y_pred)\n",
        "    auc_pr = auc(recall, precision)\n",
        "\n",
        "    # Primer subgrÃ¡fico\n",
        "    axs[0].plot([0, 1], [0, 1], color='gray', lw=1, linestyle='--', label='AUC = 0.50')\n",
        "    axs[0].plot(fpr,tpr, label='ROC curve', color='blue')\n",
        "    axs[0].set_title(f'ROC curve = {round(auc_ss,3)}')\n",
        "    axs[0].set_xlabel('FalsePositiveRate')\n",
        "    axs[0].set_ylabel('TruePositiveRate')\n",
        "    axs[0].legend()\n",
        "    # Segundo subgrÃ¡fico\n",
        "    axs[1].plot([0, 1], [0,0], color='gray', lw=1, linestyle='--', label='AUC = 0.0')\n",
        "    axs[1].plot(recall, precision, label='ROC-RP curve', color='orange')\n",
        "    axs[1].set_title(f'ROC-RP curve = {round(auc_pr,3)}')\n",
        "    axs[1].set_xlabel('Recall')\n",
        "    axs[1].set_ylabel('Precision')\n",
        "    axs[1].legend()\n",
        "    plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Rqz4YundYKN"
      },
      "source": [
        "### Data Sampling for imbalanced data\n",
        "\n",
        "> Sadly, k-fold cross-validation is not appropriate for evaluating imbalanced classifiers.\n",
        "[...]\n",
        "The reason is that the data is split into k-folds with a uniform probability distribution. This might work fine for data with a balanced class distribution, but when the distribution is severely skewed, it is likely that one or more folds will have few or no examples from the minority class.\n",
        ">\n",
        "> - Jason Brownlee, *Imbalanced Classification with Python* (2020)\n",
        "\n",
        "> The solution is to not split the data randomly when using k-fold cross-validation or a train-test\n",
        "split. [...] For example, we can use a version of k-fold cross-validation that preserves the imbalanced class distribution in each fold. It is called stratified k-fold cross-validation and will enforce the class distribution in each split of the data to match the distribution in the complete training dataset.\n",
        ">\n",
        "> - Jason Brownlee, *Imbalanced Classification with Python* (2020)\n",
        "\n",
        "> Sampling is only performed on the training dataset, the dataset used by an algorithm to\n",
        "learn a model. It is not performed on the holdout test or validation dataset.\n",
        ">\n",
        "> - Jason Brownlee, *Imbalanced Classification with Python* (2020)\n",
        "\n",
        "- Stratified k-folding CrossValidation\n",
        "- Random Downsampling\n",
        "- Adding data as kind of Oversampling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6c8aPmEOdYKN"
      },
      "source": [
        "## Adding new Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BNsGXoKYdYKO"
      },
      "source": [
        "Describing the imbalance of this dataset in terms of ration is 1:500. The dataset presents sever imbalance. Previous aproches using only 20 new LLM generated examples manually and random Downsampling technic, didn't reach a higher score than 0.56. \n",
        "\n",
        "Concluding that more new data is needed, i downloaded data shared by competitors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "downloaded_data_1[\"prompt_id\"] = downloaded_data_1[\"prompt_name\"].apply(lambda name : 0 if name == \"Car-free cities\" else 1 if name == \"Does the electoral college work?\" else 21 )\n",
        "downloaded_data_1 = downloaded_data_1[[\"prompt_id\",\"text\",\"label\"]].rename(columns={\"label\":\"generated\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "new_data = pd.concat([custom_data,downloaded_data_1],axis=0,ignore_index=True)\n",
        "new_data[\"id\"] = range(0,new_data.shape[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>prompt_id</th>\n",
              "      <th>text</th>\n",
              "      <th>generated</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>The Advantages of Limiting Car Usage in Suburb...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>Paris' Driving Ban: A Temporary Solution to En...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>Bogota's Car-Free Day: A Model for Sustainable...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>Shifting Trends: The Decline of Car Culture in...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>The End of Car Culture and the Rise of Sustain...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>65525</th>\n",
              "      <td>65525</td>\n",
              "      <td>1</td>\n",
              "      <td>\\n\\nDear Senator,\\n\\nI am writing to you regar...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>65526</th>\n",
              "      <td>65526</td>\n",
              "      <td>1</td>\n",
              "      <td>\\n\\nI remember the day distinctively. I was si...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>65527</th>\n",
              "      <td>65527</td>\n",
              "      <td>1</td>\n",
              "      <td>\\n\\nDear Senator, \\n\\nI am writing this letter...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>65528</th>\n",
              "      <td>65528</td>\n",
              "      <td>1</td>\n",
              "      <td>\\n\\nDear Senator,\\n\\nI am writing to urge you ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>65529</th>\n",
              "      <td>65529</td>\n",
              "      <td>1</td>\n",
              "      <td>\\n\\nIt was a typical summer afternoon in my ho...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>65530 rows Ã— 4 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "          id  prompt_id                                               text  \\\n",
              "0          0          0  The Advantages of Limiting Car Usage in Suburb...   \n",
              "1          1          0  Paris' Driving Ban: A Temporary Solution to En...   \n",
              "2          2          0  Bogota's Car-Free Day: A Model for Sustainable...   \n",
              "3          3          0  Shifting Trends: The Decline of Car Culture in...   \n",
              "4          4          0  The End of Car Culture and the Rise of Sustain...   \n",
              "...      ...        ...                                                ...   \n",
              "65525  65525          1  \\n\\nDear Senator,\\n\\nI am writing to you regar...   \n",
              "65526  65526          1  \\n\\nI remember the day distinctively. I was si...   \n",
              "65527  65527          1  \\n\\nDear Senator, \\n\\nI am writing this letter...   \n",
              "65528  65528          1  \\n\\nDear Senator,\\n\\nI am writing to urge you ...   \n",
              "65529  65529          1  \\n\\nIt was a typical summer afternoon in my ho...   \n",
              "\n",
              "       generated  \n",
              "0              1  \n",
              "1              1  \n",
              "2              1  \n",
              "3              1  \n",
              "4              1  \n",
              "...          ...  \n",
              "65525          1  \n",
              "65526          1  \n",
              "65527          1  \n",
              "65528          1  \n",
              "65529          1  \n",
              "\n",
              "[65530 rows x 4 columns]"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "new_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "nahQnHEBdYKO",
        "outputId": "a45cd2ec-6504-406e-c184-ef921820c04d"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>prompt_id</th>\n",
              "      <th>text</th>\n",
              "      <th>generated</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0059830c</td>\n",
              "      <td>0</td>\n",
              "      <td>Cars. Cars have been around since they became ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>005db917</td>\n",
              "      <td>0</td>\n",
              "      <td>Transportation is a large necessity in most co...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>008f63e3</td>\n",
              "      <td>0</td>\n",
              "      <td>\"America's love affair with it's vehicles seem...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>00940276</td>\n",
              "      <td>0</td>\n",
              "      <td>How often do you ride in a car? Do you drive a...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>00c39458</td>\n",
              "      <td>0</td>\n",
              "      <td>Cars are a wonderful thing. They are perhaps o...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>66903</th>\n",
              "      <td>65525</td>\n",
              "      <td>1</td>\n",
              "      <td>\\n\\nDear Senator,\\n\\nI am writing to you regar...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>66904</th>\n",
              "      <td>65526</td>\n",
              "      <td>1</td>\n",
              "      <td>\\n\\nI remember the day distinctively. I was si...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>66905</th>\n",
              "      <td>65527</td>\n",
              "      <td>1</td>\n",
              "      <td>\\n\\nDear Senator, \\n\\nI am writing this letter...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>66906</th>\n",
              "      <td>65528</td>\n",
              "      <td>1</td>\n",
              "      <td>\\n\\nDear Senator,\\n\\nI am writing to urge you ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>66907</th>\n",
              "      <td>65529</td>\n",
              "      <td>1</td>\n",
              "      <td>\\n\\nIt was a typical summer afternoon in my ho...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>66908 rows Ã— 4 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "             id  prompt_id                                               text  \\\n",
              "0      0059830c          0  Cars. Cars have been around since they became ...   \n",
              "1      005db917          0  Transportation is a large necessity in most co...   \n",
              "2      008f63e3          0  \"America's love affair with it's vehicles seem...   \n",
              "3      00940276          0  How often do you ride in a car? Do you drive a...   \n",
              "4      00c39458          0  Cars are a wonderful thing. They are perhaps o...   \n",
              "...         ...        ...                                                ...   \n",
              "66903     65525          1  \\n\\nDear Senator,\\n\\nI am writing to you regar...   \n",
              "66904     65526          1  \\n\\nI remember the day distinctively. I was si...   \n",
              "66905     65527          1  \\n\\nDear Senator, \\n\\nI am writing this letter...   \n",
              "66906     65528          1  \\n\\nDear Senator,\\n\\nI am writing to urge you ...   \n",
              "66907     65529          1  \\n\\nIt was a typical summer afternoon in my ho...   \n",
              "\n",
              "       generated  \n",
              "0              0  \n",
              "1              0  \n",
              "2              0  \n",
              "3              0  \n",
              "4              0  \n",
              "...          ...  \n",
              "66903          1  \n",
              "66904          1  \n",
              "66905          1  \n",
              "66906          1  \n",
              "66907          1  \n",
              "\n",
              "[66908 rows x 4 columns]"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "target_data = pd.concat([initial_dataset,new_data],ignore_index=True,axis=0)\n",
        "target_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Data cleaning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(66908, 4)\n",
            "(65354, 4)\n"
          ]
        }
      ],
      "source": [
        "print(target_data.shape)\n",
        "target_data.drop_duplicates(subset=[\"text\"],inplace=True,keep=\"first\")\n",
        "target_data = target_data.dropna()\n",
        "target_data.reset_index(inplace=True,drop=True)\n",
        "print(target_data.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "65Y4787kdYKP"
      },
      "source": [
        "## Data Feature Engineering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "chXVmCzYdYKP"
      },
      "source": [
        "### Training a Tokenizer\n",
        "Reading the competition discussions lead me to this [Notebook](https://www.kaggle.com/code/datafan07/train-your-own-tokenizer), where is suggested to add words with typos into the vocabulary for better performance by training a tokenizer.\n",
        "\n",
        "1. normalization\n",
        "2. pre-tokenization\n",
        "3. model\n",
        "4. post-processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ">ByteLevel: \n",
        ">\n",
        ">Splits on whitespaces while remapping all the bytes to a set of visible characters. This technique as been introduced by OpenAI with GPT-2 and has some more or less nice properties:\n",
        "> - Since it maps on bytes, a tokenizer using this only requires 256 characters as initial alphabet (the number of values a byte can have), as opposed to the 130,000+ Unicode characters.\n",
        "> - A consequence of the previous point is that it is absolutely unnecessary to have an unknown token using this since we can represent anything with 256 tokens (Youhou!! ðŸŽ‰ðŸŽ‰)\n",
        "> - For non ascii characters, it gets completely unreadable, but it works nonetheless!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Creating Byte-Pair Encoding tokenizer\n",
        "raw_tokenizer = Tokenizer(models.BPE(unk_token=\"[UNK]\"))\n",
        "#Cleaning\n",
        "raw_tokenizer.normalizer =  normalizers.Sequence(\n",
        "    [\n",
        "        normalizers.NFC(),\n",
        "        normalizers.Lowercase(),\n",
        "        normalizers.Replace(\"\\n\",\" \"),\n",
        "        normalizers.Replace(\"\\r\",\" \"),\n",
        "        normalizers.Replace(\"\\t\",\" \")\n",
        "    ]    \n",
        "    )\n",
        "#First tokenization\n",
        "raw_tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel()\n",
        "#Training\n",
        "special_tokens = [\"[UNK]\", \"[PAD]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"]\n",
        "trainer = trainers.BpeTrainer(show_progress=True,special_tokens=special_tokens)\n",
        "\n",
        "def data_iter(dataset):\n",
        "    \"\"\"\n",
        "    A generator function for iterating over a dataset in chunks.\n",
        "    \"\"\"    \n",
        "    for i in range(0, len(dataset), 1000):\n",
        "        yield dataset[i : i + 1000][\"text\"]\n",
        "\n",
        "raw_tokenizer.train_from_iterator(data_iter(target_data),trainer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "tokenizer = PreTrainedTokenizerFast(\n",
        "    tokenizer_object=raw_tokenizer,\n",
        "    unk_token=\"[UNK]\",\n",
        "    pad_token=\"[PAD]\",\n",
        "    cls_token=\"[CLS]\",\n",
        "    sep_token=\"[SEP]\",\n",
        "    mask_token=\"[MASK]\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('../data/weight/preTrainedTokenizer/tokenizer_config.json',\n",
              " '../data/weight/preTrainedTokenizer/special_tokens_map.json',\n",
              " '../data/weight/preTrainedTokenizer/tokenizer.json')"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer.save_pretrained(\"../data/weight/preTrainedTokenizer\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"martÃ­n's bag\""
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "raw_tokenizer.normalizer.normalize_str(\"MartÃ­n's bag\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Ä mart', 'Ãƒ', 'Åƒ', 'n', \"'s\", 'Ä bag']"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer.tokenize(\"MartÃ­n's bag\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MkOM8WoAdYKP"
      },
      "source": [
        "##### Words Tokenizing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "AGd5b7vMdYKP",
        "outputId": "fcebe33a-2999-4ca1-d3ae-21c47918eb45"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>prompt_id</th>\n",
              "      <th>text</th>\n",
              "      <th>generated</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0059830c</td>\n",
              "      <td>0</td>\n",
              "      <td>[Ä cars, ., Ä cars, Ä have, Ä been, Ä around, Ä sinc...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>005db917</td>\n",
              "      <td>0</td>\n",
              "      <td>[Ä transportation, Ä is, Ä a, Ä large, Ä necessity,...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>008f63e3</td>\n",
              "      <td>0</td>\n",
              "      <td>[Ä \", america, 's, Ä love, Ä affair, Ä with, Ä it, ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>00940276</td>\n",
              "      <td>0</td>\n",
              "      <td>[Ä how, Ä often, Ä do, Ä you, Ä ride, Ä in, Ä a, Ä car...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>00c39458</td>\n",
              "      <td>0</td>\n",
              "      <td>[Ä cars, Ä are, Ä a, Ä wonderful, Ä thing, ., Ä they...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>65349</th>\n",
              "      <td>65525</td>\n",
              "      <td>1</td>\n",
              "      <td>[Ä , Ä dear, Ä senator, ,, Ä , Ä i, Ä am, Ä writing, ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>65350</th>\n",
              "      <td>65526</td>\n",
              "      <td>1</td>\n",
              "      <td>[Ä , Ä i, Ä remember, Ä the, Ä day, Ä distinctively,...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>65351</th>\n",
              "      <td>65527</td>\n",
              "      <td>1</td>\n",
              "      <td>[Ä , Ä dear, Ä senator, ,, Ä Ä , Ä i, Ä am, Ä writing,...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>65352</th>\n",
              "      <td>65528</td>\n",
              "      <td>1</td>\n",
              "      <td>[Ä , Ä dear, Ä senator, ,, Ä , Ä i, Ä am, Ä writing, ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>65353</th>\n",
              "      <td>65529</td>\n",
              "      <td>1</td>\n",
              "      <td>[Ä , Ä it, Ä was, Ä a, Ä typical, Ä summer, Ä afterno...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>65354 rows Ã— 4 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "             id  prompt_id                                               text  \\\n",
              "0      0059830c          0  [Ä cars, ., Ä cars, Ä have, Ä been, Ä around, Ä sinc...   \n",
              "1      005db917          0  [Ä transportation, Ä is, Ä a, Ä large, Ä necessity,...   \n",
              "2      008f63e3          0  [Ä \", america, 's, Ä love, Ä affair, Ä with, Ä it, ...   \n",
              "3      00940276          0  [Ä how, Ä often, Ä do, Ä you, Ä ride, Ä in, Ä a, Ä car...   \n",
              "4      00c39458          0  [Ä cars, Ä are, Ä a, Ä wonderful, Ä thing, ., Ä they...   \n",
              "...         ...        ...                                                ...   \n",
              "65349     65525          1  [Ä , Ä dear, Ä senator, ,, Ä , Ä i, Ä am, Ä writing, ...   \n",
              "65350     65526          1  [Ä , Ä i, Ä remember, Ä the, Ä day, Ä distinctively,...   \n",
              "65351     65527          1  [Ä , Ä dear, Ä senator, ,, Ä Ä , Ä i, Ä am, Ä writing,...   \n",
              "65352     65528          1  [Ä , Ä dear, Ä senator, ,, Ä , Ä i, Ä am, Ä writing, ...   \n",
              "65353     65529          1  [Ä , Ä it, Ä was, Ä a, Ä typical, Ä summer, Ä afterno...   \n",
              "\n",
              "       generated  \n",
              "0              0  \n",
              "1              0  \n",
              "2              0  \n",
              "3              0  \n",
              "4              0  \n",
              "...          ...  \n",
              "65349          1  \n",
              "65350          1  \n",
              "65351          1  \n",
              "65352          1  \n",
              "65353          1  \n",
              "\n",
              "[65354 rows x 4 columns]"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenized_dataset = target_data.copy()\n",
        "tokenized_dataset[\"text\"] = target_data[\"text\"].apply(lambda x : tokenizer.tokenize(x))\n",
        "tokenized_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Deleting stopwords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [],
      "source": [
        "tokenized_stopwords=[token for word in stopwords.words('english') for token in tokenizer.tokenize(word)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [],
      "source": [
        "new_docs = []\n",
        "for doc in tokenized_dataset[\"text\"]:\n",
        "    tokens = [word for word in doc if word not in tokenized_stopwords]  # Eliminar stopwords\n",
        "    new_docs.append(tokens)\n",
        "tokenized_dataset[\"text\"]=new_docs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XNWEuYg-dYKT"
      },
      "source": [
        "#### Text to document embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Need to add all the test documents to the training phase of embedding."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OLUZL3pUdYKT",
        "outputId": "10d64093-182b-44e6-f64d-752c3f1176e6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0        ([Ä cars, ., Ä cars, Ä around, Ä since, Ä became, Ä ...\n",
              "1        ([Ä transportation, Ä large, Ä necessity, Ä countr...\n",
              "2        ([Ä \", america, Ä love, Ä affair, Ä vehicles, Ä see...\n",
              "3        ([Ä often, Ä ride, Ä car, ?, Ä drive, Ä one, Ä motor...\n",
              "4        ([Ä cars, Ä wonderful, Ä thing, ., Ä perhaps, Ä one...\n",
              "                               ...                        \n",
              "65349    ([Ä , Ä dear, Ä senator, ,, Ä , Ä writing, Ä regardi...\n",
              "65350    ([Ä , Ä remember, Ä day, Ä distinctively, ., Ä sitt...\n",
              "65351    ([Ä , Ä dear, Ä senator, ,, Ä Ä , Ä writing, Ä letter...\n",
              "65352    ([Ä , Ä dear, Ä senator, ,, Ä , Ä writing, Ä urge, Ä ...\n",
              "65353    ([Ä , Ä typical, Ä summer, Ä afternoon, Ä hometown,...\n",
              "Name: text, Length: 65354, dtype: object"
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "docs_dataset = tokenized_dataset.copy()\n",
        "docs_dataset[\"text\"] = [doc2vec.TaggedDocument(row[2],[row[0]]) for row in tokenized_dataset.values]\n",
        "docs_dataset[\"text\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [],
      "source": [
        "doc_model = doc2vec.Doc2Vec(documents=docs_dataset[\"text\"],vector_size=100,epochs=50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KBGZJYB0dYKT",
        "outputId": "154280e2-5dcc-4c7c-95e1-3f66b5bd4ac0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([ 1.7122887e+00,  6.4376479e-01, -3.6984873e-01, -3.5171069e-02,\n",
              "       -1.6545638e-03, -3.0004475e-01,  4.4553584e-01,  2.2844608e-01,\n",
              "       -2.2074780e+00,  9.4568712e-01,  9.6694887e-01, -3.5704130e-01,\n",
              "        3.7102431e-01,  3.3260074e-02, -1.0338900e+00, -1.0137997e+00,\n",
              "       -3.9622423e-01, -1.0669718e+00, -1.7820451e-01,  1.8421041e+00,\n",
              "        2.4162188e+00,  7.2322923e-01,  4.4896686e-01, -2.8433007e-01,\n",
              "        2.5232496e+00, -4.0168989e-01,  2.4442642e+00, -3.6718759e-01,\n",
              "       -1.3891307e+00,  2.8867376e-01,  2.4770258e-02, -3.8019669e-01,\n",
              "        2.5873682e+00, -1.0954715e+00, -3.6907178e-01,  6.7538804e-01,\n",
              "        6.8154478e-01,  1.3484803e+00, -3.6409205e-01,  5.5818353e-02,\n",
              "        7.4111825e-01, -1.3243847e+00,  7.3321122e-01,  1.2327325e+00,\n",
              "        1.4471658e+00, -1.4210614e+00, -6.8102854e-01,  3.0805138e-01,\n",
              "        8.8233942e-01,  2.3992360e+00,  1.6603440e+00, -8.4164333e-01,\n",
              "        1.1166921e+00, -7.5444263e-01,  5.8542472e-01,  1.3018838e-01,\n",
              "        2.5091201e-01,  1.4896506e+00, -6.8724416e-02,  9.6145833e-01,\n",
              "        7.2548562e-01,  2.8189286e-03,  7.3955053e-01,  3.7419450e-01,\n",
              "       -3.1722543e-01,  5.8845586e-01,  1.4777931e+00,  3.0775085e-02,\n",
              "       -4.6790996e-01,  1.2686594e+00,  8.0532682e-01,  6.0756552e-01,\n",
              "       -5.0517488e-01, -4.9620610e-01, -1.5412235e+00, -1.0754418e+00,\n",
              "        2.4187127e-01, -3.0442796e+00,  1.2512511e+00,  1.6363554e+00,\n",
              "       -7.8351313e-01,  1.4025778e+00,  2.0592171e-01, -7.4265969e-01,\n",
              "        8.1952763e-01, -2.6688533e+00, -3.3805305e-01,  6.7772746e-01,\n",
              "        1.6300462e+00, -6.5132029e-02, -5.1967001e-01,  2.1592607e+00,\n",
              "       -1.3318032e-01,  1.2299901e+00,  5.7349163e-01, -1.1714872e+00,\n",
              "       -8.9359283e-01, -5.7253796e-01,  3.1727667e+00,  3.6741728e-01],\n",
              "      dtype=float32)"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "doc_model.dv[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [],
      "source": [
        "doc_model.save(\"../data/weight/EmbeddingModel.bin\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 444
        },
        "id": "Ke4rS-oJdYKi",
        "outputId": "a6efe2c2-57d9-4854-df41-a381597c5c2f"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>90</th>\n",
              "      <th>91</th>\n",
              "      <th>92</th>\n",
              "      <th>93</th>\n",
              "      <th>94</th>\n",
              "      <th>95</th>\n",
              "      <th>96</th>\n",
              "      <th>97</th>\n",
              "      <th>98</th>\n",
              "      <th>99</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.609158</td>\n",
              "      <td>0.115282</td>\n",
              "      <td>0.003429</td>\n",
              "      <td>0.796281</td>\n",
              "      <td>-0.175308</td>\n",
              "      <td>-0.486566</td>\n",
              "      <td>-1.483976</td>\n",
              "      <td>0.610219</td>\n",
              "      <td>0.611135</td>\n",
              "      <td>0.610774</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.438418</td>\n",
              "      <td>0.318527</td>\n",
              "      <td>1.803023</td>\n",
              "      <td>-0.366931</td>\n",
              "      <td>1.768442</td>\n",
              "      <td>2.428774</td>\n",
              "      <td>-0.465209</td>\n",
              "      <td>0.156457</td>\n",
              "      <td>-1.924398</td>\n",
              "      <td>-0.263324</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-0.173142</td>\n",
              "      <td>-0.932471</td>\n",
              "      <td>-0.457326</td>\n",
              "      <td>-0.522826</td>\n",
              "      <td>0.837405</td>\n",
              "      <td>-0.439883</td>\n",
              "      <td>-0.709800</td>\n",
              "      <td>2.101081</td>\n",
              "      <td>0.949970</td>\n",
              "      <td>0.753548</td>\n",
              "      <td>...</td>\n",
              "      <td>-3.573737</td>\n",
              "      <td>1.148647</td>\n",
              "      <td>-0.340055</td>\n",
              "      <td>1.314282</td>\n",
              "      <td>-1.654844</td>\n",
              "      <td>0.863783</td>\n",
              "      <td>-0.723793</td>\n",
              "      <td>0.388550</td>\n",
              "      <td>0.416612</td>\n",
              "      <td>-0.155420</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-2.559303</td>\n",
              "      <td>-0.187480</td>\n",
              "      <td>1.909480</td>\n",
              "      <td>0.879880</td>\n",
              "      <td>1.636030</td>\n",
              "      <td>-1.265262</td>\n",
              "      <td>-1.318124</td>\n",
              "      <td>1.173769</td>\n",
              "      <td>-1.989154</td>\n",
              "      <td>0.644037</td>\n",
              "      <td>...</td>\n",
              "      <td>-2.891630</td>\n",
              "      <td>1.337717</td>\n",
              "      <td>1.740497</td>\n",
              "      <td>-0.410580</td>\n",
              "      <td>-2.490496</td>\n",
              "      <td>0.056314</td>\n",
              "      <td>0.013203</td>\n",
              "      <td>-0.810805</td>\n",
              "      <td>-0.058793</td>\n",
              "      <td>-1.404608</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-1.717762</td>\n",
              "      <td>-0.859337</td>\n",
              "      <td>-1.955419</td>\n",
              "      <td>0.207806</td>\n",
              "      <td>1.102318</td>\n",
              "      <td>-1.585540</td>\n",
              "      <td>-2.027091</td>\n",
              "      <td>0.885350</td>\n",
              "      <td>-1.542764</td>\n",
              "      <td>-1.024008</td>\n",
              "      <td>...</td>\n",
              "      <td>-1.225380</td>\n",
              "      <td>3.573152</td>\n",
              "      <td>1.406274</td>\n",
              "      <td>0.703647</td>\n",
              "      <td>-0.404810</td>\n",
              "      <td>0.540573</td>\n",
              "      <td>-0.423621</td>\n",
              "      <td>-0.617609</td>\n",
              "      <td>1.511330</td>\n",
              "      <td>-2.336802</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-2.287486</td>\n",
              "      <td>-0.918230</td>\n",
              "      <td>1.266380</td>\n",
              "      <td>1.769894</td>\n",
              "      <td>-0.253977</td>\n",
              "      <td>-1.897076</td>\n",
              "      <td>-2.473712</td>\n",
              "      <td>2.277956</td>\n",
              "      <td>-0.451920</td>\n",
              "      <td>0.927585</td>\n",
              "      <td>...</td>\n",
              "      <td>0.808325</td>\n",
              "      <td>1.678861</td>\n",
              "      <td>-0.968123</td>\n",
              "      <td>-1.490145</td>\n",
              "      <td>1.825365</td>\n",
              "      <td>-0.323188</td>\n",
              "      <td>0.085665</td>\n",
              "      <td>0.905258</td>\n",
              "      <td>0.341302</td>\n",
              "      <td>-1.070348</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>65349</th>\n",
              "      <td>-0.023224</td>\n",
              "      <td>-0.073207</td>\n",
              "      <td>0.048462</td>\n",
              "      <td>-0.651785</td>\n",
              "      <td>1.660518</td>\n",
              "      <td>1.043009</td>\n",
              "      <td>0.861014</td>\n",
              "      <td>-1.982811</td>\n",
              "      <td>-0.330726</td>\n",
              "      <td>0.566884</td>\n",
              "      <td>...</td>\n",
              "      <td>0.227098</td>\n",
              "      <td>-0.122984</td>\n",
              "      <td>-0.349267</td>\n",
              "      <td>0.802841</td>\n",
              "      <td>-1.017031</td>\n",
              "      <td>0.991168</td>\n",
              "      <td>0.752105</td>\n",
              "      <td>0.569557</td>\n",
              "      <td>0.862227</td>\n",
              "      <td>-0.073839</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>65350</th>\n",
              "      <td>0.219954</td>\n",
              "      <td>-0.164684</td>\n",
              "      <td>-2.101103</td>\n",
              "      <td>0.178877</td>\n",
              "      <td>1.042156</td>\n",
              "      <td>1.600690</td>\n",
              "      <td>0.475176</td>\n",
              "      <td>-1.441050</td>\n",
              "      <td>0.655473</td>\n",
              "      <td>0.341721</td>\n",
              "      <td>...</td>\n",
              "      <td>0.291544</td>\n",
              "      <td>-0.530432</td>\n",
              "      <td>1.075603</td>\n",
              "      <td>-0.646918</td>\n",
              "      <td>1.889441</td>\n",
              "      <td>0.755993</td>\n",
              "      <td>1.602319</td>\n",
              "      <td>0.941502</td>\n",
              "      <td>0.380824</td>\n",
              "      <td>-1.047735</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>65351</th>\n",
              "      <td>-0.058367</td>\n",
              "      <td>2.803957</td>\n",
              "      <td>-0.278332</td>\n",
              "      <td>-0.000573</td>\n",
              "      <td>0.059644</td>\n",
              "      <td>0.591501</td>\n",
              "      <td>-0.359922</td>\n",
              "      <td>-1.864061</td>\n",
              "      <td>-0.740880</td>\n",
              "      <td>1.572192</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.037592</td>\n",
              "      <td>-2.191806</td>\n",
              "      <td>0.366280</td>\n",
              "      <td>0.996630</td>\n",
              "      <td>-0.891282</td>\n",
              "      <td>1.421935</td>\n",
              "      <td>-0.060654</td>\n",
              "      <td>-0.351598</td>\n",
              "      <td>-0.351815</td>\n",
              "      <td>0.990657</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>65352</th>\n",
              "      <td>1.435983</td>\n",
              "      <td>0.340931</td>\n",
              "      <td>-0.585311</td>\n",
              "      <td>-0.586959</td>\n",
              "      <td>0.857043</td>\n",
              "      <td>1.332416</td>\n",
              "      <td>0.246491</td>\n",
              "      <td>-2.774402</td>\n",
              "      <td>-2.317590</td>\n",
              "      <td>-0.306120</td>\n",
              "      <td>...</td>\n",
              "      <td>-2.069377</td>\n",
              "      <td>1.434220</td>\n",
              "      <td>-2.861527</td>\n",
              "      <td>1.168988</td>\n",
              "      <td>-1.196807</td>\n",
              "      <td>2.636876</td>\n",
              "      <td>0.429715</td>\n",
              "      <td>0.788134</td>\n",
              "      <td>0.230333</td>\n",
              "      <td>-0.453739</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>65353</th>\n",
              "      <td>-0.940602</td>\n",
              "      <td>0.970519</td>\n",
              "      <td>-0.964230</td>\n",
              "      <td>-1.040686</td>\n",
              "      <td>0.023177</td>\n",
              "      <td>1.786006</td>\n",
              "      <td>0.726134</td>\n",
              "      <td>0.532319</td>\n",
              "      <td>-0.590089</td>\n",
              "      <td>0.761406</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.308722</td>\n",
              "      <td>0.841296</td>\n",
              "      <td>-1.744575</td>\n",
              "      <td>0.822835</td>\n",
              "      <td>0.962526</td>\n",
              "      <td>1.847089</td>\n",
              "      <td>0.098947</td>\n",
              "      <td>0.773195</td>\n",
              "      <td>0.745719</td>\n",
              "      <td>-0.478337</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>65354 rows Ã— 100 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "             0         1         2         3         4         5         6   \\\n",
              "0      0.609158  0.115282  0.003429  0.796281 -0.175308 -0.486566 -1.483976   \n",
              "1     -0.173142 -0.932471 -0.457326 -0.522826  0.837405 -0.439883 -0.709800   \n",
              "2     -2.559303 -0.187480  1.909480  0.879880  1.636030 -1.265262 -1.318124   \n",
              "3     -1.717762 -0.859337 -1.955419  0.207806  1.102318 -1.585540 -2.027091   \n",
              "4     -2.287486 -0.918230  1.266380  1.769894 -0.253977 -1.897076 -2.473712   \n",
              "...         ...       ...       ...       ...       ...       ...       ...   \n",
              "65349 -0.023224 -0.073207  0.048462 -0.651785  1.660518  1.043009  0.861014   \n",
              "65350  0.219954 -0.164684 -2.101103  0.178877  1.042156  1.600690  0.475176   \n",
              "65351 -0.058367  2.803957 -0.278332 -0.000573  0.059644  0.591501 -0.359922   \n",
              "65352  1.435983  0.340931 -0.585311 -0.586959  0.857043  1.332416  0.246491   \n",
              "65353 -0.940602  0.970519 -0.964230 -1.040686  0.023177  1.786006  0.726134   \n",
              "\n",
              "             7         8         9   ...        90        91        92  \\\n",
              "0      0.610219  0.611135  0.610774  ... -0.438418  0.318527  1.803023   \n",
              "1      2.101081  0.949970  0.753548  ... -3.573737  1.148647 -0.340055   \n",
              "2      1.173769 -1.989154  0.644037  ... -2.891630  1.337717  1.740497   \n",
              "3      0.885350 -1.542764 -1.024008  ... -1.225380  3.573152  1.406274   \n",
              "4      2.277956 -0.451920  0.927585  ...  0.808325  1.678861 -0.968123   \n",
              "...         ...       ...       ...  ...       ...       ...       ...   \n",
              "65349 -1.982811 -0.330726  0.566884  ...  0.227098 -0.122984 -0.349267   \n",
              "65350 -1.441050  0.655473  0.341721  ...  0.291544 -0.530432  1.075603   \n",
              "65351 -1.864061 -0.740880  1.572192  ... -0.037592 -2.191806  0.366280   \n",
              "65352 -2.774402 -2.317590 -0.306120  ... -2.069377  1.434220 -2.861527   \n",
              "65353  0.532319 -0.590089  0.761406  ... -0.308722  0.841296 -1.744575   \n",
              "\n",
              "             93        94        95        96        97        98        99  \n",
              "0     -0.366931  1.768442  2.428774 -0.465209  0.156457 -1.924398 -0.263324  \n",
              "1      1.314282 -1.654844  0.863783 -0.723793  0.388550  0.416612 -0.155420  \n",
              "2     -0.410580 -2.490496  0.056314  0.013203 -0.810805 -0.058793 -1.404608  \n",
              "3      0.703647 -0.404810  0.540573 -0.423621 -0.617609  1.511330 -2.336802  \n",
              "4     -1.490145  1.825365 -0.323188  0.085665  0.905258  0.341302 -1.070348  \n",
              "...         ...       ...       ...       ...       ...       ...       ...  \n",
              "65349  0.802841 -1.017031  0.991168  0.752105  0.569557  0.862227 -0.073839  \n",
              "65350 -0.646918  1.889441  0.755993  1.602319  0.941502  0.380824 -1.047735  \n",
              "65351  0.996630 -0.891282  1.421935 -0.060654 -0.351598 -0.351815  0.990657  \n",
              "65352  1.168988 -1.196807  2.636876  0.429715  0.788134  0.230333 -0.453739  \n",
              "65353  0.822835  0.962526  1.847089  0.098947  0.773195  0.745719 -0.478337  \n",
              "\n",
              "[65354 rows x 100 columns]"
            ]
          },
          "execution_count": 46,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "arr = [doc_model.dv[docs_dataset[\"text\"][i].tags] for i in docs_dataset.index]\n",
        "embeddings_dataset = pd.DataFrame(np.reshape(arr,(len(arr), 100)))\n",
        "embeddings_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "id": "Sop_gw5PdYKj",
        "outputId": "489cc0f3-00c3-476a-c619-89a7dc34fe64"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>90</th>\n",
              "      <th>91</th>\n",
              "      <th>92</th>\n",
              "      <th>93</th>\n",
              "      <th>94</th>\n",
              "      <th>95</th>\n",
              "      <th>96</th>\n",
              "      <th>97</th>\n",
              "      <th>98</th>\n",
              "      <th>99</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>65354.000000</td>\n",
              "      <td>65354.000000</td>\n",
              "      <td>65354.000000</td>\n",
              "      <td>65354.000000</td>\n",
              "      <td>65354.000000</td>\n",
              "      <td>65354.000000</td>\n",
              "      <td>65354.000000</td>\n",
              "      <td>65354.000000</td>\n",
              "      <td>65354.000000</td>\n",
              "      <td>65354.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>65354.000000</td>\n",
              "      <td>65354.000000</td>\n",
              "      <td>65354.000000</td>\n",
              "      <td>65354.000000</td>\n",
              "      <td>65354.000000</td>\n",
              "      <td>65354.000000</td>\n",
              "      <td>65354.000000</td>\n",
              "      <td>65354.000000</td>\n",
              "      <td>65354.000000</td>\n",
              "      <td>65354.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>0.395612</td>\n",
              "      <td>-0.147919</td>\n",
              "      <td>-0.001299</td>\n",
              "      <td>0.243384</td>\n",
              "      <td>0.152915</td>\n",
              "      <td>0.045535</td>\n",
              "      <td>-0.251683</td>\n",
              "      <td>0.275193</td>\n",
              "      <td>-0.942407</td>\n",
              "      <td>0.379380</td>\n",
              "      <td>...</td>\n",
              "      <td>0.024653</td>\n",
              "      <td>0.653022</td>\n",
              "      <td>0.136681</td>\n",
              "      <td>0.651390</td>\n",
              "      <td>0.770183</td>\n",
              "      <td>0.239828</td>\n",
              "      <td>-0.052048</td>\n",
              "      <td>-0.302242</td>\n",
              "      <td>0.710541</td>\n",
              "      <td>-0.336194</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>1.027466</td>\n",
              "      <td>1.061158</td>\n",
              "      <td>0.998902</td>\n",
              "      <td>1.024195</td>\n",
              "      <td>1.021180</td>\n",
              "      <td>1.125943</td>\n",
              "      <td>1.110366</td>\n",
              "      <td>1.102551</td>\n",
              "      <td>1.161033</td>\n",
              "      <td>1.017252</td>\n",
              "      <td>...</td>\n",
              "      <td>1.078750</td>\n",
              "      <td>1.045016</td>\n",
              "      <td>0.981930</td>\n",
              "      <td>1.018411</td>\n",
              "      <td>1.067373</td>\n",
              "      <td>0.999555</td>\n",
              "      <td>1.047770</td>\n",
              "      <td>1.024566</td>\n",
              "      <td>1.116092</td>\n",
              "      <td>1.044735</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>-5.602212</td>\n",
              "      <td>-4.956829</td>\n",
              "      <td>-5.266984</td>\n",
              "      <td>-6.323804</td>\n",
              "      <td>-5.354230</td>\n",
              "      <td>-5.374154</td>\n",
              "      <td>-6.080651</td>\n",
              "      <td>-5.390715</td>\n",
              "      <td>-6.990065</td>\n",
              "      <td>-4.441182</td>\n",
              "      <td>...</td>\n",
              "      <td>-6.500263</td>\n",
              "      <td>-4.635933</td>\n",
              "      <td>-4.355635</td>\n",
              "      <td>-4.485164</td>\n",
              "      <td>-4.803183</td>\n",
              "      <td>-5.165876</td>\n",
              "      <td>-4.984920</td>\n",
              "      <td>-6.333243</td>\n",
              "      <td>-4.660988</td>\n",
              "      <td>-6.432693</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>-0.254643</td>\n",
              "      <td>-0.828295</td>\n",
              "      <td>-0.647771</td>\n",
              "      <td>-0.402894</td>\n",
              "      <td>-0.499617</td>\n",
              "      <td>-0.678686</td>\n",
              "      <td>-0.950633</td>\n",
              "      <td>-0.420243</td>\n",
              "      <td>-1.670641</td>\n",
              "      <td>-0.279367</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.668991</td>\n",
              "      <td>-0.017294</td>\n",
              "      <td>-0.487412</td>\n",
              "      <td>-0.005313</td>\n",
              "      <td>0.098043</td>\n",
              "      <td>-0.409071</td>\n",
              "      <td>-0.734966</td>\n",
              "      <td>-0.952872</td>\n",
              "      <td>-0.023869</td>\n",
              "      <td>-0.998682</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>0.420294</td>\n",
              "      <td>-0.141375</td>\n",
              "      <td>-0.013796</td>\n",
              "      <td>0.256822</td>\n",
              "      <td>0.150024</td>\n",
              "      <td>0.055520</td>\n",
              "      <td>-0.223224</td>\n",
              "      <td>0.295601</td>\n",
              "      <td>-0.918618</td>\n",
              "      <td>0.368887</td>\n",
              "      <td>...</td>\n",
              "      <td>0.031216</td>\n",
              "      <td>0.646783</td>\n",
              "      <td>0.140222</td>\n",
              "      <td>0.643295</td>\n",
              "      <td>0.788262</td>\n",
              "      <td>0.226972</td>\n",
              "      <td>-0.079584</td>\n",
              "      <td>-0.288895</td>\n",
              "      <td>0.686378</td>\n",
              "      <td>-0.339193</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>1.066034</td>\n",
              "      <td>0.536718</td>\n",
              "      <td>0.631478</td>\n",
              "      <td>0.897963</td>\n",
              "      <td>0.807304</td>\n",
              "      <td>0.774418</td>\n",
              "      <td>0.469484</td>\n",
              "      <td>0.985491</td>\n",
              "      <td>-0.189866</td>\n",
              "      <td>1.028998</td>\n",
              "      <td>...</td>\n",
              "      <td>0.721976</td>\n",
              "      <td>1.327169</td>\n",
              "      <td>0.768591</td>\n",
              "      <td>1.302705</td>\n",
              "      <td>1.453350</td>\n",
              "      <td>0.870422</td>\n",
              "      <td>0.610971</td>\n",
              "      <td>0.362364</td>\n",
              "      <td>1.424375</td>\n",
              "      <td>0.327576</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>6.126747</td>\n",
              "      <td>6.428709</td>\n",
              "      <td>6.093700</td>\n",
              "      <td>5.165659</td>\n",
              "      <td>5.082952</td>\n",
              "      <td>5.486690</td>\n",
              "      <td>4.584780</td>\n",
              "      <td>8.116519</td>\n",
              "      <td>5.243529</td>\n",
              "      <td>6.424573</td>\n",
              "      <td>...</td>\n",
              "      <td>6.600666</td>\n",
              "      <td>7.858045</td>\n",
              "      <td>6.515896</td>\n",
              "      <td>8.467405</td>\n",
              "      <td>7.161427</td>\n",
              "      <td>5.992213</td>\n",
              "      <td>5.476920</td>\n",
              "      <td>4.716230</td>\n",
              "      <td>6.813169</td>\n",
              "      <td>4.942352</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>8 rows Ã— 100 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                 0             1             2             3             4   \\\n",
              "count  65354.000000  65354.000000  65354.000000  65354.000000  65354.000000   \n",
              "mean       0.395612     -0.147919     -0.001299      0.243384      0.152915   \n",
              "std        1.027466      1.061158      0.998902      1.024195      1.021180   \n",
              "min       -5.602212     -4.956829     -5.266984     -6.323804     -5.354230   \n",
              "25%       -0.254643     -0.828295     -0.647771     -0.402894     -0.499617   \n",
              "50%        0.420294     -0.141375     -0.013796      0.256822      0.150024   \n",
              "75%        1.066034      0.536718      0.631478      0.897963      0.807304   \n",
              "max        6.126747      6.428709      6.093700      5.165659      5.082952   \n",
              "\n",
              "                 5             6             7             8             9   \\\n",
              "count  65354.000000  65354.000000  65354.000000  65354.000000  65354.000000   \n",
              "mean       0.045535     -0.251683      0.275193     -0.942407      0.379380   \n",
              "std        1.125943      1.110366      1.102551      1.161033      1.017252   \n",
              "min       -5.374154     -6.080651     -5.390715     -6.990065     -4.441182   \n",
              "25%       -0.678686     -0.950633     -0.420243     -1.670641     -0.279367   \n",
              "50%        0.055520     -0.223224      0.295601     -0.918618      0.368887   \n",
              "75%        0.774418      0.469484      0.985491     -0.189866      1.028998   \n",
              "max        5.486690      4.584780      8.116519      5.243529      6.424573   \n",
              "\n",
              "       ...            90            91            92            93  \\\n",
              "count  ...  65354.000000  65354.000000  65354.000000  65354.000000   \n",
              "mean   ...      0.024653      0.653022      0.136681      0.651390   \n",
              "std    ...      1.078750      1.045016      0.981930      1.018411   \n",
              "min    ...     -6.500263     -4.635933     -4.355635     -4.485164   \n",
              "25%    ...     -0.668991     -0.017294     -0.487412     -0.005313   \n",
              "50%    ...      0.031216      0.646783      0.140222      0.643295   \n",
              "75%    ...      0.721976      1.327169      0.768591      1.302705   \n",
              "max    ...      6.600666      7.858045      6.515896      8.467405   \n",
              "\n",
              "                 94            95            96            97            98  \\\n",
              "count  65354.000000  65354.000000  65354.000000  65354.000000  65354.000000   \n",
              "mean       0.770183      0.239828     -0.052048     -0.302242      0.710541   \n",
              "std        1.067373      0.999555      1.047770      1.024566      1.116092   \n",
              "min       -4.803183     -5.165876     -4.984920     -6.333243     -4.660988   \n",
              "25%        0.098043     -0.409071     -0.734966     -0.952872     -0.023869   \n",
              "50%        0.788262      0.226972     -0.079584     -0.288895      0.686378   \n",
              "75%        1.453350      0.870422      0.610971      0.362364      1.424375   \n",
              "max        7.161427      5.992213      5.476920      4.716230      6.813169   \n",
              "\n",
              "                 99  \n",
              "count  65354.000000  \n",
              "mean      -0.336194  \n",
              "std        1.044735  \n",
              "min       -6.432693  \n",
              "25%       -0.998682  \n",
              "50%       -0.339193  \n",
              "75%        0.327576  \n",
              "max        4.942352  \n",
              "\n",
              "[8 rows x 100 columns]"
            ]
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "embeddings_dataset.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XL1gXWWodYKj"
      },
      "source": [
        "#### Normalizar embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5XdVRiYOdYKj",
        "outputId": "fdec5bf1-af94-4e9a-f178-6a6c7c01367d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0        13.892263\n",
              "1        13.328569\n",
              "2        14.220605\n",
              "3        14.551357\n",
              "4        16.151571\n",
              "           ...    \n",
              "65349     9.853381\n",
              "65350    12.673384\n",
              "65351    10.743855\n",
              "65352    11.286342\n",
              "65353    11.192432\n",
              "Name: norm, Length: 65354, dtype: float32"
            ]
          },
          "execution_count": 48,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "embeddings_dataset[\"norm\"]=np.linalg.norm(embeddings_dataset, axis=1)\n",
        "embeddings_dataset[\"norm\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "id": "Y-ns5umfdYKj",
        "outputId": "e0e26dd1-7bc0-4a9e-d7c6-199d29518bec"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>92</th>\n",
              "      <th>93</th>\n",
              "      <th>94</th>\n",
              "      <th>95</th>\n",
              "      <th>96</th>\n",
              "      <th>97</th>\n",
              "      <th>98</th>\n",
              "      <th>99</th>\n",
              "      <th>100</th>\n",
              "      <th>normalized_norm</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>65354.000000</td>\n",
              "      <td>65354.000000</td>\n",
              "      <td>65354.000000</td>\n",
              "      <td>65354.000000</td>\n",
              "      <td>65354.000000</td>\n",
              "      <td>65354.000000</td>\n",
              "      <td>65354.000000</td>\n",
              "      <td>65354.000000</td>\n",
              "      <td>65354.000000</td>\n",
              "      <td>65354.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>65354.000000</td>\n",
              "      <td>65354.000000</td>\n",
              "      <td>65354.000000</td>\n",
              "      <td>65354.000000</td>\n",
              "      <td>65354.000000</td>\n",
              "      <td>65354.000000</td>\n",
              "      <td>65354.000000</td>\n",
              "      <td>65354.000000</td>\n",
              "      <td>6.535400e+04</td>\n",
              "      <td>65354.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>0.026653</td>\n",
              "      <td>-0.009464</td>\n",
              "      <td>-0.000197</td>\n",
              "      <td>0.016458</td>\n",
              "      <td>0.010459</td>\n",
              "      <td>0.002886</td>\n",
              "      <td>-0.015876</td>\n",
              "      <td>0.019336</td>\n",
              "      <td>-0.061606</td>\n",
              "      <td>0.025368</td>\n",
              "      <td>...</td>\n",
              "      <td>0.009252</td>\n",
              "      <td>0.042896</td>\n",
              "      <td>0.051496</td>\n",
              "      <td>0.015329</td>\n",
              "      <td>-0.004515</td>\n",
              "      <td>-0.019319</td>\n",
              "      <td>0.046191</td>\n",
              "      <td>-0.022246</td>\n",
              "      <td>7.071067e-01</td>\n",
              "      <td>0.304545</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>0.064862</td>\n",
              "      <td>0.066234</td>\n",
              "      <td>0.062498</td>\n",
              "      <td>0.063825</td>\n",
              "      <td>0.063946</td>\n",
              "      <td>0.070003</td>\n",
              "      <td>0.068782</td>\n",
              "      <td>0.068711</td>\n",
              "      <td>0.072137</td>\n",
              "      <td>0.064015</td>\n",
              "      <td>...</td>\n",
              "      <td>0.061470</td>\n",
              "      <td>0.063767</td>\n",
              "      <td>0.067509</td>\n",
              "      <td>0.062578</td>\n",
              "      <td>0.065461</td>\n",
              "      <td>0.063731</td>\n",
              "      <td>0.069474</td>\n",
              "      <td>0.064941</td>\n",
              "      <td>3.329731e-08</td>\n",
              "      <td>0.060327</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>-0.243751</td>\n",
              "      <td>-0.276438</td>\n",
              "      <td>-0.263549</td>\n",
              "      <td>-0.240561</td>\n",
              "      <td>-0.263634</td>\n",
              "      <td>-0.293598</td>\n",
              "      <td>-0.275147</td>\n",
              "      <td>-0.272187</td>\n",
              "      <td>-0.333155</td>\n",
              "      <td>-0.235636</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.259937</td>\n",
              "      <td>-0.210113</td>\n",
              "      <td>-0.218038</td>\n",
              "      <td>-0.259823</td>\n",
              "      <td>-0.265264</td>\n",
              "      <td>-0.314676</td>\n",
              "      <td>-0.240049</td>\n",
              "      <td>-0.276494</td>\n",
              "      <td>7.071066e-01</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>-0.016567</td>\n",
              "      <td>-0.054284</td>\n",
              "      <td>-0.042844</td>\n",
              "      <td>-0.026465</td>\n",
              "      <td>-0.032741</td>\n",
              "      <td>-0.044704</td>\n",
              "      <td>-0.062579</td>\n",
              "      <td>-0.027168</td>\n",
              "      <td>-0.110894</td>\n",
              "      <td>-0.018230</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.032062</td>\n",
              "      <td>-0.000322</td>\n",
              "      <td>0.006283</td>\n",
              "      <td>-0.027177</td>\n",
              "      <td>-0.048966</td>\n",
              "      <td>-0.062524</td>\n",
              "      <td>-0.001581</td>\n",
              "      <td>-0.066674</td>\n",
              "      <td>7.071068e-01</td>\n",
              "      <td>0.261894</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>0.027946</td>\n",
              "      <td>-0.009526</td>\n",
              "      <td>-0.000930</td>\n",
              "      <td>0.017321</td>\n",
              "      <td>0.009958</td>\n",
              "      <td>0.003703</td>\n",
              "      <td>-0.015042</td>\n",
              "      <td>0.019793</td>\n",
              "      <td>-0.061696</td>\n",
              "      <td>0.024508</td>\n",
              "      <td>...</td>\n",
              "      <td>0.009363</td>\n",
              "      <td>0.043206</td>\n",
              "      <td>0.052499</td>\n",
              "      <td>0.015189</td>\n",
              "      <td>-0.005310</td>\n",
              "      <td>-0.019364</td>\n",
              "      <td>0.045991</td>\n",
              "      <td>-0.022730</td>\n",
              "      <td>7.071068e-01</td>\n",
              "      <td>0.304718</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>0.071248</td>\n",
              "      <td>0.035421</td>\n",
              "      <td>0.041639</td>\n",
              "      <td>0.059711</td>\n",
              "      <td>0.053335</td>\n",
              "      <td>0.051042</td>\n",
              "      <td>0.031089</td>\n",
              "      <td>0.066195</td>\n",
              "      <td>-0.012563</td>\n",
              "      <td>0.068696</td>\n",
              "      <td>...</td>\n",
              "      <td>0.051104</td>\n",
              "      <td>0.085978</td>\n",
              "      <td>0.097877</td>\n",
              "      <td>0.057308</td>\n",
              "      <td>0.039671</td>\n",
              "      <td>0.024117</td>\n",
              "      <td>0.093929</td>\n",
              "      <td>0.021511</td>\n",
              "      <td>7.071068e-01</td>\n",
              "      <td>0.344303</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>0.288341</td>\n",
              "      <td>0.249216</td>\n",
              "      <td>0.279243</td>\n",
              "      <td>0.270375</td>\n",
              "      <td>0.282558</td>\n",
              "      <td>0.262080</td>\n",
              "      <td>0.239142</td>\n",
              "      <td>0.295370</td>\n",
              "      <td>0.214805</td>\n",
              "      <td>0.288238</td>\n",
              "      <td>...</td>\n",
              "      <td>0.272810</td>\n",
              "      <td>0.335553</td>\n",
              "      <td>0.332427</td>\n",
              "      <td>0.275623</td>\n",
              "      <td>0.282360</td>\n",
              "      <td>0.243853</td>\n",
              "      <td>0.311407</td>\n",
              "      <td>0.263388</td>\n",
              "      <td>7.071069e-01</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>8 rows Ã— 102 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                  0             1             2             3             4  \\\n",
              "count  65354.000000  65354.000000  65354.000000  65354.000000  65354.000000   \n",
              "mean       0.026653     -0.009464     -0.000197      0.016458      0.010459   \n",
              "std        0.064862      0.066234      0.062498      0.063825      0.063946   \n",
              "min       -0.243751     -0.276438     -0.263549     -0.240561     -0.263634   \n",
              "25%       -0.016567     -0.054284     -0.042844     -0.026465     -0.032741   \n",
              "50%        0.027946     -0.009526     -0.000930      0.017321      0.009958   \n",
              "75%        0.071248      0.035421      0.041639      0.059711      0.053335   \n",
              "max        0.288341      0.249216      0.279243      0.270375      0.282558   \n",
              "\n",
              "                  5             6             7             8             9  \\\n",
              "count  65354.000000  65354.000000  65354.000000  65354.000000  65354.000000   \n",
              "mean       0.002886     -0.015876      0.019336     -0.061606      0.025368   \n",
              "std        0.070003      0.068782      0.068711      0.072137      0.064015   \n",
              "min       -0.293598     -0.275147     -0.272187     -0.333155     -0.235636   \n",
              "25%       -0.044704     -0.062579     -0.027168     -0.110894     -0.018230   \n",
              "50%        0.003703     -0.015042      0.019793     -0.061696      0.024508   \n",
              "75%        0.051042      0.031089      0.066195     -0.012563      0.068696   \n",
              "max        0.262080      0.239142      0.295370      0.214805      0.288238   \n",
              "\n",
              "       ...            92            93            94            95  \\\n",
              "count  ...  65354.000000  65354.000000  65354.000000  65354.000000   \n",
              "mean   ...      0.009252      0.042896      0.051496      0.015329   \n",
              "std    ...      0.061470      0.063767      0.067509      0.062578   \n",
              "min    ...     -0.259937     -0.210113     -0.218038     -0.259823   \n",
              "25%    ...     -0.032062     -0.000322      0.006283     -0.027177   \n",
              "50%    ...      0.009363      0.043206      0.052499      0.015189   \n",
              "75%    ...      0.051104      0.085978      0.097877      0.057308   \n",
              "max    ...      0.272810      0.335553      0.332427      0.275623   \n",
              "\n",
              "                 96            97            98            99           100  \\\n",
              "count  65354.000000  65354.000000  65354.000000  65354.000000  6.535400e+04   \n",
              "mean      -0.004515     -0.019319      0.046191     -0.022246  7.071067e-01   \n",
              "std        0.065461      0.063731      0.069474      0.064941  3.329731e-08   \n",
              "min       -0.265264     -0.314676     -0.240049     -0.276494  7.071066e-01   \n",
              "25%       -0.048966     -0.062524     -0.001581     -0.066674  7.071068e-01   \n",
              "50%       -0.005310     -0.019364      0.045991     -0.022730  7.071068e-01   \n",
              "75%        0.039671      0.024117      0.093929      0.021511  7.071068e-01   \n",
              "max        0.282360      0.243853      0.311407      0.263388  7.071069e-01   \n",
              "\n",
              "       normalized_norm  \n",
              "count     65354.000000  \n",
              "mean          0.304545  \n",
              "std           0.060327  \n",
              "min           0.000000  \n",
              "25%           0.261894  \n",
              "50%           0.304718  \n",
              "75%           0.344303  \n",
              "max           1.000000  \n",
              "\n",
              "[8 rows x 102 columns]"
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "norm_embeddings_dataset = pd.DataFrame(np.apply_along_axis(lambda x: x / np.linalg.norm(x), axis=1, arr=embeddings_dataset))\n",
        "norm_embeddings_dataset[\"normalized_norm\"] = (embeddings_dataset['norm'] - embeddings_dataset['norm'].min()) / (embeddings_dataset['norm'].max() - embeddings_dataset['norm'].min())\n",
        "norm_embeddings_dataset.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>92</th>\n",
              "      <th>93</th>\n",
              "      <th>94</th>\n",
              "      <th>95</th>\n",
              "      <th>96</th>\n",
              "      <th>97</th>\n",
              "      <th>98</th>\n",
              "      <th>99</th>\n",
              "      <th>100</th>\n",
              "      <th>normalized_norm</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.031006</td>\n",
              "      <td>0.005868</td>\n",
              "      <td>0.000175</td>\n",
              "      <td>0.040530</td>\n",
              "      <td>-0.008923</td>\n",
              "      <td>-0.024766</td>\n",
              "      <td>-0.075533</td>\n",
              "      <td>0.031060</td>\n",
              "      <td>0.031106</td>\n",
              "      <td>0.031088</td>\n",
              "      <td>...</td>\n",
              "      <td>0.091773</td>\n",
              "      <td>-0.018677</td>\n",
              "      <td>0.090013</td>\n",
              "      <td>0.123623</td>\n",
              "      <td>-0.023679</td>\n",
              "      <td>0.007964</td>\n",
              "      <td>-0.097951</td>\n",
              "      <td>-0.013403</td>\n",
              "      <td>0.707107</td>\n",
              "      <td>0.386702</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-0.009186</td>\n",
              "      <td>-0.049469</td>\n",
              "      <td>-0.024262</td>\n",
              "      <td>-0.027737</td>\n",
              "      <td>0.044426</td>\n",
              "      <td>-0.023337</td>\n",
              "      <td>-0.037656</td>\n",
              "      <td>0.111466</td>\n",
              "      <td>0.050398</td>\n",
              "      <td>0.039977</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.018041</td>\n",
              "      <td>0.069725</td>\n",
              "      <td>-0.087793</td>\n",
              "      <td>0.045825</td>\n",
              "      <td>-0.038399</td>\n",
              "      <td>0.020613</td>\n",
              "      <td>0.022102</td>\n",
              "      <td>-0.008245</td>\n",
              "      <td>0.707107</td>\n",
              "      <td>0.370435</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-0.127259</td>\n",
              "      <td>-0.009322</td>\n",
              "      <td>0.094947</td>\n",
              "      <td>0.043751</td>\n",
              "      <td>0.081350</td>\n",
              "      <td>-0.062914</td>\n",
              "      <td>-0.065543</td>\n",
              "      <td>0.058365</td>\n",
              "      <td>-0.098909</td>\n",
              "      <td>0.032024</td>\n",
              "      <td>...</td>\n",
              "      <td>0.086545</td>\n",
              "      <td>-0.020416</td>\n",
              "      <td>-0.123838</td>\n",
              "      <td>0.002800</td>\n",
              "      <td>0.000657</td>\n",
              "      <td>-0.040317</td>\n",
              "      <td>-0.002923</td>\n",
              "      <td>-0.069843</td>\n",
              "      <td>0.707107</td>\n",
              "      <td>0.396177</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-0.083473</td>\n",
              "      <td>-0.041759</td>\n",
              "      <td>-0.095021</td>\n",
              "      <td>0.010098</td>\n",
              "      <td>0.053566</td>\n",
              "      <td>-0.077048</td>\n",
              "      <td>-0.098504</td>\n",
              "      <td>0.043023</td>\n",
              "      <td>-0.074969</td>\n",
              "      <td>-0.049760</td>\n",
              "      <td>...</td>\n",
              "      <td>0.068336</td>\n",
              "      <td>0.034193</td>\n",
              "      <td>-0.019671</td>\n",
              "      <td>0.026269</td>\n",
              "      <td>-0.020585</td>\n",
              "      <td>-0.030012</td>\n",
              "      <td>0.073441</td>\n",
              "      <td>-0.113554</td>\n",
              "      <td>0.707107</td>\n",
              "      <td>0.405722</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-0.100145</td>\n",
              "      <td>-0.040200</td>\n",
              "      <td>0.055441</td>\n",
              "      <td>0.077485</td>\n",
              "      <td>-0.011119</td>\n",
              "      <td>-0.083053</td>\n",
              "      <td>-0.108298</td>\n",
              "      <td>0.099728</td>\n",
              "      <td>-0.019785</td>\n",
              "      <td>0.040609</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.042384</td>\n",
              "      <td>-0.065238</td>\n",
              "      <td>0.079913</td>\n",
              "      <td>-0.014149</td>\n",
              "      <td>0.003750</td>\n",
              "      <td>0.039632</td>\n",
              "      <td>0.014942</td>\n",
              "      <td>-0.046859</td>\n",
              "      <td>0.707107</td>\n",
              "      <td>0.451901</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>65349</th>\n",
              "      <td>-0.001667</td>\n",
              "      <td>-0.005254</td>\n",
              "      <td>0.003478</td>\n",
              "      <td>-0.046774</td>\n",
              "      <td>0.119164</td>\n",
              "      <td>0.074849</td>\n",
              "      <td>0.061789</td>\n",
              "      <td>-0.142292</td>\n",
              "      <td>-0.023734</td>\n",
              "      <td>0.040681</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.025064</td>\n",
              "      <td>0.057614</td>\n",
              "      <td>-0.072985</td>\n",
              "      <td>0.071129</td>\n",
              "      <td>0.053973</td>\n",
              "      <td>0.040873</td>\n",
              "      <td>0.061876</td>\n",
              "      <td>-0.005299</td>\n",
              "      <td>0.707107</td>\n",
              "      <td>0.270148</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>65350</th>\n",
              "      <td>0.012272</td>\n",
              "      <td>-0.009188</td>\n",
              "      <td>-0.117230</td>\n",
              "      <td>0.009980</td>\n",
              "      <td>0.058147</td>\n",
              "      <td>0.089310</td>\n",
              "      <td>0.026512</td>\n",
              "      <td>-0.080403</td>\n",
              "      <td>0.036572</td>\n",
              "      <td>0.019066</td>\n",
              "      <td>...</td>\n",
              "      <td>0.060013</td>\n",
              "      <td>-0.036095</td>\n",
              "      <td>0.105421</td>\n",
              "      <td>0.042180</td>\n",
              "      <td>0.089401</td>\n",
              "      <td>0.052531</td>\n",
              "      <td>0.021248</td>\n",
              "      <td>-0.058458</td>\n",
              "      <td>0.707107</td>\n",
              "      <td>0.351528</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>65351</th>\n",
              "      <td>-0.003841</td>\n",
              "      <td>0.184542</td>\n",
              "      <td>-0.018318</td>\n",
              "      <td>-0.000038</td>\n",
              "      <td>0.003925</td>\n",
              "      <td>0.038930</td>\n",
              "      <td>-0.023688</td>\n",
              "      <td>-0.122683</td>\n",
              "      <td>-0.048761</td>\n",
              "      <td>0.103474</td>\n",
              "      <td>...</td>\n",
              "      <td>0.024107</td>\n",
              "      <td>0.065593</td>\n",
              "      <td>-0.058660</td>\n",
              "      <td>0.093585</td>\n",
              "      <td>-0.003992</td>\n",
              "      <td>-0.023140</td>\n",
              "      <td>-0.023155</td>\n",
              "      <td>0.065200</td>\n",
              "      <td>0.707107</td>\n",
              "      <td>0.295845</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>65352</th>\n",
              "      <td>0.089967</td>\n",
              "      <td>0.021360</td>\n",
              "      <td>-0.036671</td>\n",
              "      <td>-0.036774</td>\n",
              "      <td>0.053695</td>\n",
              "      <td>0.083478</td>\n",
              "      <td>0.015443</td>\n",
              "      <td>-0.173821</td>\n",
              "      <td>-0.145201</td>\n",
              "      <td>-0.019179</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.179279</td>\n",
              "      <td>0.073239</td>\n",
              "      <td>-0.074982</td>\n",
              "      <td>0.165204</td>\n",
              "      <td>0.026922</td>\n",
              "      <td>0.049378</td>\n",
              "      <td>0.014431</td>\n",
              "      <td>-0.028427</td>\n",
              "      <td>0.707107</td>\n",
              "      <td>0.311500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>65353</th>\n",
              "      <td>-0.059425</td>\n",
              "      <td>0.061315</td>\n",
              "      <td>-0.060917</td>\n",
              "      <td>-0.065748</td>\n",
              "      <td>0.001464</td>\n",
              "      <td>0.112835</td>\n",
              "      <td>0.045875</td>\n",
              "      <td>0.033630</td>\n",
              "      <td>-0.037280</td>\n",
              "      <td>0.048104</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.110217</td>\n",
              "      <td>0.051984</td>\n",
              "      <td>0.060810</td>\n",
              "      <td>0.116694</td>\n",
              "      <td>0.006251</td>\n",
              "      <td>0.048848</td>\n",
              "      <td>0.047112</td>\n",
              "      <td>-0.030220</td>\n",
              "      <td>0.707107</td>\n",
              "      <td>0.308790</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>65354 rows Ã— 102 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "              0         1         2         3         4         5         6  \\\n",
              "0      0.031006  0.005868  0.000175  0.040530 -0.008923 -0.024766 -0.075533   \n",
              "1     -0.009186 -0.049469 -0.024262 -0.027737  0.044426 -0.023337 -0.037656   \n",
              "2     -0.127259 -0.009322  0.094947  0.043751  0.081350 -0.062914 -0.065543   \n",
              "3     -0.083473 -0.041759 -0.095021  0.010098  0.053566 -0.077048 -0.098504   \n",
              "4     -0.100145 -0.040200  0.055441  0.077485 -0.011119 -0.083053 -0.108298   \n",
              "...         ...       ...       ...       ...       ...       ...       ...   \n",
              "65349 -0.001667 -0.005254  0.003478 -0.046774  0.119164  0.074849  0.061789   \n",
              "65350  0.012272 -0.009188 -0.117230  0.009980  0.058147  0.089310  0.026512   \n",
              "65351 -0.003841  0.184542 -0.018318 -0.000038  0.003925  0.038930 -0.023688   \n",
              "65352  0.089967  0.021360 -0.036671 -0.036774  0.053695  0.083478  0.015443   \n",
              "65353 -0.059425  0.061315 -0.060917 -0.065748  0.001464  0.112835  0.045875   \n",
              "\n",
              "              7         8         9  ...        92        93        94  \\\n",
              "0      0.031060  0.031106  0.031088  ...  0.091773 -0.018677  0.090013   \n",
              "1      0.111466  0.050398  0.039977  ... -0.018041  0.069725 -0.087793   \n",
              "2      0.058365 -0.098909  0.032024  ...  0.086545 -0.020416 -0.123838   \n",
              "3      0.043023 -0.074969 -0.049760  ...  0.068336  0.034193 -0.019671   \n",
              "4      0.099728 -0.019785  0.040609  ... -0.042384 -0.065238  0.079913   \n",
              "...         ...       ...       ...  ...       ...       ...       ...   \n",
              "65349 -0.142292 -0.023734  0.040681  ... -0.025064  0.057614 -0.072985   \n",
              "65350 -0.080403  0.036572  0.019066  ...  0.060013 -0.036095  0.105421   \n",
              "65351 -0.122683 -0.048761  0.103474  ...  0.024107  0.065593 -0.058660   \n",
              "65352 -0.173821 -0.145201 -0.019179  ... -0.179279  0.073239 -0.074982   \n",
              "65353  0.033630 -0.037280  0.048104  ... -0.110217  0.051984  0.060810   \n",
              "\n",
              "             95        96        97        98        99       100  \\\n",
              "0      0.123623 -0.023679  0.007964 -0.097951 -0.013403  0.707107   \n",
              "1      0.045825 -0.038399  0.020613  0.022102 -0.008245  0.707107   \n",
              "2      0.002800  0.000657 -0.040317 -0.002923 -0.069843  0.707107   \n",
              "3      0.026269 -0.020585 -0.030012  0.073441 -0.113554  0.707107   \n",
              "4     -0.014149  0.003750  0.039632  0.014942 -0.046859  0.707107   \n",
              "...         ...       ...       ...       ...       ...       ...   \n",
              "65349  0.071129  0.053973  0.040873  0.061876 -0.005299  0.707107   \n",
              "65350  0.042180  0.089401  0.052531  0.021248 -0.058458  0.707107   \n",
              "65351  0.093585 -0.003992 -0.023140 -0.023155  0.065200  0.707107   \n",
              "65352  0.165204  0.026922  0.049378  0.014431 -0.028427  0.707107   \n",
              "65353  0.116694  0.006251  0.048848  0.047112 -0.030220  0.707107   \n",
              "\n",
              "       normalized_norm  \n",
              "0             0.386702  \n",
              "1             0.370435  \n",
              "2             0.396177  \n",
              "3             0.405722  \n",
              "4             0.451901  \n",
              "...                ...  \n",
              "65349         0.270148  \n",
              "65350         0.351528  \n",
              "65351         0.295845  \n",
              "65352         0.311500  \n",
              "65353         0.308790  \n",
              "\n",
              "[65354 rows x 102 columns]"
            ]
          },
          "execution_count": 50,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "norm_embeddings_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "2jUZ8xrgdYKj"
      },
      "outputs": [],
      "source": [
        "norm_embeddings_dataset = norm_embeddings_dataset.drop([100],axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 52,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "docs_dataset.shape[0]==norm_embeddings_dataset.shape[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Index([                0,                 1,                 2,\n",
              "                       3,                 4,                 5,\n",
              "                       6,                 7,                 8,\n",
              "                       9,\n",
              "       ...\n",
              "                      91,                92,                93,\n",
              "                      94,                95,                96,\n",
              "                      97,                98,                99,\n",
              "       'normalized_norm'],\n",
              "      dtype='object', length=101)"
            ]
          },
          "execution_count": 53,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "norm_embeddings_dataset.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 826
        },
        "id": "MwmoLIkHdYKk",
        "outputId": "ce49eba2-4bc9-4547-b019-5ce71a71e360"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>prompt_id</th>\n",
              "      <th>text</th>\n",
              "      <th>generated</th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>...</th>\n",
              "      <th>91</th>\n",
              "      <th>92</th>\n",
              "      <th>93</th>\n",
              "      <th>94</th>\n",
              "      <th>95</th>\n",
              "      <th>96</th>\n",
              "      <th>97</th>\n",
              "      <th>98</th>\n",
              "      <th>99</th>\n",
              "      <th>normalized_norm</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0059830c</td>\n",
              "      <td>0</td>\n",
              "      <td>([Ä cars, ., Ä cars, Ä around, Ä since, Ä became, Ä ...</td>\n",
              "      <td>0</td>\n",
              "      <td>0.031006</td>\n",
              "      <td>0.005868</td>\n",
              "      <td>0.000175</td>\n",
              "      <td>0.040530</td>\n",
              "      <td>-0.008923</td>\n",
              "      <td>-0.024766</td>\n",
              "      <td>...</td>\n",
              "      <td>0.016213</td>\n",
              "      <td>0.091773</td>\n",
              "      <td>-0.018677</td>\n",
              "      <td>0.090013</td>\n",
              "      <td>0.123623</td>\n",
              "      <td>-0.023679</td>\n",
              "      <td>0.007964</td>\n",
              "      <td>-0.097951</td>\n",
              "      <td>-0.013403</td>\n",
              "      <td>0.386702</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>005db917</td>\n",
              "      <td>0</td>\n",
              "      <td>([Ä transportation, Ä large, Ä necessity, Ä countr...</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.009186</td>\n",
              "      <td>-0.049469</td>\n",
              "      <td>-0.024262</td>\n",
              "      <td>-0.027737</td>\n",
              "      <td>0.044426</td>\n",
              "      <td>-0.023337</td>\n",
              "      <td>...</td>\n",
              "      <td>0.060938</td>\n",
              "      <td>-0.018041</td>\n",
              "      <td>0.069725</td>\n",
              "      <td>-0.087793</td>\n",
              "      <td>0.045825</td>\n",
              "      <td>-0.038399</td>\n",
              "      <td>0.020613</td>\n",
              "      <td>0.022102</td>\n",
              "      <td>-0.008245</td>\n",
              "      <td>0.370435</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>008f63e3</td>\n",
              "      <td>0</td>\n",
              "      <td>([Ä \", america, Ä love, Ä affair, Ä vehicles, Ä see...</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.127259</td>\n",
              "      <td>-0.009322</td>\n",
              "      <td>0.094947</td>\n",
              "      <td>0.043751</td>\n",
              "      <td>0.081350</td>\n",
              "      <td>-0.062914</td>\n",
              "      <td>...</td>\n",
              "      <td>0.066517</td>\n",
              "      <td>0.086545</td>\n",
              "      <td>-0.020416</td>\n",
              "      <td>-0.123838</td>\n",
              "      <td>0.002800</td>\n",
              "      <td>0.000657</td>\n",
              "      <td>-0.040317</td>\n",
              "      <td>-0.002923</td>\n",
              "      <td>-0.069843</td>\n",
              "      <td>0.396177</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>00940276</td>\n",
              "      <td>0</td>\n",
              "      <td>([Ä often, Ä ride, Ä car, ?, Ä drive, Ä one, Ä motor...</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.083473</td>\n",
              "      <td>-0.041759</td>\n",
              "      <td>-0.095021</td>\n",
              "      <td>0.010098</td>\n",
              "      <td>0.053566</td>\n",
              "      <td>-0.077048</td>\n",
              "      <td>...</td>\n",
              "      <td>0.173633</td>\n",
              "      <td>0.068336</td>\n",
              "      <td>0.034193</td>\n",
              "      <td>-0.019671</td>\n",
              "      <td>0.026269</td>\n",
              "      <td>-0.020585</td>\n",
              "      <td>-0.030012</td>\n",
              "      <td>0.073441</td>\n",
              "      <td>-0.113554</td>\n",
              "      <td>0.405722</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>00c39458</td>\n",
              "      <td>0</td>\n",
              "      <td>([Ä cars, Ä wonderful, Ä thing, ., Ä perhaps, Ä one...</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.100145</td>\n",
              "      <td>-0.040200</td>\n",
              "      <td>0.055441</td>\n",
              "      <td>0.077485</td>\n",
              "      <td>-0.011119</td>\n",
              "      <td>-0.083053</td>\n",
              "      <td>...</td>\n",
              "      <td>0.073500</td>\n",
              "      <td>-0.042384</td>\n",
              "      <td>-0.065238</td>\n",
              "      <td>0.079913</td>\n",
              "      <td>-0.014149</td>\n",
              "      <td>0.003750</td>\n",
              "      <td>0.039632</td>\n",
              "      <td>0.014942</td>\n",
              "      <td>-0.046859</td>\n",
              "      <td>0.451901</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>65349</th>\n",
              "      <td>65525</td>\n",
              "      <td>1</td>\n",
              "      <td>([Ä , Ä dear, Ä senator, ,, Ä , Ä writing, Ä regardi...</td>\n",
              "      <td>1</td>\n",
              "      <td>-0.001667</td>\n",
              "      <td>-0.005254</td>\n",
              "      <td>0.003478</td>\n",
              "      <td>-0.046774</td>\n",
              "      <td>0.119164</td>\n",
              "      <td>0.074849</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.008826</td>\n",
              "      <td>-0.025064</td>\n",
              "      <td>0.057614</td>\n",
              "      <td>-0.072985</td>\n",
              "      <td>0.071129</td>\n",
              "      <td>0.053973</td>\n",
              "      <td>0.040873</td>\n",
              "      <td>0.061876</td>\n",
              "      <td>-0.005299</td>\n",
              "      <td>0.270148</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>65350</th>\n",
              "      <td>65526</td>\n",
              "      <td>1</td>\n",
              "      <td>([Ä , Ä remember, Ä day, Ä distinctively, ., Ä sitt...</td>\n",
              "      <td>1</td>\n",
              "      <td>0.012272</td>\n",
              "      <td>-0.009188</td>\n",
              "      <td>-0.117230</td>\n",
              "      <td>0.009980</td>\n",
              "      <td>0.058147</td>\n",
              "      <td>0.089310</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.029595</td>\n",
              "      <td>0.060013</td>\n",
              "      <td>-0.036095</td>\n",
              "      <td>0.105421</td>\n",
              "      <td>0.042180</td>\n",
              "      <td>0.089401</td>\n",
              "      <td>0.052531</td>\n",
              "      <td>0.021248</td>\n",
              "      <td>-0.058458</td>\n",
              "      <td>0.351528</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>65351</th>\n",
              "      <td>65527</td>\n",
              "      <td>1</td>\n",
              "      <td>([Ä , Ä dear, Ä senator, ,, Ä Ä , Ä writing, Ä letter...</td>\n",
              "      <td>1</td>\n",
              "      <td>-0.003841</td>\n",
              "      <td>0.184542</td>\n",
              "      <td>-0.018318</td>\n",
              "      <td>-0.000038</td>\n",
              "      <td>0.003925</td>\n",
              "      <td>0.038930</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.144254</td>\n",
              "      <td>0.024107</td>\n",
              "      <td>0.065593</td>\n",
              "      <td>-0.058660</td>\n",
              "      <td>0.093585</td>\n",
              "      <td>-0.003992</td>\n",
              "      <td>-0.023140</td>\n",
              "      <td>-0.023155</td>\n",
              "      <td>0.065200</td>\n",
              "      <td>0.295845</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>65352</th>\n",
              "      <td>65528</td>\n",
              "      <td>1</td>\n",
              "      <td>([Ä , Ä dear, Ä senator, ,, Ä , Ä writing, Ä urge, Ä ...</td>\n",
              "      <td>1</td>\n",
              "      <td>0.089967</td>\n",
              "      <td>0.021360</td>\n",
              "      <td>-0.036671</td>\n",
              "      <td>-0.036774</td>\n",
              "      <td>0.053695</td>\n",
              "      <td>0.083478</td>\n",
              "      <td>...</td>\n",
              "      <td>0.089856</td>\n",
              "      <td>-0.179279</td>\n",
              "      <td>0.073239</td>\n",
              "      <td>-0.074982</td>\n",
              "      <td>0.165204</td>\n",
              "      <td>0.026922</td>\n",
              "      <td>0.049378</td>\n",
              "      <td>0.014431</td>\n",
              "      <td>-0.028427</td>\n",
              "      <td>0.311500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>65353</th>\n",
              "      <td>65529</td>\n",
              "      <td>1</td>\n",
              "      <td>([Ä , Ä typical, Ä summer, Ä afternoon, Ä hometown,...</td>\n",
              "      <td>1</td>\n",
              "      <td>-0.059425</td>\n",
              "      <td>0.061315</td>\n",
              "      <td>-0.060917</td>\n",
              "      <td>-0.065748</td>\n",
              "      <td>0.001464</td>\n",
              "      <td>0.112835</td>\n",
              "      <td>...</td>\n",
              "      <td>0.053151</td>\n",
              "      <td>-0.110217</td>\n",
              "      <td>0.051984</td>\n",
              "      <td>0.060810</td>\n",
              "      <td>0.116694</td>\n",
              "      <td>0.006251</td>\n",
              "      <td>0.048848</td>\n",
              "      <td>0.047112</td>\n",
              "      <td>-0.030220</td>\n",
              "      <td>0.308790</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>65354 rows Ã— 105 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "             id  prompt_id                                               text  \\\n",
              "0      0059830c          0  ([Ä cars, ., Ä cars, Ä around, Ä since, Ä became, Ä ...   \n",
              "1      005db917          0  ([Ä transportation, Ä large, Ä necessity, Ä countr...   \n",
              "2      008f63e3          0  ([Ä \", america, Ä love, Ä affair, Ä vehicles, Ä see...   \n",
              "3      00940276          0  ([Ä often, Ä ride, Ä car, ?, Ä drive, Ä one, Ä motor...   \n",
              "4      00c39458          0  ([Ä cars, Ä wonderful, Ä thing, ., Ä perhaps, Ä one...   \n",
              "...         ...        ...                                                ...   \n",
              "65349     65525          1  ([Ä , Ä dear, Ä senator, ,, Ä , Ä writing, Ä regardi...   \n",
              "65350     65526          1  ([Ä , Ä remember, Ä day, Ä distinctively, ., Ä sitt...   \n",
              "65351     65527          1  ([Ä , Ä dear, Ä senator, ,, Ä Ä , Ä writing, Ä letter...   \n",
              "65352     65528          1  ([Ä , Ä dear, Ä senator, ,, Ä , Ä writing, Ä urge, Ä ...   \n",
              "65353     65529          1  ([Ä , Ä typical, Ä summer, Ä afternoon, Ä hometown,...   \n",
              "\n",
              "       generated         0         1         2         3         4         5  \\\n",
              "0              0  0.031006  0.005868  0.000175  0.040530 -0.008923 -0.024766   \n",
              "1              0 -0.009186 -0.049469 -0.024262 -0.027737  0.044426 -0.023337   \n",
              "2              0 -0.127259 -0.009322  0.094947  0.043751  0.081350 -0.062914   \n",
              "3              0 -0.083473 -0.041759 -0.095021  0.010098  0.053566 -0.077048   \n",
              "4              0 -0.100145 -0.040200  0.055441  0.077485 -0.011119 -0.083053   \n",
              "...          ...       ...       ...       ...       ...       ...       ...   \n",
              "65349          1 -0.001667 -0.005254  0.003478 -0.046774  0.119164  0.074849   \n",
              "65350          1  0.012272 -0.009188 -0.117230  0.009980  0.058147  0.089310   \n",
              "65351          1 -0.003841  0.184542 -0.018318 -0.000038  0.003925  0.038930   \n",
              "65352          1  0.089967  0.021360 -0.036671 -0.036774  0.053695  0.083478   \n",
              "65353          1 -0.059425  0.061315 -0.060917 -0.065748  0.001464  0.112835   \n",
              "\n",
              "       ...        91        92        93        94        95        96  \\\n",
              "0      ...  0.016213  0.091773 -0.018677  0.090013  0.123623 -0.023679   \n",
              "1      ...  0.060938 -0.018041  0.069725 -0.087793  0.045825 -0.038399   \n",
              "2      ...  0.066517  0.086545 -0.020416 -0.123838  0.002800  0.000657   \n",
              "3      ...  0.173633  0.068336  0.034193 -0.019671  0.026269 -0.020585   \n",
              "4      ...  0.073500 -0.042384 -0.065238  0.079913 -0.014149  0.003750   \n",
              "...    ...       ...       ...       ...       ...       ...       ...   \n",
              "65349  ... -0.008826 -0.025064  0.057614 -0.072985  0.071129  0.053973   \n",
              "65350  ... -0.029595  0.060013 -0.036095  0.105421  0.042180  0.089401   \n",
              "65351  ... -0.144254  0.024107  0.065593 -0.058660  0.093585 -0.003992   \n",
              "65352  ...  0.089856 -0.179279  0.073239 -0.074982  0.165204  0.026922   \n",
              "65353  ...  0.053151 -0.110217  0.051984  0.060810  0.116694  0.006251   \n",
              "\n",
              "             97        98        99  normalized_norm  \n",
              "0      0.007964 -0.097951 -0.013403         0.386702  \n",
              "1      0.020613  0.022102 -0.008245         0.370435  \n",
              "2     -0.040317 -0.002923 -0.069843         0.396177  \n",
              "3     -0.030012  0.073441 -0.113554         0.405722  \n",
              "4      0.039632  0.014942 -0.046859         0.451901  \n",
              "...         ...       ...       ...              ...  \n",
              "65349  0.040873  0.061876 -0.005299         0.270148  \n",
              "65350  0.052531  0.021248 -0.058458         0.351528  \n",
              "65351 -0.023140 -0.023155  0.065200         0.295845  \n",
              "65352  0.049378  0.014431 -0.028427         0.311500  \n",
              "65353  0.048848  0.047112 -0.030220         0.308790  \n",
              "\n",
              "[65354 rows x 105 columns]"
            ]
          },
          "execution_count": 54,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pre_processed_data = pd.concat([docs_dataset,norm_embeddings_dataset],axis=1)\n",
        "pre_processed_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ok4YnFIkdYKk"
      },
      "source": [
        "### Inferred Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "5LK85lV3dYKk"
      },
      "outputs": [],
      "source": [
        "def features(dataset):\n",
        "    token_count=dataset[\"text\"].apply(lambda x: len(x))\n",
        "    sentence_count = []\n",
        "    punctuation_count = []\n",
        "    apostrofees_count = []\n",
        "    unk_count = []\n",
        "    for doc in dataset[\"text\"]:\n",
        "        unk = 0\n",
        "        dot = 0\n",
        "        punctuation = 0\n",
        "        apostrofees = 0\n",
        "        for token in doc.words:\n",
        "            if(token.endswith(\".\")):\n",
        "                dot+=1\n",
        "                punctuation+=1\n",
        "            elif(token.endswith(\",\") or token.endswith(\"?\") or token.endswith(\"!\")):\n",
        "                punctuation+=1\n",
        "            elif(token.count(\"'\")>0):\n",
        "                    apostrofees+=token.count(\"'\")\n",
        "            elif(token==\"[UNK]\"):\n",
        "                unk+=1\n",
        "        sentence_count.append(dot)\n",
        "        punctuation_count.append(punctuation)\n",
        "        apostrofees_count.append(apostrofees)\n",
        "        unk_count.append(unk)\n",
        "    df = pd.DataFrame(\n",
        "        columns=[\"token_num\",\"sent_num\",\"punct_sym\",\"apostrof_sym\",\"unk_num\"]\n",
        "    )\n",
        "    df[\"token_num\"]=token_count\n",
        "    df[\"sent_num\"]=sentence_count\n",
        "    df[\"punct_sym\"]=punctuation_count\n",
        "    df[\"apostrof_sym\"]=apostrofees_count\n",
        "    df[\"unk_num\"]=unk_count\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>token_num</th>\n",
              "      <th>sent_num</th>\n",
              "      <th>punct_sym</th>\n",
              "      <th>apostrof_sym</th>\n",
              "      <th>unk_num</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2</td>\n",
              "      <td>24</td>\n",
              "      <td>61</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>26</td>\n",
              "      <td>51</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>48</td>\n",
              "      <td>77</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2</td>\n",
              "      <td>35</td>\n",
              "      <td>80</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2</td>\n",
              "      <td>48</td>\n",
              "      <td>84</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>65349</th>\n",
              "      <td>2</td>\n",
              "      <td>18</td>\n",
              "      <td>36</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>65350</th>\n",
              "      <td>2</td>\n",
              "      <td>23</td>\n",
              "      <td>41</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>65351</th>\n",
              "      <td>2</td>\n",
              "      <td>20</td>\n",
              "      <td>39</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>65352</th>\n",
              "      <td>2</td>\n",
              "      <td>18</td>\n",
              "      <td>35</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>65353</th>\n",
              "      <td>2</td>\n",
              "      <td>14</td>\n",
              "      <td>21</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>65354 rows Ã— 5 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       token_num  sent_num  punct_sym  apostrof_sym  unk_num\n",
              "0              2        24         61             0        0\n",
              "1              2        26         51             2        0\n",
              "2              2        48         77             1        0\n",
              "3              2        35         80             0        0\n",
              "4              2        48         84             1        0\n",
              "...          ...       ...        ...           ...      ...\n",
              "65349          2        18         36             0        0\n",
              "65350          2        23         41             0        0\n",
              "65351          2        20         39             0        0\n",
              "65352          2        18         35             0        0\n",
              "65353          2        14         21             0        0\n",
              "\n",
              "[65354 rows x 5 columns]"
            ]
          },
          "execution_count": 56,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_features = features(pre_processed_data)\n",
        "train_features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>token_num</th>\n",
              "      <th>sent_num</th>\n",
              "      <th>punct_sym</th>\n",
              "      <th>apostrof_sym</th>\n",
              "      <th>unk_num</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.003912</td>\n",
              "      <td>0.004492</td>\n",
              "      <td>0.005521</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.003912</td>\n",
              "      <td>0.004866</td>\n",
              "      <td>0.004616</td>\n",
              "      <td>0.004868</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.003912</td>\n",
              "      <td>0.008983</td>\n",
              "      <td>0.006969</td>\n",
              "      <td>0.002434</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.003912</td>\n",
              "      <td>0.006550</td>\n",
              "      <td>0.007240</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.003912</td>\n",
              "      <td>0.008983</td>\n",
              "      <td>0.007602</td>\n",
              "      <td>0.002434</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>65349</th>\n",
              "      <td>0.003912</td>\n",
              "      <td>0.003369</td>\n",
              "      <td>0.003258</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>65350</th>\n",
              "      <td>0.003912</td>\n",
              "      <td>0.004305</td>\n",
              "      <td>0.003711</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>65351</th>\n",
              "      <td>0.003912</td>\n",
              "      <td>0.003743</td>\n",
              "      <td>0.003530</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>65352</th>\n",
              "      <td>0.003912</td>\n",
              "      <td>0.003369</td>\n",
              "      <td>0.003168</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>65353</th>\n",
              "      <td>0.003912</td>\n",
              "      <td>0.002620</td>\n",
              "      <td>0.001901</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>65354 rows Ã— 5 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       token_num  sent_num  punct_sym  apostrof_sym  unk_num\n",
              "0       0.003912  0.004492   0.005521      0.000000        0\n",
              "1       0.003912  0.004866   0.004616      0.004868        0\n",
              "2       0.003912  0.008983   0.006969      0.002434        0\n",
              "3       0.003912  0.006550   0.007240      0.000000        0\n",
              "4       0.003912  0.008983   0.007602      0.002434        0\n",
              "...          ...       ...        ...           ...      ...\n",
              "65349   0.003912  0.003369   0.003258      0.000000        0\n",
              "65350   0.003912  0.004305   0.003711      0.000000        0\n",
              "65351   0.003912  0.003743   0.003530      0.000000        0\n",
              "65352   0.003912  0.003369   0.003168      0.000000        0\n",
              "65353   0.003912  0.002620   0.001901      0.000000        0\n",
              "\n",
              "[65354 rows x 5 columns]"
            ]
          },
          "execution_count": 57,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "for col in train_features.columns:\n",
        "    if(train_features[col].max()>0):\n",
        "        train_features[col] = train_features[col]/np.linalg.norm(train_features[col])\n",
        "train_features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {},
      "outputs": [],
      "source": [
        "pre_processed_data=pd.concat([pre_processed_data,train_features],axis=1).drop(\"text\",axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "id": "KKJrJzEWdYKl",
        "outputId": "50ad4a58-1b24-408d-db60-c5bad8697938"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>prompt_id</th>\n",
              "      <th>generated</th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>...</th>\n",
              "      <th>96</th>\n",
              "      <th>97</th>\n",
              "      <th>98</th>\n",
              "      <th>99</th>\n",
              "      <th>normalized_norm</th>\n",
              "      <th>token_num</th>\n",
              "      <th>sent_num</th>\n",
              "      <th>punct_sym</th>\n",
              "      <th>apostrof_sym</th>\n",
              "      <th>unk_num</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>65354.000000</td>\n",
              "      <td>65354.000000</td>\n",
              "      <td>65354.000000</td>\n",
              "      <td>65354.000000</td>\n",
              "      <td>65354.000000</td>\n",
              "      <td>65354.000000</td>\n",
              "      <td>65354.000000</td>\n",
              "      <td>65354.000000</td>\n",
              "      <td>65354.000000</td>\n",
              "      <td>65354.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>65354.000000</td>\n",
              "      <td>65354.000000</td>\n",
              "      <td>65354.000000</td>\n",
              "      <td>65354.000000</td>\n",
              "      <td>65354.000000</td>\n",
              "      <td>6.535400e+04</td>\n",
              "      <td>65354.000000</td>\n",
              "      <td>65354.000000</td>\n",
              "      <td>65354.000000</td>\n",
              "      <td>65354.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>16.330110</td>\n",
              "      <td>0.581189</td>\n",
              "      <td>0.026653</td>\n",
              "      <td>-0.009464</td>\n",
              "      <td>-0.000197</td>\n",
              "      <td>0.016458</td>\n",
              "      <td>0.010459</td>\n",
              "      <td>0.002886</td>\n",
              "      <td>-0.015876</td>\n",
              "      <td>0.019336</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.004515</td>\n",
              "      <td>-0.019319</td>\n",
              "      <td>0.046191</td>\n",
              "      <td>-0.022246</td>\n",
              "      <td>0.304545</td>\n",
              "      <td>3.911685e-03</td>\n",
              "      <td>0.003592</td>\n",
              "      <td>0.003512</td>\n",
              "      <td>0.001235</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>8.604177</td>\n",
              "      <td>0.493368</td>\n",
              "      <td>0.064862</td>\n",
              "      <td>0.066234</td>\n",
              "      <td>0.062498</td>\n",
              "      <td>0.063825</td>\n",
              "      <td>0.063946</td>\n",
              "      <td>0.070003</td>\n",
              "      <td>0.068782</td>\n",
              "      <td>0.068711</td>\n",
              "      <td>...</td>\n",
              "      <td>0.065461</td>\n",
              "      <td>0.063731</td>\n",
              "      <td>0.069474</td>\n",
              "      <td>0.064941</td>\n",
              "      <td>0.060327</td>\n",
              "      <td>1.734737e-18</td>\n",
              "      <td>0.001549</td>\n",
              "      <td>0.001722</td>\n",
              "      <td>0.003712</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.243751</td>\n",
              "      <td>-0.276438</td>\n",
              "      <td>-0.263549</td>\n",
              "      <td>-0.240561</td>\n",
              "      <td>-0.263634</td>\n",
              "      <td>-0.293598</td>\n",
              "      <td>-0.275147</td>\n",
              "      <td>-0.272187</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.265264</td>\n",
              "      <td>-0.314676</td>\n",
              "      <td>-0.240049</td>\n",
              "      <td>-0.276494</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.911685e-03</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>21.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.016567</td>\n",
              "      <td>-0.054284</td>\n",
              "      <td>-0.042844</td>\n",
              "      <td>-0.026465</td>\n",
              "      <td>-0.032741</td>\n",
              "      <td>-0.044704</td>\n",
              "      <td>-0.062579</td>\n",
              "      <td>-0.027168</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.048966</td>\n",
              "      <td>-0.062524</td>\n",
              "      <td>-0.001581</td>\n",
              "      <td>-0.066674</td>\n",
              "      <td>0.261894</td>\n",
              "      <td>3.911685e-03</td>\n",
              "      <td>0.002620</td>\n",
              "      <td>0.002263</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>21.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.027946</td>\n",
              "      <td>-0.009526</td>\n",
              "      <td>-0.000930</td>\n",
              "      <td>0.017321</td>\n",
              "      <td>0.009958</td>\n",
              "      <td>0.003703</td>\n",
              "      <td>-0.015042</td>\n",
              "      <td>0.019793</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.005310</td>\n",
              "      <td>-0.019364</td>\n",
              "      <td>0.045991</td>\n",
              "      <td>-0.022730</td>\n",
              "      <td>0.304718</td>\n",
              "      <td>3.911685e-03</td>\n",
              "      <td>0.003369</td>\n",
              "      <td>0.003349</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>21.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.071248</td>\n",
              "      <td>0.035421</td>\n",
              "      <td>0.041639</td>\n",
              "      <td>0.059711</td>\n",
              "      <td>0.053335</td>\n",
              "      <td>0.051042</td>\n",
              "      <td>0.031089</td>\n",
              "      <td>0.066195</td>\n",
              "      <td>...</td>\n",
              "      <td>0.039671</td>\n",
              "      <td>0.024117</td>\n",
              "      <td>0.093929</td>\n",
              "      <td>0.021511</td>\n",
              "      <td>0.344303</td>\n",
              "      <td>3.911685e-03</td>\n",
              "      <td>0.004492</td>\n",
              "      <td>0.004435</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>21.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.288341</td>\n",
              "      <td>0.249216</td>\n",
              "      <td>0.279243</td>\n",
              "      <td>0.270375</td>\n",
              "      <td>0.282558</td>\n",
              "      <td>0.262080</td>\n",
              "      <td>0.239142</td>\n",
              "      <td>0.295370</td>\n",
              "      <td>...</td>\n",
              "      <td>0.282360</td>\n",
              "      <td>0.243853</td>\n",
              "      <td>0.311407</td>\n",
              "      <td>0.263388</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>3.911685e-03</td>\n",
              "      <td>0.019090</td>\n",
              "      <td>0.027604</td>\n",
              "      <td>0.197141</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>8 rows Ã— 108 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "          prompt_id     generated             0             1             2  \\\n",
              "count  65354.000000  65354.000000  65354.000000  65354.000000  65354.000000   \n",
              "mean      16.330110      0.581189      0.026653     -0.009464     -0.000197   \n",
              "std        8.604177      0.493368      0.064862      0.066234      0.062498   \n",
              "min        0.000000      0.000000     -0.243751     -0.276438     -0.263549   \n",
              "25%       21.000000      0.000000     -0.016567     -0.054284     -0.042844   \n",
              "50%       21.000000      1.000000      0.027946     -0.009526     -0.000930   \n",
              "75%       21.000000      1.000000      0.071248      0.035421      0.041639   \n",
              "max       21.000000      1.000000      0.288341      0.249216      0.279243   \n",
              "\n",
              "                  3             4             5             6             7  \\\n",
              "count  65354.000000  65354.000000  65354.000000  65354.000000  65354.000000   \n",
              "mean       0.016458      0.010459      0.002886     -0.015876      0.019336   \n",
              "std        0.063825      0.063946      0.070003      0.068782      0.068711   \n",
              "min       -0.240561     -0.263634     -0.293598     -0.275147     -0.272187   \n",
              "25%       -0.026465     -0.032741     -0.044704     -0.062579     -0.027168   \n",
              "50%        0.017321      0.009958      0.003703     -0.015042      0.019793   \n",
              "75%        0.059711      0.053335      0.051042      0.031089      0.066195   \n",
              "max        0.270375      0.282558      0.262080      0.239142      0.295370   \n",
              "\n",
              "       ...            96            97            98            99  \\\n",
              "count  ...  65354.000000  65354.000000  65354.000000  65354.000000   \n",
              "mean   ...     -0.004515     -0.019319      0.046191     -0.022246   \n",
              "std    ...      0.065461      0.063731      0.069474      0.064941   \n",
              "min    ...     -0.265264     -0.314676     -0.240049     -0.276494   \n",
              "25%    ...     -0.048966     -0.062524     -0.001581     -0.066674   \n",
              "50%    ...     -0.005310     -0.019364      0.045991     -0.022730   \n",
              "75%    ...      0.039671      0.024117      0.093929      0.021511   \n",
              "max    ...      0.282360      0.243853      0.311407      0.263388   \n",
              "\n",
              "       normalized_norm     token_num      sent_num     punct_sym  \\\n",
              "count     65354.000000  6.535400e+04  65354.000000  65354.000000   \n",
              "mean          0.304545  3.911685e-03      0.003592      0.003512   \n",
              "std           0.060327  1.734737e-18      0.001549      0.001722   \n",
              "min           0.000000  3.911685e-03      0.000000      0.000000   \n",
              "25%           0.261894  3.911685e-03      0.002620      0.002263   \n",
              "50%           0.304718  3.911685e-03      0.003369      0.003349   \n",
              "75%           0.344303  3.911685e-03      0.004492      0.004435   \n",
              "max           1.000000  3.911685e-03      0.019090      0.027604   \n",
              "\n",
              "       apostrof_sym  unk_num  \n",
              "count  65354.000000  65354.0  \n",
              "mean       0.001235      0.0  \n",
              "std        0.003712      0.0  \n",
              "min        0.000000      0.0  \n",
              "25%        0.000000      0.0  \n",
              "50%        0.000000      0.0  \n",
              "75%        0.000000      0.0  \n",
              "max        0.197141      0.0  \n",
              "\n",
              "[8 rows x 108 columns]"
            ]
          },
          "execution_count": 59,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pre_processed_data.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {},
      "outputs": [],
      "source": [
        "pre_processed_data.to_csv(\"../data/pre_process_train.csv\",index=False)\n",
        "#pre_processed_data.loc[:, ~pre_processed_data.columns.str.contains('^Unnamed')].to_csv(\"../data/pre_process_train.csv\",index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pboIBijydYKl"
      },
      "source": [
        "## Model Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_35963/184775720.py:1: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  pre_processed_data=pd.read_csv(\"../../data/pre_process_train.csv\")\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>prompt_id</th>\n",
              "      <th>generated</th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>...</th>\n",
              "      <th>96</th>\n",
              "      <th>97</th>\n",
              "      <th>98</th>\n",
              "      <th>99</th>\n",
              "      <th>normalized_norm</th>\n",
              "      <th>token_num</th>\n",
              "      <th>sent_num</th>\n",
              "      <th>punct_sym</th>\n",
              "      <th>apostrof_sym</th>\n",
              "      <th>unk_num</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0059830c</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.031006</td>\n",
              "      <td>0.005868</td>\n",
              "      <td>0.000175</td>\n",
              "      <td>0.040530</td>\n",
              "      <td>-0.008923</td>\n",
              "      <td>-0.024766</td>\n",
              "      <td>-0.075533</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.023679</td>\n",
              "      <td>0.007964</td>\n",
              "      <td>-0.097951</td>\n",
              "      <td>-0.013403</td>\n",
              "      <td>0.386702</td>\n",
              "      <td>0.003912</td>\n",
              "      <td>0.004492</td>\n",
              "      <td>0.005521</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>005db917</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.009186</td>\n",
              "      <td>-0.049469</td>\n",
              "      <td>-0.024262</td>\n",
              "      <td>-0.027737</td>\n",
              "      <td>0.044426</td>\n",
              "      <td>-0.023337</td>\n",
              "      <td>-0.037656</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.038399</td>\n",
              "      <td>0.020613</td>\n",
              "      <td>0.022102</td>\n",
              "      <td>-0.008245</td>\n",
              "      <td>0.370435</td>\n",
              "      <td>0.003912</td>\n",
              "      <td>0.004866</td>\n",
              "      <td>0.004616</td>\n",
              "      <td>0.004868</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>008f63e3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.127259</td>\n",
              "      <td>-0.009322</td>\n",
              "      <td>0.094947</td>\n",
              "      <td>0.043751</td>\n",
              "      <td>0.081350</td>\n",
              "      <td>-0.062914</td>\n",
              "      <td>-0.065543</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000657</td>\n",
              "      <td>-0.040317</td>\n",
              "      <td>-0.002923</td>\n",
              "      <td>-0.069843</td>\n",
              "      <td>0.396177</td>\n",
              "      <td>0.003912</td>\n",
              "      <td>0.008983</td>\n",
              "      <td>0.006969</td>\n",
              "      <td>0.002434</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>00940276</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.083473</td>\n",
              "      <td>-0.041759</td>\n",
              "      <td>-0.095021</td>\n",
              "      <td>0.010098</td>\n",
              "      <td>0.053566</td>\n",
              "      <td>-0.077048</td>\n",
              "      <td>-0.098504</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.020585</td>\n",
              "      <td>-0.030012</td>\n",
              "      <td>0.073441</td>\n",
              "      <td>-0.113554</td>\n",
              "      <td>0.405722</td>\n",
              "      <td>0.003912</td>\n",
              "      <td>0.006550</td>\n",
              "      <td>0.007240</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>00c39458</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.100145</td>\n",
              "      <td>-0.040200</td>\n",
              "      <td>0.055441</td>\n",
              "      <td>0.077485</td>\n",
              "      <td>-0.011119</td>\n",
              "      <td>-0.083053</td>\n",
              "      <td>-0.108298</td>\n",
              "      <td>...</td>\n",
              "      <td>0.003750</td>\n",
              "      <td>0.039632</td>\n",
              "      <td>0.014942</td>\n",
              "      <td>-0.046859</td>\n",
              "      <td>0.451901</td>\n",
              "      <td>0.003912</td>\n",
              "      <td>0.008983</td>\n",
              "      <td>0.007602</td>\n",
              "      <td>0.002434</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>65349</th>\n",
              "      <td>65525</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>-0.001667</td>\n",
              "      <td>-0.005254</td>\n",
              "      <td>0.003478</td>\n",
              "      <td>-0.046774</td>\n",
              "      <td>0.119164</td>\n",
              "      <td>0.074849</td>\n",
              "      <td>0.061789</td>\n",
              "      <td>...</td>\n",
              "      <td>0.053973</td>\n",
              "      <td>0.040873</td>\n",
              "      <td>0.061876</td>\n",
              "      <td>-0.005299</td>\n",
              "      <td>0.270148</td>\n",
              "      <td>0.003912</td>\n",
              "      <td>0.003369</td>\n",
              "      <td>0.003258</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>65350</th>\n",
              "      <td>65526</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0.012272</td>\n",
              "      <td>-0.009188</td>\n",
              "      <td>-0.117230</td>\n",
              "      <td>0.009980</td>\n",
              "      <td>0.058147</td>\n",
              "      <td>0.089310</td>\n",
              "      <td>0.026512</td>\n",
              "      <td>...</td>\n",
              "      <td>0.089401</td>\n",
              "      <td>0.052531</td>\n",
              "      <td>0.021248</td>\n",
              "      <td>-0.058458</td>\n",
              "      <td>0.351528</td>\n",
              "      <td>0.003912</td>\n",
              "      <td>0.004305</td>\n",
              "      <td>0.003711</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>65351</th>\n",
              "      <td>65527</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>-0.003841</td>\n",
              "      <td>0.184542</td>\n",
              "      <td>-0.018318</td>\n",
              "      <td>-0.000038</td>\n",
              "      <td>0.003925</td>\n",
              "      <td>0.038930</td>\n",
              "      <td>-0.023688</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.003992</td>\n",
              "      <td>-0.023140</td>\n",
              "      <td>-0.023155</td>\n",
              "      <td>0.065200</td>\n",
              "      <td>0.295845</td>\n",
              "      <td>0.003912</td>\n",
              "      <td>0.003743</td>\n",
              "      <td>0.003530</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>65352</th>\n",
              "      <td>65528</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0.089967</td>\n",
              "      <td>0.021360</td>\n",
              "      <td>-0.036671</td>\n",
              "      <td>-0.036774</td>\n",
              "      <td>0.053695</td>\n",
              "      <td>0.083478</td>\n",
              "      <td>0.015443</td>\n",
              "      <td>...</td>\n",
              "      <td>0.026922</td>\n",
              "      <td>0.049378</td>\n",
              "      <td>0.014431</td>\n",
              "      <td>-0.028427</td>\n",
              "      <td>0.311500</td>\n",
              "      <td>0.003912</td>\n",
              "      <td>0.003369</td>\n",
              "      <td>0.003168</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>65353</th>\n",
              "      <td>65529</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>-0.059425</td>\n",
              "      <td>0.061315</td>\n",
              "      <td>-0.060917</td>\n",
              "      <td>-0.065748</td>\n",
              "      <td>0.001464</td>\n",
              "      <td>0.112835</td>\n",
              "      <td>0.045875</td>\n",
              "      <td>...</td>\n",
              "      <td>0.006251</td>\n",
              "      <td>0.048848</td>\n",
              "      <td>0.047112</td>\n",
              "      <td>-0.030220</td>\n",
              "      <td>0.308790</td>\n",
              "      <td>0.003912</td>\n",
              "      <td>0.002620</td>\n",
              "      <td>0.001901</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>65354 rows Ã— 109 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "             id  prompt_id  generated         0         1         2         3  \\\n",
              "0      0059830c          0          0  0.031006  0.005868  0.000175  0.040530   \n",
              "1      005db917          0          0 -0.009186 -0.049469 -0.024262 -0.027737   \n",
              "2      008f63e3          0          0 -0.127259 -0.009322  0.094947  0.043751   \n",
              "3      00940276          0          0 -0.083473 -0.041759 -0.095021  0.010098   \n",
              "4      00c39458          0          0 -0.100145 -0.040200  0.055441  0.077485   \n",
              "...         ...        ...        ...       ...       ...       ...       ...   \n",
              "65349     65525          1          1 -0.001667 -0.005254  0.003478 -0.046774   \n",
              "65350     65526          1          1  0.012272 -0.009188 -0.117230  0.009980   \n",
              "65351     65527          1          1 -0.003841  0.184542 -0.018318 -0.000038   \n",
              "65352     65528          1          1  0.089967  0.021360 -0.036671 -0.036774   \n",
              "65353     65529          1          1 -0.059425  0.061315 -0.060917 -0.065748   \n",
              "\n",
              "              4         5         6  ...        96        97        98  \\\n",
              "0     -0.008923 -0.024766 -0.075533  ... -0.023679  0.007964 -0.097951   \n",
              "1      0.044426 -0.023337 -0.037656  ... -0.038399  0.020613  0.022102   \n",
              "2      0.081350 -0.062914 -0.065543  ...  0.000657 -0.040317 -0.002923   \n",
              "3      0.053566 -0.077048 -0.098504  ... -0.020585 -0.030012  0.073441   \n",
              "4     -0.011119 -0.083053 -0.108298  ...  0.003750  0.039632  0.014942   \n",
              "...         ...       ...       ...  ...       ...       ...       ...   \n",
              "65349  0.119164  0.074849  0.061789  ...  0.053973  0.040873  0.061876   \n",
              "65350  0.058147  0.089310  0.026512  ...  0.089401  0.052531  0.021248   \n",
              "65351  0.003925  0.038930 -0.023688  ... -0.003992 -0.023140 -0.023155   \n",
              "65352  0.053695  0.083478  0.015443  ...  0.026922  0.049378  0.014431   \n",
              "65353  0.001464  0.112835  0.045875  ...  0.006251  0.048848  0.047112   \n",
              "\n",
              "             99  normalized_norm  token_num  sent_num  punct_sym  \\\n",
              "0     -0.013403         0.386702   0.003912  0.004492   0.005521   \n",
              "1     -0.008245         0.370435   0.003912  0.004866   0.004616   \n",
              "2     -0.069843         0.396177   0.003912  0.008983   0.006969   \n",
              "3     -0.113554         0.405722   0.003912  0.006550   0.007240   \n",
              "4     -0.046859         0.451901   0.003912  0.008983   0.007602   \n",
              "...         ...              ...        ...       ...        ...   \n",
              "65349 -0.005299         0.270148   0.003912  0.003369   0.003258   \n",
              "65350 -0.058458         0.351528   0.003912  0.004305   0.003711   \n",
              "65351  0.065200         0.295845   0.003912  0.003743   0.003530   \n",
              "65352 -0.028427         0.311500   0.003912  0.003369   0.003168   \n",
              "65353 -0.030220         0.308790   0.003912  0.002620   0.001901   \n",
              "\n",
              "       apostrof_sym  unk_num  \n",
              "0          0.000000        0  \n",
              "1          0.004868        0  \n",
              "2          0.002434        0  \n",
              "3          0.000000        0  \n",
              "4          0.002434        0  \n",
              "...             ...      ...  \n",
              "65349      0.000000        0  \n",
              "65350      0.000000        0  \n",
              "65351      0.000000        0  \n",
              "65352      0.000000        0  \n",
              "65353      0.000000        0  \n",
              "\n",
              "[65354 rows x 109 columns]"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pre_processed_data=pd.read_csv(\"../../data/pre_process_train.csv\")\n",
        "pre_processed_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "sieb_gK4dYKl"
      },
      "outputs": [],
      "source": [
        "def downsampling(X_train,y_train,rand_state,ratio):\n",
        "    # Concatenate X_train and y_train for ease of downsampling\n",
        "    train_data = pd.concat([X_train, y_train], axis=1)\n",
        "\n",
        "    # Identify the class with the majority of instances\n",
        "    majority_class = train_data[y_train.name].value_counts().idxmax()\n",
        "\n",
        "    # Separate instances of each class\n",
        "    majority_class_data = train_data[train_data[y_train.name] == majority_class]\n",
        "    minority_class_data = train_data[train_data[y_train.name] != majority_class]\n",
        "\n",
        "\n",
        "    currRatio = len(majority_class_data) / len(minority_class_data)\n",
        "    if(currRatio<=ratio):\n",
        "        raise ValueError(f\"El ratio de entrada debe ser menor al ratio actual.r={currRatio}\")\n",
        "\n",
        "\n",
        "    # Downsample the majority class to match the number of instances in the minority class\n",
        "    downsampled_majority_class_data = resample(\n",
        "        majority_class_data,\n",
        "        replace=False,\n",
        "        n_samples=math.floor(ratio*len(minority_class_data)),\n",
        "        random_state=rand_state\n",
        "    )\n",
        "\n",
        "    # Combine the downsampled majority class with the minority class\n",
        "    downsampled_train_data = pd.concat([downsampled_majority_class_data, minority_class_data])\n",
        "\n",
        "    # Shuffle the data to avoid any order-based patterns\n",
        "    downsampled_train_data = downsampled_train_data.sample(frac=1, random_state=rand_state)\n",
        "\n",
        "    # Separate X_train and y_train after downsampling\n",
        "    X_train_downsampled = downsampled_train_data.drop(columns=[y_train.name])\n",
        "    y_train_downsampled = downsampled_train_data[y_train.name]\n",
        "\n",
        "    return X_train_downsampled, y_train_downsampled"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Dense connected Neural Network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Arquitecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "input = keras.Input(shape=(pre_processed_data.shape[1]-2,))\n",
        "x = keras.layers.Dense(pre_processed_data.shape[1]-2,activation=\"relu\")(input)\n",
        "x = keras.layers.Dense(100,activation=\"relu\")(input)\n",
        "output = keras.layers.Dense(1,activation=\"sigmoid\")(x)\n",
        "\n",
        "NNmodel = keras.Model(input,output)\n",
        "\n",
        "NNmodel.compile(optimizer='adam',\n",
        "               loss=keras.losses.BinaryCrossentropy(),\n",
        "               metrics=[keras.metrics.AUC()])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "eE-YSa9jdYKm",
        "outputId": "1017a0f1-e1de-49da-ed83-603324aa1644"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DOWNSAMPLED:(24633:32022)\n",
            "Epoch 1/25\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-01-08 00:23:24.384589: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 48496680 exceeds 10% of free system memory.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "57/57 [==============================] - 1s 7ms/step - loss: 0.7315 - auc: 0.5868 - val_loss: 0.6212 - val_auc: 0.8080\n",
            "Epoch 2/25\n",
            "57/57 [==============================] - 0s 5ms/step - loss: 0.5874 - auc: 0.8100 - val_loss: 0.5395 - val_auc: 0.8378\n",
            "Epoch 3/25\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.5173 - auc: 0.8388 - val_loss: 0.4847 - val_auc: 0.8548\n",
            "Epoch 4/25\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.4779 - auc: 0.8575 - val_loss: 0.4601 - val_auc: 0.8680\n",
            "Epoch 5/25\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.4560 - auc: 0.8695 - val_loss: 0.4398 - val_auc: 0.8790\n",
            "Epoch 6/25\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.4402 - auc: 0.8787 - val_loss: 0.4255 - val_auc: 0.8870\n",
            "Epoch 7/25\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.4277 - auc: 0.8857 - val_loss: 0.4134 - val_auc: 0.8936\n",
            "Epoch 8/25\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.4163 - auc: 0.8923 - val_loss: 0.4050 - val_auc: 0.9008\n",
            "Epoch 9/25\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.4069 - auc: 0.8973 - val_loss: 0.3917 - val_auc: 0.9060\n",
            "Epoch 10/25\n",
            "57/57 [==============================] - 0s 4ms/step - loss: 0.3972 - auc: 0.9031 - val_loss: 0.3828 - val_auc: 0.9108\n",
            "Epoch 11/25\n",
            "57/57 [==============================] - 0s 4ms/step - loss: 0.3883 - auc: 0.9077 - val_loss: 0.3753 - val_auc: 0.9151\n",
            "Epoch 12/25\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.3810 - auc: 0.9114 - val_loss: 0.3671 - val_auc: 0.9196\n",
            "Epoch 13/25\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.3737 - auc: 0.9151 - val_loss: 0.3609 - val_auc: 0.9225\n",
            "Epoch 14/25\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.3675 - auc: 0.9179 - val_loss: 0.3523 - val_auc: 0.9257\n",
            "Epoch 15/25\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.3603 - auc: 0.9215 - val_loss: 0.3474 - val_auc: 0.9283\n",
            "Epoch 16/25\n",
            "57/57 [==============================] - 0s 4ms/step - loss: 0.3548 - auc: 0.9240 - val_loss: 0.3404 - val_auc: 0.9312\n",
            "Epoch 17/25\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.3491 - auc: 0.9266 - val_loss: 0.3342 - val_auc: 0.9339\n",
            "Epoch 18/25\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.3431 - auc: 0.9292 - val_loss: 0.3288 - val_auc: 0.9361\n",
            "Epoch 19/25\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.3382 - auc: 0.9313 - val_loss: 0.3235 - val_auc: 0.9383\n",
            "Epoch 20/25\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.3334 - auc: 0.9333 - val_loss: 0.3189 - val_auc: 0.9399\n",
            "Epoch 21/25\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.3286 - auc: 0.9353 - val_loss: 0.3158 - val_auc: 0.9418\n",
            "Epoch 22/25\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.3243 - auc: 0.9370 - val_loss: 0.3098 - val_auc: 0.9437\n",
            "Epoch 23/25\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.3195 - auc: 0.9389 - val_loss: 0.3063 - val_auc: 0.9452\n",
            "Epoch 24/25\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.3158 - auc: 0.9403 - val_loss: 0.3027 - val_auc: 0.9466\n",
            "Epoch 25/25\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.3119 - auc: 0.9419 - val_loss: 0.3030 - val_auc: 0.9478\n",
            "DOWNSAMPLED:(24634:32024)\n",
            "Epoch 1/25\n",
            "42/57 [=====================>........] - ETA: 0s - loss: 0.3116 - auc: 0.9415"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-01-08 00:23:30.370940: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 48499248 exceeds 10% of free system memory.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "57/57 [==============================] - 0s 4ms/step - loss: 0.3098 - auc: 0.9424 - val_loss: 0.2967 - val_auc: 0.9478\n",
            "Epoch 2/25\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.3056 - auc: 0.9441 - val_loss: 0.2947 - val_auc: 0.9489\n",
            "Epoch 3/25\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.3015 - auc: 0.9457 - val_loss: 0.2928 - val_auc: 0.9497\n",
            "Epoch 4/25\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.2987 - auc: 0.9466 - val_loss: 0.2889 - val_auc: 0.9503\n",
            "Epoch 5/25\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.2952 - auc: 0.9480 - val_loss: 0.2850 - val_auc: 0.9513\n",
            "Epoch 6/25\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.2925 - auc: 0.9488 - val_loss: 0.2859 - val_auc: 0.9517\n",
            "Epoch 7/25\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.2897 - auc: 0.9497 - val_loss: 0.2840 - val_auc: 0.9523\n",
            "Epoch 8/25\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.2866 - auc: 0.9509 - val_loss: 0.2817 - val_auc: 0.9531\n",
            "Epoch 9/25\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.2846 - auc: 0.9515 - val_loss: 0.2784 - val_auc: 0.9538\n",
            "Epoch 10/25\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.2807 - auc: 0.9529 - val_loss: 0.2749 - val_auc: 0.9543\n",
            "Epoch 11/25\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.2783 - auc: 0.9537 - val_loss: 0.2736 - val_auc: 0.9548\n",
            "Epoch 12/25\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.2758 - auc: 0.9545 - val_loss: 0.2713 - val_auc: 0.9553\n",
            "Epoch 13/25\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.2734 - auc: 0.9553 - val_loss: 0.2722 - val_auc: 0.9556\n",
            "Epoch 14/25\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.2719 - auc: 0.9556 - val_loss: 0.2688 - val_auc: 0.9563\n",
            "Epoch 15/25\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.2701 - auc: 0.9562 - val_loss: 0.2678 - val_auc: 0.9565\n",
            "Epoch 16/25\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.2678 - auc: 0.9569 - val_loss: 0.2658 - val_auc: 0.9569\n",
            "Epoch 17/25\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.2658 - auc: 0.9576 - val_loss: 0.2649 - val_auc: 0.9570\n",
            "Epoch 18/25\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.2656 - auc: 0.9574 - val_loss: 0.2636 - val_auc: 0.9576\n",
            "Epoch 19/25\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.2629 - auc: 0.9583 - val_loss: 0.2637 - val_auc: 0.9578\n",
            "Epoch 20/25\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.2606 - auc: 0.9590 - val_loss: 0.2612 - val_auc: 0.9582\n",
            "Epoch 21/25\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.2591 - auc: 0.9594 - val_loss: 0.2639 - val_auc: 0.9583\n",
            "Epoch 22/25\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.2578 - auc: 0.9598 - val_loss: 0.2642 - val_auc: 0.9583\n",
            "Epoch 23/25\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.2562 - auc: 0.9603 - val_loss: 0.2577 - val_auc: 0.9589\n",
            "Epoch 24/25\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.2549 - auc: 0.9606 - val_loss: 0.2567 - val_auc: 0.9592\n",
            "Epoch 25/25\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.2530 - auc: 0.9612 - val_loss: 0.2555 - val_auc: 0.9596\n",
            "DOWNSAMPLED:(24634:32024)\n",
            "Epoch 1/25\n",
            "41/57 [====================>.........] - ETA: 0s - loss: 0.2532 - auc: 0.9610"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-01-08 00:23:35.516129: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 48499248 exceeds 10% of free system memory.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "57/57 [==============================] - 0s 4ms/step - loss: 0.2520 - auc: 0.9614 - val_loss: 0.2533 - val_auc: 0.9606\n",
            "Epoch 2/25\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.2502 - auc: 0.9619 - val_loss: 0.2582 - val_auc: 0.9603\n",
            "Epoch 3/25\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.2489 - auc: 0.9623 - val_loss: 0.2534 - val_auc: 0.9607\n",
            "Epoch 4/25\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.2482 - auc: 0.9625 - val_loss: 0.2554 - val_auc: 0.9610\n",
            "Epoch 5/25\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.2471 - auc: 0.9628 - val_loss: 0.2517 - val_auc: 0.9610\n",
            "Epoch 6/25\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.2449 - auc: 0.9635 - val_loss: 0.2557 - val_auc: 0.9613\n",
            "Epoch 7/25\n",
            "57/57 [==============================] - 0s 4ms/step - loss: 0.2436 - auc: 0.9639 - val_loss: 0.2512 - val_auc: 0.9615\n",
            "Epoch 8/25\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.2426 - auc: 0.9641 - val_loss: 0.2499 - val_auc: 0.9617\n",
            "Epoch 9/25\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.2416 - auc: 0.9644 - val_loss: 0.2485 - val_auc: 0.9619\n",
            "Epoch 10/25\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.2407 - auc: 0.9646 - val_loss: 0.2484 - val_auc: 0.9620\n",
            "Epoch 11/25\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.2394 - auc: 0.9650 - val_loss: 0.2482 - val_auc: 0.9623\n",
            "Epoch 12/25\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.2385 - auc: 0.9653 - val_loss: 0.2463 - val_auc: 0.9625\n",
            "Epoch 13/25\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.2367 - auc: 0.9658 - val_loss: 0.2490 - val_auc: 0.9627\n",
            "Epoch 14/25\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.2357 - auc: 0.9660 - val_loss: 0.2494 - val_auc: 0.9628\n",
            "Epoch 15/25\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.2347 - auc: 0.9664 - val_loss: 0.2445 - val_auc: 0.9629\n",
            "Epoch 16/25\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.2343 - auc: 0.9664 - val_loss: 0.2444 - val_auc: 0.9629\n",
            "Epoch 17/25\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.2335 - auc: 0.9666 - val_loss: 0.2478 - val_auc: 0.9633\n",
            "Epoch 18/25\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.2321 - auc: 0.9670 - val_loss: 0.2425 - val_auc: 0.9635\n",
            "Epoch 19/25\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.2321 - auc: 0.9669 - val_loss: 0.2417 - val_auc: 0.9639\n",
            "Epoch 20/25\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.2297 - auc: 0.9677 - val_loss: 0.2410 - val_auc: 0.9638\n",
            "Epoch 21/25\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.2296 - auc: 0.9677 - val_loss: 0.2409 - val_auc: 0.9639\n",
            "Epoch 22/25\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.2270 - auc: 0.9685 - val_loss: 0.2409 - val_auc: 0.9639\n",
            "Epoch 23/25\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.2263 - auc: 0.9686 - val_loss: 0.2410 - val_auc: 0.9645\n",
            "Epoch 24/25\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.2256 - auc: 0.9688 - val_loss: 0.2430 - val_auc: 0.9647\n",
            "Epoch 25/25\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.2240 - auc: 0.9693 - val_loss: 0.2382 - val_auc: 0.9647\n",
            "DOWNSAMPLED:(24634:32024)\n",
            "Epoch 1/25\n",
            "42/57 [=====================>........] - ETA: 0s - loss: 0.2249 - auc: 0.9689"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-01-08 00:23:40.359707: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 48499248 exceeds 10% of free system memory.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "57/57 [==============================] - 0s 4ms/step - loss: 0.2257 - auc: 0.9687 - val_loss: 0.2206 - val_auc: 0.9700\n",
            "Epoch 2/25\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.2242 - auc: 0.9691 - val_loss: 0.2205 - val_auc: 0.9698\n",
            "Epoch 3/25\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.2235 - auc: 0.9693 - val_loss: 0.2209 - val_auc: 0.9697\n",
            "Epoch 4/25\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.2226 - auc: 0.9696 - val_loss: 0.2203 - val_auc: 0.9698\n",
            "Epoch 5/25\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.2209 - auc: 0.9700 - val_loss: 0.2203 - val_auc: 0.9699\n",
            "Epoch 6/25\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.2204 - auc: 0.9702 - val_loss: 0.2202 - val_auc: 0.9697\n",
            "Epoch 7/25\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.2195 - auc: 0.9704 - val_loss: 0.2207 - val_auc: 0.9695\n",
            "Epoch 8/25\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.2182 - auc: 0.9707 - val_loss: 0.2193 - val_auc: 0.9701\n",
            "Epoch 9/25\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.2190 - auc: 0.9705 - val_loss: 0.2200 - val_auc: 0.9698\n",
            "Epoch 10/25\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.2165 - auc: 0.9712 - val_loss: 0.2209 - val_auc: 0.9697\n",
            "Epoch 11/25\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.2163 - auc: 0.9712 - val_loss: 0.2188 - val_auc: 0.9702\n",
            "Epoch 12/25\n",
            "57/57 [==============================] - 0s 4ms/step - loss: 0.2148 - auc: 0.9717 - val_loss: 0.2184 - val_auc: 0.9702\n",
            "Epoch 13/25\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.2140 - auc: 0.9718 - val_loss: 0.2182 - val_auc: 0.9703\n",
            "Epoch 14/25\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.2129 - auc: 0.9721 - val_loss: 0.2193 - val_auc: 0.9704\n",
            "Epoch 15/25\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.2125 - auc: 0.9722 - val_loss: 0.2187 - val_auc: 0.9704\n",
            "Epoch 16/25\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.2111 - auc: 0.9726 - val_loss: 0.2177 - val_auc: 0.9703\n",
            "Epoch 17/25\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.2099 - auc: 0.9730 - val_loss: 0.2191 - val_auc: 0.9702\n",
            "Epoch 18/25\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.2101 - auc: 0.9728 - val_loss: 0.2217 - val_auc: 0.9706\n",
            "Epoch 19/25\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.2092 - auc: 0.9731 - val_loss: 0.2169 - val_auc: 0.9706\n",
            "Epoch 20/25\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.2087 - auc: 0.9732 - val_loss: 0.2219 - val_auc: 0.9705\n",
            "Epoch 21/25\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.2083 - auc: 0.9733 - val_loss: 0.2173 - val_auc: 0.9705\n",
            "Epoch 22/25\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.2061 - auc: 0.9739 - val_loss: 0.2160 - val_auc: 0.9707\n",
            "Epoch 23/25\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.2058 - auc: 0.9739 - val_loss: 0.2166 - val_auc: 0.9706\n",
            "Epoch 24/25\n",
            "57/57 [==============================] - 0s 4ms/step - loss: 0.2045 - auc: 0.9743 - val_loss: 0.2154 - val_auc: 0.9710\n",
            "Epoch 25/25\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.2038 - auc: 0.9745 - val_loss: 0.2163 - val_auc: 0.9709\n",
            "DOWNSAMPLED:(24634:32024)\n",
            "Epoch 1/25\n",
            "37/57 [==================>...........] - ETA: 0s - loss: 0.2067 - auc: 0.9736"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-01-08 00:23:45.316950: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 48499248 exceeds 10% of free system memory.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "57/57 [==============================] - 0s 4ms/step - loss: 0.2043 - auc: 0.9743 - val_loss: 0.2106 - val_auc: 0.9730\n",
            "Epoch 2/25\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.2036 - auc: 0.9745 - val_loss: 0.2105 - val_auc: 0.9727\n",
            "Epoch 3/25\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.2019 - auc: 0.9750 - val_loss: 0.2168 - val_auc: 0.9724\n",
            "Epoch 4/25\n",
            "57/57 [==============================] - 0s 4ms/step - loss: 0.2017 - auc: 0.9750 - val_loss: 0.2102 - val_auc: 0.9725\n",
            "Epoch 5/25\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.2008 - auc: 0.9752 - val_loss: 0.2105 - val_auc: 0.9726\n",
            "Epoch 6/25\n",
            "57/57 [==============================] - 0s 4ms/step - loss: 0.2005 - auc: 0.9753 - val_loss: 0.2104 - val_auc: 0.9723\n",
            "Epoch 7/25\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.1989 - auc: 0.9757 - val_loss: 0.2102 - val_auc: 0.9726\n",
            "Epoch 8/25\n",
            "57/57 [==============================] - 0s 4ms/step - loss: 0.1995 - auc: 0.9756 - val_loss: 0.2102 - val_auc: 0.9724\n",
            "Epoch 9/25\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.1972 - auc: 0.9761 - val_loss: 0.2105 - val_auc: 0.9725\n",
            "Epoch 10/25\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.1973 - auc: 0.9761 - val_loss: 0.2102 - val_auc: 0.9724\n",
            "Epoch 11/25\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.1968 - auc: 0.9762 - val_loss: 0.2100 - val_auc: 0.9725\n",
            "Epoch 12/25\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.1957 - auc: 0.9765 - val_loss: 0.2115 - val_auc: 0.9724\n",
            "Epoch 13/25\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.1949 - auc: 0.9766 - val_loss: 0.2104 - val_auc: 0.9723\n",
            "Epoch 14/25\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.1955 - auc: 0.9765 - val_loss: 0.2099 - val_auc: 0.9725\n",
            "Epoch 15/25\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.1939 - auc: 0.9769 - val_loss: 0.2099 - val_auc: 0.9724\n",
            "Epoch 16/25\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.1930 - auc: 0.9771 - val_loss: 0.2129 - val_auc: 0.9725\n",
            "Epoch 17/25\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.1940 - auc: 0.9768 - val_loss: 0.2095 - val_auc: 0.9727\n",
            "Epoch 18/25\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.1917 - auc: 0.9774 - val_loss: 0.2099 - val_auc: 0.9725\n",
            "Epoch 19/25\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.1909 - auc: 0.9776 - val_loss: 0.2096 - val_auc: 0.9725\n",
            "Epoch 20/25\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.1910 - auc: 0.9775 - val_loss: 0.2113 - val_auc: 0.9723\n",
            "Epoch 21/25\n",
            "57/57 [==============================] - 0s 4ms/step - loss: 0.1901 - auc: 0.9778 - val_loss: 0.2095 - val_auc: 0.9725\n",
            "Epoch 22/25\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.1893 - auc: 0.9780 - val_loss: 0.2113 - val_auc: 0.9726\n",
            "Epoch 23/25\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.1891 - auc: 0.9780 - val_loss: 0.2108 - val_auc: 0.9726\n",
            "Epoch 24/25\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.1880 - auc: 0.9783 - val_loss: 0.2096 - val_auc: 0.9727\n",
            "Epoch 25/25\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.1877 - auc: 0.9783 - val_loss: 0.2217 - val_auc: 0.9722\n",
            "DOWNSAMPLED:(24634:32024)\n",
            "Epoch 1/25\n",
            "57/57 [==============================] - 0s 4ms/step - loss: 0.1900 - auc: 0.9777 - val_loss: 0.1939 - val_auc: 0.9768\n",
            "Epoch 2/25\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.1885 - auc: 0.9781 - val_loss: 0.1917 - val_auc: 0.9769\n",
            "Epoch 3/25\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.1888 - auc: 0.9780 - val_loss: 0.1927 - val_auc: 0.9767\n",
            "Epoch 4/25\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.1869 - auc: 0.9785 - val_loss: 0.1933 - val_auc: 0.9765\n",
            "Epoch 5/25\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.1863 - auc: 0.9786 - val_loss: 0.1953 - val_auc: 0.9765\n",
            "Epoch 6/25\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.1864 - auc: 0.9786 - val_loss: 0.1939 - val_auc: 0.9762\n",
            "Epoch 7/25\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.1863 - auc: 0.9786 - val_loss: 0.1936 - val_auc: 0.9763\n",
            "Epoch 8/25\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.1851 - auc: 0.9789 - val_loss: 0.1943 - val_auc: 0.9763\n",
            "Epoch 9/25\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.1841 - auc: 0.9791 - val_loss: 0.1935 - val_auc: 0.9764\n",
            "Epoch 10/25\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.1836 - auc: 0.9793 - val_loss: 0.1970 - val_auc: 0.9760\n",
            "Epoch 11/25\n",
            "57/57 [==============================] - 0s 4ms/step - loss: 0.1848 - auc: 0.9789 - val_loss: 0.1951 - val_auc: 0.9762\n",
            "Epoch 12/25\n",
            "57/57 [==============================] - 0s 4ms/step - loss: 0.1827 - auc: 0.9794 - val_loss: 0.1956 - val_auc: 0.9758\n",
            "Epoch 13/25\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.1821 - auc: 0.9796 - val_loss: 0.1948 - val_auc: 0.9761\n",
            "Epoch 14/25\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.1815 - auc: 0.9797 - val_loss: 0.2001 - val_auc: 0.9756\n",
            "Epoch 15/25\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.1808 - auc: 0.9799 - val_loss: 0.1948 - val_auc: 0.9761\n",
            "Epoch 16/25\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.1808 - auc: 0.9798 - val_loss: 0.1957 - val_auc: 0.9757\n",
            "Epoch 17/25\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.1798 - auc: 0.9801 - val_loss: 0.1954 - val_auc: 0.9759\n",
            "Epoch 18/25\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.1796 - auc: 0.9801 - val_loss: 0.2033 - val_auc: 0.9755\n",
            "Epoch 19/25\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.1794 - auc: 0.9802 - val_loss: 0.1980 - val_auc: 0.9757\n",
            "Epoch 20/25\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.1785 - auc: 0.9804 - val_loss: 0.2006 - val_auc: 0.9756\n",
            "Epoch 21/25\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.1776 - auc: 0.9806 - val_loss: 0.1951 - val_auc: 0.9759\n",
            "Epoch 22/25\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.1780 - auc: 0.9805 - val_loss: 0.1981 - val_auc: 0.9756\n",
            "Epoch 23/25\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.1780 - auc: 0.9805 - val_loss: 0.1984 - val_auc: 0.9756\n",
            "Epoch 24/25\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.1763 - auc: 0.9809 - val_loss: 0.1956 - val_auc: 0.9757\n",
            "Epoch 25/25\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.1755 - auc: 0.9811 - val_loss: 0.1952 - val_auc: 0.9758\n",
            "DOWNSAMPLED:(24634:32024)\n",
            "Epoch 1/25\n",
            "57/57 [==============================] - 0s 4ms/step - loss: 0.1786 - auc: 0.9802 - val_loss: 0.1776 - val_auc: 0.9806\n",
            "Epoch 2/25\n",
            "57/57 [==============================] - 0s 4ms/step - loss: 0.1776 - auc: 0.9804 - val_loss: 0.1778 - val_auc: 0.9804\n",
            "Epoch 3/25\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.1770 - auc: 0.9806 - val_loss: 0.1786 - val_auc: 0.9806\n",
            "Epoch 4/25\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.1764 - auc: 0.9808 - val_loss: 0.1791 - val_auc: 0.9799\n",
            "Epoch 5/25\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.1768 - auc: 0.9806 - val_loss: 0.1789 - val_auc: 0.9803\n",
            "Epoch 6/25\n",
            "57/57 [==============================] - 0s 4ms/step - loss: 0.1755 - auc: 0.9810 - val_loss: 0.1798 - val_auc: 0.9800\n",
            "Epoch 7/25\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.1764 - auc: 0.9807 - val_loss: 0.1821 - val_auc: 0.9797\n",
            "Epoch 8/25\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.1745 - auc: 0.9812 - val_loss: 0.1802 - val_auc: 0.9799\n",
            "Epoch 9/25\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.1736 - auc: 0.9814 - val_loss: 0.1831 - val_auc: 0.9791\n",
            "Epoch 10/25\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.1736 - auc: 0.9814 - val_loss: 0.1870 - val_auc: 0.9794\n",
            "Epoch 11/25\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.1739 - auc: 0.9813 - val_loss: 0.1839 - val_auc: 0.9791\n",
            "Epoch 12/25\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.1728 - auc: 0.9816 - val_loss: 0.1814 - val_auc: 0.9793\n",
            "Epoch 13/25\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.1733 - auc: 0.9814 - val_loss: 0.1812 - val_auc: 0.9794\n",
            "Epoch 14/25\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.1724 - auc: 0.9816 - val_loss: 0.1824 - val_auc: 0.9791\n",
            "Epoch 15/25\n",
            "57/57 [==============================] - 0s 4ms/step - loss: 0.1713 - auc: 0.9819 - val_loss: 0.1826 - val_auc: 0.9791\n",
            "Epoch 16/25\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.1710 - auc: 0.9820 - val_loss: 0.1825 - val_auc: 0.9791\n",
            "Epoch 17/25\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.1704 - auc: 0.9821 - val_loss: 0.1823 - val_auc: 0.9791\n",
            "Epoch 18/25\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.1698 - auc: 0.9822 - val_loss: 0.1839 - val_auc: 0.9789\n",
            "Epoch 19/25\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.1691 - auc: 0.9824 - val_loss: 0.1821 - val_auc: 0.9792\n",
            "Epoch 20/25\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.1692 - auc: 0.9823 - val_loss: 0.1829 - val_auc: 0.9790\n",
            "Epoch 21/25\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.1688 - auc: 0.9825 - val_loss: 0.1838 - val_auc: 0.9787\n",
            "Epoch 22/25\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.1688 - auc: 0.9824 - val_loss: 0.1835 - val_auc: 0.9788\n",
            "Epoch 23/25\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.1678 - auc: 0.9826 - val_loss: 0.1833 - val_auc: 0.9788\n",
            "Epoch 24/25\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.1671 - auc: 0.9828 - val_loss: 0.1834 - val_auc: 0.9789\n",
            "Epoch 25/25\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.1671 - auc: 0.9828 - val_loss: 0.1860 - val_auc: 0.9784\n",
            "DOWNSAMPLED:(24634:32024)\n",
            "Epoch 1/25\n",
            "57/57 [==============================] - 0s 4ms/step - loss: 0.1698 - auc: 0.9821 - val_loss: 0.1664 - val_auc: 0.9836\n",
            "Epoch 2/25\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.1679 - auc: 0.9825 - val_loss: 0.1706 - val_auc: 0.9833\n",
            "Epoch 3/25\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.1680 - auc: 0.9825 - val_loss: 0.1669 - val_auc: 0.9831\n",
            "Epoch 4/25\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.1683 - auc: 0.9825 - val_loss: 0.1786 - val_auc: 0.9826\n",
            "Epoch 5/25\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.1673 - auc: 0.9827 - val_loss: 0.1682 - val_auc: 0.9827\n",
            "Epoch 6/25\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.1666 - auc: 0.9829 - val_loss: 0.1690 - val_auc: 0.9825\n",
            "Epoch 7/25\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.1663 - auc: 0.9829 - val_loss: 0.1702 - val_auc: 0.9825\n",
            "Epoch 8/25\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.1658 - auc: 0.9830 - val_loss: 0.1699 - val_auc: 0.9823\n",
            "Epoch 9/25\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.1657 - auc: 0.9830 - val_loss: 0.1714 - val_auc: 0.9822\n",
            "Epoch 10/25\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.1662 - auc: 0.9829 - val_loss: 0.1711 - val_auc: 0.9822\n",
            "Epoch 11/25\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.1647 - auc: 0.9832 - val_loss: 0.1707 - val_auc: 0.9821\n",
            "Epoch 12/25\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.1651 - auc: 0.9832 - val_loss: 0.1719 - val_auc: 0.9820\n",
            "Epoch 13/25\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.1635 - auc: 0.9835 - val_loss: 0.1801 - val_auc: 0.9814\n",
            "Epoch 14/25\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.1639 - auc: 0.9834 - val_loss: 0.1724 - val_auc: 0.9816\n",
            "Epoch 15/25\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.1625 - auc: 0.9837 - val_loss: 0.1726 - val_auc: 0.9817\n",
            "Epoch 16/25\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.1629 - auc: 0.9836 - val_loss: 0.1735 - val_auc: 0.9814\n",
            "Epoch 17/25\n",
            "57/57 [==============================] - 0s 4ms/step - loss: 0.1621 - auc: 0.9838 - val_loss: 0.1732 - val_auc: 0.9815\n",
            "Epoch 18/25\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.1640 - auc: 0.9833 - val_loss: 0.1746 - val_auc: 0.9814\n",
            "Epoch 19/25\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.1629 - auc: 0.9836 - val_loss: 0.1753 - val_auc: 0.9815\n",
            "Epoch 20/25\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.1609 - auc: 0.9840 - val_loss: 0.1741 - val_auc: 0.9811\n",
            "Epoch 21/25\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.1606 - auc: 0.9841 - val_loss: 0.1739 - val_auc: 0.9812\n",
            "Epoch 22/25\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.1596 - auc: 0.9843 - val_loss: 0.1755 - val_auc: 0.9811\n",
            "Epoch 23/25\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.1596 - auc: 0.9843 - val_loss: 0.1746 - val_auc: 0.9810\n",
            "Epoch 24/25\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.1602 - auc: 0.9842 - val_loss: 0.1793 - val_auc: 0.9808\n",
            "Epoch 25/25\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.1596 - auc: 0.9843 - val_loss: 0.1791 - val_auc: 0.9809\n",
            "DOWNSAMPLED:(24634:32024)\n",
            "Epoch 1/25\n",
            "57/57 [==============================] - 0s 4ms/step - loss: 0.1629 - auc: 0.9836 - val_loss: 0.1657 - val_auc: 0.9832\n",
            "Epoch 2/25\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.1606 - auc: 0.9841 - val_loss: 0.1656 - val_auc: 0.9830\n",
            "Epoch 3/25\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.1598 - auc: 0.9842 - val_loss: 0.1684 - val_auc: 0.9826\n",
            "Epoch 4/25\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.1606 - auc: 0.9841 - val_loss: 0.1668 - val_auc: 0.9827\n",
            "Epoch 5/25\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.1589 - auc: 0.9844 - val_loss: 0.1668 - val_auc: 0.9825\n",
            "Epoch 6/25\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.1583 - auc: 0.9846 - val_loss: 0.1688 - val_auc: 0.9821\n",
            "Epoch 7/25\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.1591 - auc: 0.9844 - val_loss: 0.1699 - val_auc: 0.9823\n",
            "Epoch 8/25\n",
            "57/57 [==============================] - 0s 4ms/step - loss: 0.1579 - auc: 0.9847 - val_loss: 0.1687 - val_auc: 0.9820\n",
            "Epoch 9/25\n",
            "57/57 [==============================] - 0s 4ms/step - loss: 0.1572 - auc: 0.9848 - val_loss: 0.1689 - val_auc: 0.9821\n",
            "Epoch 10/25\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.1568 - auc: 0.9849 - val_loss: 0.1699 - val_auc: 0.9817\n",
            "Epoch 11/25\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.1564 - auc: 0.9850 - val_loss: 0.1724 - val_auc: 0.9816\n",
            "Epoch 12/25\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.1567 - auc: 0.9849 - val_loss: 0.1726 - val_auc: 0.9817\n",
            "Epoch 13/25\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.1564 - auc: 0.9850 - val_loss: 0.1710 - val_auc: 0.9813\n",
            "Epoch 14/25\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.1555 - auc: 0.9851 - val_loss: 0.1738 - val_auc: 0.9816\n",
            "Epoch 15/25\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.1552 - auc: 0.9852 - val_loss: 0.1762 - val_auc: 0.9811\n",
            "Epoch 16/25\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.1554 - auc: 0.9851 - val_loss: 0.1754 - val_auc: 0.9812\n",
            "Epoch 17/25\n",
            "57/57 [==============================] - 0s 4ms/step - loss: 0.1543 - auc: 0.9854 - val_loss: 0.1721 - val_auc: 0.9812\n",
            "Epoch 18/25\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.1548 - auc: 0.9853 - val_loss: 0.1734 - val_auc: 0.9810\n",
            "Epoch 19/25\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.1536 - auc: 0.9855 - val_loss: 0.1750 - val_auc: 0.9808\n",
            "Epoch 20/25\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.1537 - auc: 0.9854 - val_loss: 0.1766 - val_auc: 0.9807\n",
            "Epoch 21/25\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.1549 - auc: 0.9852 - val_loss: 0.1771 - val_auc: 0.9806\n",
            "Epoch 22/25\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.1534 - auc: 0.9855 - val_loss: 0.1739 - val_auc: 0.9808\n",
            "Epoch 23/25\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.1517 - auc: 0.9859 - val_loss: 0.1746 - val_auc: 0.9805\n",
            "Epoch 24/25\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.1516 - auc: 0.9858 - val_loss: 0.1752 - val_auc: 0.9806\n",
            "Epoch 25/25\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.1517 - auc: 0.9859 - val_loss: 0.1750 - val_auc: 0.9804\n",
            "DOWNSAMPLED:(24634:32024)\n",
            "Epoch 1/25\n",
            "57/57 [==============================] - 0s 4ms/step - loss: 0.1556 - auc: 0.9850 - val_loss: 0.1518 - val_auc: 0.9865\n",
            "Epoch 2/25\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.1542 - auc: 0.9853 - val_loss: 0.1487 - val_auc: 0.9863\n",
            "Epoch 3/25\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.1540 - auc: 0.9853 - val_loss: 0.1500 - val_auc: 0.9860\n",
            "Epoch 4/25\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.1536 - auc: 0.9854 - val_loss: 0.1502 - val_auc: 0.9860\n",
            "Epoch 5/25\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.1562 - auc: 0.9849 - val_loss: 0.1505 - val_auc: 0.9858\n",
            "Epoch 6/25\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.1541 - auc: 0.9853 - val_loss: 0.1517 - val_auc: 0.9857\n",
            "Epoch 7/25\n",
            "57/57 [==============================] - 0s 4ms/step - loss: 0.1521 - auc: 0.9857 - val_loss: 0.1532 - val_auc: 0.9853\n",
            "Epoch 8/25\n",
            "57/57 [==============================] - 0s 4ms/step - loss: 0.1528 - auc: 0.9856 - val_loss: 0.1603 - val_auc: 0.9853\n",
            "Epoch 9/25\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.1517 - auc: 0.9859 - val_loss: 0.1540 - val_auc: 0.9853\n",
            "Epoch 10/25\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.1511 - auc: 0.9859 - val_loss: 0.1551 - val_auc: 0.9852\n",
            "Epoch 11/25\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.1511 - auc: 0.9860 - val_loss: 0.1535 - val_auc: 0.9852\n",
            "Epoch 12/25\n",
            "57/57 [==============================] - 0s 4ms/step - loss: 0.1503 - auc: 0.9861 - val_loss: 0.1550 - val_auc: 0.9849\n",
            "Epoch 13/25\n",
            "57/57 [==============================] - 0s 4ms/step - loss: 0.1500 - auc: 0.9862 - val_loss: 0.1568 - val_auc: 0.9847\n",
            "Epoch 14/25\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.1503 - auc: 0.9861 - val_loss: 0.1584 - val_auc: 0.9846\n",
            "Epoch 15/25\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.1499 - auc: 0.9861 - val_loss: 0.1558 - val_auc: 0.9847\n",
            "Epoch 16/25\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.1492 - auc: 0.9863 - val_loss: 0.1604 - val_auc: 0.9843\n",
            "Epoch 17/25\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.1514 - auc: 0.9858 - val_loss: 0.1569 - val_auc: 0.9846\n",
            "Epoch 18/25\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.1489 - auc: 0.9863 - val_loss: 0.1575 - val_auc: 0.9844\n",
            "Epoch 19/25\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.1486 - auc: 0.9864 - val_loss: 0.1631 - val_auc: 0.9842\n",
            "Epoch 20/25\n",
            "57/57 [==============================] - 0s 4ms/step - loss: 0.1481 - auc: 0.9865 - val_loss: 0.1608 - val_auc: 0.9841\n",
            "Epoch 21/25\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.1478 - auc: 0.9865 - val_loss: 0.1636 - val_auc: 0.9839\n",
            "Epoch 22/25\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.1481 - auc: 0.9865 - val_loss: 0.1631 - val_auc: 0.9840\n",
            "Epoch 23/25\n",
            "57/57 [==============================] - 0s 4ms/step - loss: 0.1471 - auc: 0.9867 - val_loss: 0.1603 - val_auc: 0.9838\n",
            "Epoch 24/25\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.1471 - auc: 0.9867 - val_loss: 0.1595 - val_auc: 0.9840\n",
            "Epoch 25/25\n",
            "57/57 [==============================] - 0s 3ms/step - loss: 0.1465 - auc: 0.9868 - val_loss: 0.1604 - val_auc: 0.9840\n"
          ]
        }
      ],
      "source": [
        "rand_state= random.randint(0,1000)\n",
        "#NN training with 10-fold cross-validation stratified with random under-sampling technic for imbalanced data\n",
        "X = pre_processed_data.drop([\"id\",\"generated\"],axis=1)\n",
        "Y = pre_processed_data[\"generated\"]\n",
        "folds = StratifiedKFold(10,shuffle=True,random_state=rand_state)\n",
        "for fold, (train_index, val_index) in enumerate(folds.split(X,Y)):\n",
        "    X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n",
        "    y_train, y_val = Y.iloc[train_index], Y.iloc[val_index]\n",
        "\n",
        "    X_train_dwns, y_train_dwns = downsampling(X_train,y_train,rand_state,ratio=1.3)\n",
        "\n",
        "    #print(f\"Fold {fold + 1}: Train {len(train_index)} samples, Validation {len(val_index)} samples\")\n",
        "    #print(f\"TRAIN:({y_train[y_train==0].size}:{y_train[y_train==1].size})\")\n",
        "    print(f\"DOWNSAMPLED:({y_train_dwns[y_train_dwns==0].size}:{y_train_dwns[y_train_dwns==1].size})\")\n",
        "    #print(f\"VAL:({y_val[y_val==0].size}:{y_val[y_val==1].size})\")\n",
        "\n",
        "    NNmodel.fit(X_train_dwns,\n",
        "                y_train_dwns,\n",
        "                validation_data=(X_val,y_val),\n",
        "                batch_size=1000,\n",
        "                epochs=25)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'ROCcurves' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mROCcurves\u001b[49m(NNmodel,X,Y)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'ROCcurves' is not defined"
          ]
        }
      ],
      "source": [
        "ROCcurves(NNmodel,X,Y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ub2SWyaC9uL2",
        "outputId": "d63e26ff-e443-4621-9757-f9444e0fdfa6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2043/2043 [==============================] - 2s 1ms/step\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x7f2861d1ee90>"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "pred = list(map(lambda pred : 1 if pred > 0.5 else 0,NNmodel.predict(x=X)))\n",
        "display = ConfusionMatrixDisplay(confusion_matrix(pred,Y),display_labels=[\"True\",\"False\"])\n",
        "display.plot()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o5dVLLxVyjOg",
        "outputId": "4d54fa2d-90ac-45bf-fddb-16e7cebe5806"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 14ms/step\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "array([[1.]], dtype=float32)"
            ]
          },
          "execution_count": 54,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "NNmodel.predict(x=pd.DataFrame(X.iloc[1385]).transpose())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/mrtc101/Desktop/ciencias de la computacion/Cursado/4.2Inteligencia Artificial 2/Final/venv/lib/python3.11/site-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ],
      "source": [
        "#Save model weights\n",
        "NNmodel.save(\"../data/weight/DenseNetwork.h5\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Embeddings ploting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "KeyError",
          "evalue": "20",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "File \u001b[0;32m~/Desktop/ciencias de la computacion/Cursado/4.2Inteligencia Artificial 2/Final/venv/lib/python3.11/site-packages/pandas/core/indexes/base.py:3791\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3790\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3791\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3792\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
            "File \u001b[0;32mindex.pyx:152\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "File \u001b[0;32mindex.pyx:181\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7080\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 20",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[56], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m plt\u001b[38;5;241m.\u001b[39mscatter(x\u001b[38;5;241m=\u001b[39m\u001b[43mpre_processed_data\u001b[49m\u001b[43m[\u001b[49m\u001b[43mpre_processed_data\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgenerated\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m]\u001b[49m,\n\u001b[1;32m      2\u001b[0m             y\u001b[38;5;241m=\u001b[39mpre_processed_data[pre_processed_data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenerated\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m40\u001b[39m])\n\u001b[1;32m      3\u001b[0m plt\u001b[38;5;241m.\u001b[39mscatter(x\u001b[38;5;241m=\u001b[39mpre_processed_data[pre_processed_data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenerated\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m20\u001b[39m],\n\u001b[1;32m      4\u001b[0m             y\u001b[38;5;241m=\u001b[39mpre_processed_data[pre_processed_data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenerated\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m40\u001b[39m])\n",
            "File \u001b[0;32m~/Desktop/ciencias de la computacion/Cursado/4.2Inteligencia Artificial 2/Final/venv/lib/python3.11/site-packages/pandas/core/frame.py:3893\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3891\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   3892\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 3893\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3894\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   3895\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
            "File \u001b[0;32m~/Desktop/ciencias de la computacion/Cursado/4.2Inteligencia Artificial 2/Final/venv/lib/python3.11/site-packages/pandas/core/indexes/base.py:3798\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3793\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   3794\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m   3795\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[1;32m   3796\u001b[0m     ):\n\u001b[1;32m   3797\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3798\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3799\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3800\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3801\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3802\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3803\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
            "\u001b[0;31mKeyError\u001b[0m: 20"
          ]
        }
      ],
      "source": [
        "plt.scatter(x=pre_processed_data[pre_processed_data[\"generated\"]==0][20],\n",
        "            y=pre_processed_data[pre_processed_data[\"generated\"]==0][40])\n",
        "plt.scatter(x=pre_processed_data[pre_processed_data[\"generated\"]==1][20],\n",
        "            y=pre_processed_data[pre_processed_data[\"generated\"]==1][40])\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0rc1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
