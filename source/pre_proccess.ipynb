{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM - Detect AI Generated Text \n",
    "# DATA PRE-PROCESS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/mrtc101/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/mrtc101/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "/home/mrtc101/Desktop/ciencias de la computacion/Cursado/4.2Inteligencia Artificial 2/Final/venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-01-22 15:52:46.783119: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-01-22 15:52:46.783154: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-01-22 15:52:46.815060: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-01-22 15:52:46.884467: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-01-22 15:52:48.016652: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import random\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "from nltk.tokenize import word_tokenize,sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from gensim.models import fasttext\n",
    "from gensim.models import doc2vec\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.utils import resample\n",
    "from sklearn.metrics import roc_curve ,precision_recall_curve,auc,confusion_matrix,ConfusionMatrixDisplay\n",
    "\n",
    "from tokenizers import (\n",
    "    decoders,\n",
    "    models,\n",
    "    normalizers,\n",
    "    pre_tokenizers,\n",
    "    processors,\n",
    "    trainers,\n",
    "    Tokenizer,\n",
    ")\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "import keras\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_dataset = pd.read_csv(\"../data/train_essays.csv\")\n",
    "prompts_dataset = pd.read_csv(\"../data/train_prompts.csv\")\n",
    "custom_data = pd.read_csv(\"../data/custom_essays.csv\")\n",
    "downloaded_data_1 = pd.read_csv(\"../data/train_v4_drcat_01.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding new Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Describing the imbalance of this dataset in terms of ration is 1:500. The dataset presents sever imbalance. Previous aproches using only 20 new LLM generated examples manually and random Downsampling technic, didn't reach a higher score than 0.56. \n",
    "\n",
    "Concluding that more new data is needed, i downloaded data shared by competitors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "downloaded_data_1[\"prompt_id\"] = downloaded_data_1[\"prompt_name\"].apply(lambda name : 0 if name == \"Car-free cities\" else 1 if name == \"Does the electoral college work?\" else 21 )\n",
    "downloaded_data_1 = downloaded_data_1[[\"prompt_id\",\"text\",\"label\"]].rename(columns={\"label\":\"generated\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = pd.concat([custom_data,downloaded_data_1],axis=0,ignore_index=True)\n",
    "new_data[\"id\"] = range(0,new_data.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>prompt_id</th>\n",
       "      <th>text</th>\n",
       "      <th>generated</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>The Advantages of Limiting Car Usage in Suburb...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Paris' Driving Ban: A Temporary Solution to En...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>Bogota's Car-Free Day: A Model for Sustainable...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>Shifting Trends: The Decline of Car Culture in...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>The End of Car Culture and the Rise of Sustain...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73590</th>\n",
       "      <td>73590</td>\n",
       "      <td>21</td>\n",
       "      <td>I am writing you today to disagree with your t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73591</th>\n",
       "      <td>73591</td>\n",
       "      <td>21</td>\n",
       "      <td>Dear Principal,\\n\\nIn conclusion, I would obse...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73592</th>\n",
       "      <td>73592</td>\n",
       "      <td>21</td>\n",
       "      <td>Dear Mrs. Principal,\\n\\nin these kinds of cons...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73593</th>\n",
       "      <td>73593</td>\n",
       "      <td>21</td>\n",
       "      <td>I enjoyed Form five and excitedly ex claims ed...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73594</th>\n",
       "      <td>73594</td>\n",
       "      <td>21</td>\n",
       "      <td>Dear TEACHER_NAME,\\n\\nWell Ms/Mr TEACHER_NAME ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>73595 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  prompt_id                                               text  \\\n",
       "0          0          0  The Advantages of Limiting Car Usage in Suburb...   \n",
       "1          1          0  Paris' Driving Ban: A Temporary Solution to En...   \n",
       "2          2          0  Bogota's Car-Free Day: A Model for Sustainable...   \n",
       "3          3          0  Shifting Trends: The Decline of Car Culture in...   \n",
       "4          4          0  The End of Car Culture and the Rise of Sustain...   \n",
       "...      ...        ...                                                ...   \n",
       "73590  73590         21  I am writing you today to disagree with your t...   \n",
       "73591  73591         21  Dear Principal,\\n\\nIn conclusion, I would obse...   \n",
       "73592  73592         21  Dear Mrs. Principal,\\n\\nin these kinds of cons...   \n",
       "73593  73593         21  I enjoyed Form five and excitedly ex claims ed...   \n",
       "73594  73594         21  Dear TEACHER_NAME,\\n\\nWell Ms/Mr TEACHER_NAME ...   \n",
       "\n",
       "       generated  \n",
       "0              1  \n",
       "1              1  \n",
       "2              1  \n",
       "3              1  \n",
       "4              1  \n",
       "...          ...  \n",
       "73590          1  \n",
       "73591          1  \n",
       "73592          1  \n",
       "73593          1  \n",
       "73594          1  \n",
       "\n",
       "[73595 rows x 4 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>prompt_id</th>\n",
       "      <th>text</th>\n",
       "      <th>generated</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0059830c</td>\n",
       "      <td>0</td>\n",
       "      <td>Cars. Cars have been around since they became ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>005db917</td>\n",
       "      <td>0</td>\n",
       "      <td>Transportation is a large necessity in most co...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>008f63e3</td>\n",
       "      <td>0</td>\n",
       "      <td>\"America's love affair with it's vehicles seem...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00940276</td>\n",
       "      <td>0</td>\n",
       "      <td>How often do you ride in a car? Do you drive a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00c39458</td>\n",
       "      <td>0</td>\n",
       "      <td>Cars are a wonderful thing. They are perhaps o...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74968</th>\n",
       "      <td>73590</td>\n",
       "      <td>21</td>\n",
       "      <td>I am writing you today to disagree with your t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74969</th>\n",
       "      <td>73591</td>\n",
       "      <td>21</td>\n",
       "      <td>Dear Principal,\\n\\nIn conclusion, I would obse...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74970</th>\n",
       "      <td>73592</td>\n",
       "      <td>21</td>\n",
       "      <td>Dear Mrs. Principal,\\n\\nin these kinds of cons...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74971</th>\n",
       "      <td>73593</td>\n",
       "      <td>21</td>\n",
       "      <td>I enjoyed Form five and excitedly ex claims ed...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74972</th>\n",
       "      <td>73594</td>\n",
       "      <td>21</td>\n",
       "      <td>Dear TEACHER_NAME,\\n\\nWell Ms/Mr TEACHER_NAME ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>74973 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             id  prompt_id                                               text  \\\n",
       "0      0059830c          0  Cars. Cars have been around since they became ...   \n",
       "1      005db917          0  Transportation is a large necessity in most co...   \n",
       "2      008f63e3          0  \"America's love affair with it's vehicles seem...   \n",
       "3      00940276          0  How often do you ride in a car? Do you drive a...   \n",
       "4      00c39458          0  Cars are a wonderful thing. They are perhaps o...   \n",
       "...         ...        ...                                                ...   \n",
       "74968     73590         21  I am writing you today to disagree with your t...   \n",
       "74969     73591         21  Dear Principal,\\n\\nIn conclusion, I would obse...   \n",
       "74970     73592         21  Dear Mrs. Principal,\\n\\nin these kinds of cons...   \n",
       "74971     73593         21  I enjoyed Form five and excitedly ex claims ed...   \n",
       "74972     73594         21  Dear TEACHER_NAME,\\n\\nWell Ms/Mr TEACHER_NAME ...   \n",
       "\n",
       "       generated  \n",
       "0              0  \n",
       "1              0  \n",
       "2              0  \n",
       "3              0  \n",
       "4              0  \n",
       "...          ...  \n",
       "74968          1  \n",
       "74969          1  \n",
       "74970          1  \n",
       "74971          1  \n",
       "74972          1  \n",
       "\n",
       "[74973 rows x 4 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_data = pd.concat([initial_dataset,new_data],ignore_index=True,axis=0)\n",
    "target_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(74973, 4)\n",
      "(73595, 4)\n"
     ]
    }
   ],
   "source": [
    "print(target_data.shape)\n",
    "target_data.drop_duplicates(subset=[\"text\"],inplace=True,keep=\"first\")\n",
    "target_data = target_data.dropna()\n",
    "target_data.reset_index(inplace=True,drop=True)\n",
    "print(target_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a Tokenizer\n",
    "Reading the competition discussions lead me to this [Notebook](https://www.kaggle.com/code/datafan07/train-your-own-tokenizer), where is suggested to add words with typos into the vocabulary for better performance by training a tokenizer.\n",
    "\n",
    "1. normalization\n",
    "2. pre-tokenization\n",
    "3. model\n",
    "4. post-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">ByteLevel: \n",
    ">\n",
    ">Splits on whitespaces while remapping all the bytes to a set of visible characters. This technique as been introduced by OpenAI with GPT-2 and has some more or less nice properties:\n",
    "> - Since it maps on bytes, a tokenizer using this only requires 256 characters as initial alphabet (the number of values a byte can have), as opposed to the 130,000+ Unicode characters.\n",
    "> - A consequence of the previous point is that it is absolutely unnecessary to have an unknown token using this since we can represent anything with 256 tokens (Youhou!! ðŸŽ‰ðŸŽ‰)\n",
    "> - For non ascii characters, it gets completely unreadable, but it works nonetheless!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Byte-Pair Encoding tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Creating Byte-Pair Encoding tokenizer\n",
    "raw_tokenizer = Tokenizer(models.BPE(unk_token=\"[UNK]\"))\n",
    "#Cleaning\n",
    "raw_tokenizer.normalizer =  normalizers.Sequence(\n",
    "    [\n",
    "        normalizers.NFC(),\n",
    "        normalizers.Lowercase(),\n",
    "        normalizers.Replace(\"\\n\",\" \"),\n",
    "        normalizers.Replace(\"\\r\",\" \"),\n",
    "        normalizers.Replace(\"\\t\",\" \")\n",
    "    ]    \n",
    "    )\n",
    "#First tokenization\n",
    "raw_tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel()\n",
    "#Training\n",
    "special_tokens = [\"[UNK]\", \"[PAD]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"]\n",
    "trainer = trainers.BpeTrainer(show_progress=True,special_tokens=special_tokens)\n",
    "\n",
    "def data_iter(dataset):\n",
    "    \"\"\"\n",
    "    A generator function for iterating over a dataset in chunks.\n",
    "    \"\"\"    \n",
    "    for i in range(0, len(dataset), 1000):\n",
    "        yield dataset[i : i + 1000][\"text\"]\n",
    "\n",
    "raw_tokenizer.train_from_iterator(data_iter(target_data),trainer)\n",
    "\n",
    "tokenizer = PreTrainedTokenizerFast(\n",
    "    tokenizer_object=raw_tokenizer,\n",
    "    unk_token=\"[UNK]\",\n",
    "    pad_token=\"[PAD]\",\n",
    "    cls_token=\"[CLS]\",\n",
    "    sep_token=\"[SEP]\",\n",
    "    mask_token=\"[MASK]\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('../data/lexp/byte_pair_tokenizer/tokenizer_config.json',\n",
       " '../data/lexp/byte_pair_tokenizer/special_tokens_map.json',\n",
       " '../data/lexp/byte_pair_tokenizer/tokenizer.json')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained(\"../data/lexp/byte_pair_tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"martÃ­n's bag\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_tokenizer.normalizer.normalize_str(\"MartÃ­n's bag\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ä mart', 'ÃƒÅƒ', 'n', \"'s\", 'Ä bag']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(\"MartÃ­n's bag\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Words Tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>prompt_id</th>\n",
       "      <th>text</th>\n",
       "      <th>generated</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0059830c</td>\n",
       "      <td>0</td>\n",
       "      <td>[Ä cars, ., Ä cars, Ä have, Ä been, Ä around, Ä sinc...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>005db917</td>\n",
       "      <td>0</td>\n",
       "      <td>[Ä transportation, Ä is, Ä a, Ä large, Ä necessity,...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>008f63e3</td>\n",
       "      <td>0</td>\n",
       "      <td>[Ä \", america, 's, Ä love, Ä affair, Ä with, Ä it, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00940276</td>\n",
       "      <td>0</td>\n",
       "      <td>[Ä how, Ä often, Ä do, Ä you, Ä ride, Ä in, Ä a, Ä car...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00c39458</td>\n",
       "      <td>0</td>\n",
       "      <td>[Ä cars, Ä are, Ä a, Ä wonderful, Ä thing, ., Ä they...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73590</th>\n",
       "      <td>73590</td>\n",
       "      <td>21</td>\n",
       "      <td>[Ä i, Ä am, Ä writing, Ä you, Ä today, Ä to, Ä disagr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73591</th>\n",
       "      <td>73591</td>\n",
       "      <td>21</td>\n",
       "      <td>[Ä dear, Ä principal, ,, Ä , Ä in, Ä conclusion, ,,...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73592</th>\n",
       "      <td>73592</td>\n",
       "      <td>21</td>\n",
       "      <td>[Ä dear, Ä mrs, ., Ä principal, ,, Ä , Ä in, Ä these...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73593</th>\n",
       "      <td>73593</td>\n",
       "      <td>21</td>\n",
       "      <td>[Ä i, Ä enjoyed, Ä form, Ä five, Ä and, Ä excitedly,...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73594</th>\n",
       "      <td>73594</td>\n",
       "      <td>21</td>\n",
       "      <td>[Ä dear, Ä teacher, _, name, ,, Ä , Ä well, Ä ms, /...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>73595 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             id  prompt_id                                               text  \\\n",
       "0      0059830c          0  [Ä cars, ., Ä cars, Ä have, Ä been, Ä around, Ä sinc...   \n",
       "1      005db917          0  [Ä transportation, Ä is, Ä a, Ä large, Ä necessity,...   \n",
       "2      008f63e3          0  [Ä \", america, 's, Ä love, Ä affair, Ä with, Ä it, ...   \n",
       "3      00940276          0  [Ä how, Ä often, Ä do, Ä you, Ä ride, Ä in, Ä a, Ä car...   \n",
       "4      00c39458          0  [Ä cars, Ä are, Ä a, Ä wonderful, Ä thing, ., Ä they...   \n",
       "...         ...        ...                                                ...   \n",
       "73590     73590         21  [Ä i, Ä am, Ä writing, Ä you, Ä today, Ä to, Ä disagr...   \n",
       "73591     73591         21  [Ä dear, Ä principal, ,, Ä , Ä in, Ä conclusion, ,,...   \n",
       "73592     73592         21  [Ä dear, Ä mrs, ., Ä principal, ,, Ä , Ä in, Ä these...   \n",
       "73593     73593         21  [Ä i, Ä enjoyed, Ä form, Ä five, Ä and, Ä excitedly,...   \n",
       "73594     73594         21  [Ä dear, Ä teacher, _, name, ,, Ä , Ä well, Ä ms, /...   \n",
       "\n",
       "       generated  \n",
       "0              0  \n",
       "1              0  \n",
       "2              0  \n",
       "3              0  \n",
       "4              0  \n",
       "...          ...  \n",
       "73590          1  \n",
       "73591          1  \n",
       "73592          1  \n",
       "73593          1  \n",
       "73594          1  \n",
       "\n",
       "[73595 rows x 4 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset = target_data.copy()\n",
    "tokenized_dataset[\"text\"] = target_data[\"text\"].apply(lambda x : tokenizer.tokenize(x))\n",
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deleting stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_stopwords=[token for word in stopwords.words('english') for token in tokenizer.tokenize(word)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_docs = []\n",
    "for doc in tokenized_dataset[\"text\"]:\n",
    "    tokens = [word for word in doc if word not in tokenized_stopwords]  # Eliminar stopwords\n",
    "    new_docs.append(tokens)\n",
    "tokenized_dataset[\"text\"]=new_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset.to_csv(\"../data/lexp/tokenized_data/tokenized.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        ([Ä cars, ., Ä cars, Ä around, Ä since, Ä became, Ä ...\n",
       "1        ([Ä transportation, Ä large, Ä necessity, Ä countr...\n",
       "2        ([Ä \", america, Ä love, Ä affair, Ä vehicles, Ä see...\n",
       "3        ([Ä often, Ä ride, Ä car, ?, Ä drive, Ä one, Ä motor...\n",
       "4        ([Ä cars, Ä wonderful, Ä thing, ., Ä perhaps, Ä one...\n",
       "                               ...                        \n",
       "73590    ([Ä writing, Ä today, Ä disagree, Ä taking, Ä actio...\n",
       "73591    ([Ä dear, Ä principal, ,, Ä , Ä conclusion, ,, Ä wo...\n",
       "73592    ([Ä dear, Ä mrs, ., Ä principal, ,, Ä , Ä kinds, Ä c...\n",
       "73593    ([Ä enjoyed, Ä form, Ä five, Ä excitedly, Ä ex, Ä cl...\n",
       "73594    ([Ä dear, Ä teacher, _, name, ,, Ä , Ä well, Ä ms, ...\n",
       "Name: text, Length: 73595, dtype: object"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_dataset = tokenized_dataset.copy()\n",
    "docs_dataset[\"text\"] = [doc2vec.TaggedDocument(row[2],['z'+str(row[0])]) for row in tokenized_dataset.values]\n",
    "docs_dataset[\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_model = doc2vec.Doc2Vec(documents=docs_dataset[\"text\"],vector_size=100,epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_model.save(\"../data/lexp/embedding_model/docModel.bin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inferred Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset = pd.read_csv(\"../data/tokenized_data/tokenized.csv\",)\n",
    "tokenized_dataset[\"text\"] = tokenized_dataset[\"text\"].apply(lambda x : eval(x))\n",
    "docs_dataset = tokenized_dataset.copy()\n",
    "docs_dataset[\"text\"] = [doc2vec.TaggedDocument(row[2],['z'+row[0]]) for row in tokenized_dataset.values]\n",
    "docs_dataset[\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def features(dataset):\n",
    "    token_count=dataset[\"text\"].apply(lambda x: len(x))\n",
    "    sentence_count = []\n",
    "    punctuation_count = []\n",
    "    apostrofees_count = []\n",
    "    unk_count = []\n",
    "    for doc in dataset[\"text\"]:\n",
    "        unk = 0\n",
    "        dot = 0\n",
    "        punctuation = 0\n",
    "        apostrofees = 0\n",
    "        for token in doc.words:\n",
    "            if(token.endswith(\".\")):\n",
    "                dot+=1\n",
    "                punctuation+=1\n",
    "            elif(token.endswith(\",\") or token.endswith(\"?\") or token.endswith(\"!\")):\n",
    "                punctuation+=1\n",
    "            elif(token.count(\"'\")>0):\n",
    "                    apostrofees+=token.count(\"'\")\n",
    "            elif(token==\"[UNK]\"):\n",
    "                unk+=1\n",
    "        sentence_count.append(dot)\n",
    "        punctuation_count.append(punctuation)\n",
    "        apostrofees_count.append(apostrofees)\n",
    "        unk_count.append(unk)\n",
    "    df = pd.DataFrame(\n",
    "        columns=[\"token_num\",\"sent_num\",\"punct_sym\",\"apostrof_sym\",\"unk_num\"]\n",
    "    )\n",
    "    df[\"token_num\"]=token_count\n",
    "    df[\"sent_num\"]=sentence_count\n",
    "    df[\"punct_sym\"]=punctuation_count\n",
    "    df[\"apostrof_sym\"]=apostrofees_count\n",
    "    df[\"unk_num\"]=unk_count\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalizar embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_model = doc2vec.Doc2Vec.load(\"../data/lexp/embedding_model/docModel.bin\")\n",
    "arr = [doc_model.dv[doc.tags] for doc in docs_dataset[\"text\"]]\n",
    "embeddings_dataset = pd.DataFrame(np.reshape(arr,(len(arr), 100)))\n",
    "embeddings_dataset[\"norm\"]=np.linalg.norm(embeddings_dataset, axis=1)\n",
    "norm_embeddings_dataset = pd.DataFrame(np.apply_along_axis(lambda x: x / np.linalg.norm(x), axis=1, arr=embeddings_dataset))\n",
    "norm_embeddings_dataset[\"normalized_norm\"] = (embeddings_dataset['norm'] - embeddings_dataset['norm'].min()) / (embeddings_dataset['norm'].max() - embeddings_dataset['norm'].min())\n",
    "# Eliminar la columna '100' del DataFrame\n",
    "norm_embeddings_dataset = norm_embeddings_dataset.drop([100], axis=1)\n",
    "\n",
    "# Concatenar los DataFrames\n",
    "doc2vec_data = pd.concat([docs_dataset, norm_embeddings_dataset], axis=1)\n",
    "\n",
    "# Calcular caracterÃ­sticas adicionales con la funciÃ³n 'features' (no proporcionada en tu cÃ³digo)\n",
    "doc_features = features(doc2vec_data)\n",
    "\n",
    "# Normalizar las caracterÃ­sticas adicionales\n",
    "for col in doc_features.columns:\n",
    "    if doc_features[col].max() > 0:\n",
    "        doc_features[col] = doc_features[col] / np.linalg.norm(doc_features[col])\n",
    "\n",
    "# Concatenar las caracterÃ­sticas adicionales al DataFrame principal y eliminar la columna 'text'\n",
    "doc2vec_data = pd.concat([doc2vec_data, doc_features], axis=1).drop(\"text\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar el DataFrame resultante en un archivo CSV\n",
    "from sklearn.model_selection import train_test_split\n",
    "train,test = train_test_split(doc2vec_data, test_size=0.6)\n",
    "train.to_csv(\"../data/lexp/pre_processed/train.csv\", index=False)\n",
    "test.to_csv(\"../data/lexp/pre_processed/test.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
